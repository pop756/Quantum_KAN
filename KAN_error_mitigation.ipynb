{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# 양자 장치 설정\n",
    "dev = qml.device('default.mixed', wires=2)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def random_circuit(params):\n",
    "    \"\"\"임의의 게이트를 포함하는 기본 회로\"\"\"\n",
    "\n",
    "    for i in range(10):\n",
    "        qml.RX(params[i*3+0], wires=0)\n",
    "        qml.RY(params[i*3+1], wires=1)\n",
    "        qml.CNOT(wires=[0, 1])\n",
    "        qml.RZ(params[i*3+2], wires=0)\n",
    "    return qml.expval(qml.PauliZ(0)@qml.PauliZ(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extra_polation(circ,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02):\n",
    "    \n",
    "    real_vaule = circ(theta)\n",
    "    dev = circ.device\n",
    "    ops = circ.qtape.operations\n",
    "    ops_inv = ops[::-1]\n",
    "\n",
    "\n",
    "    meas = circ.qtape.measurements[0]\n",
    "    def noise_circ():\n",
    "        for op in ops:\n",
    "            eval(f'qml.{op}')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    def noise_circ_inv():\n",
    "        for op in ops_inv:\n",
    "            eval(f'qml.adjoint(qml.{op})')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def extra_polation(factor):\n",
    "        Z = qml.PauliZ\n",
    "        Y = qml.PauliY\n",
    "        X = qml.PauliX\n",
    "        for i in range(factor):\n",
    "            noise_circ()\n",
    "            noise_circ_inv()\n",
    "        noise_circ()\n",
    "        return eval(f\"qml.{meas}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    res = []\n",
    "    for factor in noise_factor:\n",
    "        res.append(extra_polation(factor))\n",
    "    return res,real_vaule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22/500 [00:21<10:13,  1.28s/it]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# 데이터셋 생성 클래스\n",
    "class QuantumData(Dataset):\n",
    "    def __init__(self, size=500):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _ in tqdm(range(size)):\n",
    "            theta = [np.random.uniform(0, 2 * np.pi) for i in range(30)]\n",
    "            noise_factor,result = extra_polation(random_circuit,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02)\n",
    "            self.data.append(np.array(noise_factor))\n",
    "            self.labels.append(result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "dataset = QuantumData()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return torch.squeeze(self.fc3(x))\n",
    "\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.3895, Val Loss: 0.3306\n",
      "Epoch 2, Train Loss: 0.3735, Val Loss: 0.3151\n",
      "Epoch 3, Train Loss: 0.3668, Val Loss: 0.3029\n",
      "Epoch 4, Train Loss: 0.3603, Val Loss: 0.2962\n",
      "Epoch 5, Train Loss: 0.3506, Val Loss: 0.2901\n",
      "Epoch 6, Train Loss: 0.3460, Val Loss: 0.2818\n",
      "Epoch 7, Train Loss: 0.3269, Val Loss: 0.2742\n",
      "Epoch 8, Train Loss: 0.3309, Val Loss: 0.2664\n",
      "Epoch 9, Train Loss: 0.3212, Val Loss: 0.2558\n",
      "Epoch 10, Train Loss: 0.3101, Val Loss: 0.2445\n",
      "Epoch 11, Train Loss: 0.2921, Val Loss: 0.2301\n",
      "Epoch 12, Train Loss: 0.2632, Val Loss: 0.2074\n",
      "Epoch 13, Train Loss: 0.2413, Val Loss: 0.1838\n",
      "Epoch 14, Train Loss: 0.2137, Val Loss: 0.1617\n",
      "Epoch 15, Train Loss: 0.1844, Val Loss: 0.1358\n",
      "Epoch 16, Train Loss: 0.1415, Val Loss: 0.1049\n",
      "Epoch 17, Train Loss: 0.1091, Val Loss: 0.0751\n",
      "Epoch 18, Train Loss: 0.0789, Val Loss: 0.0530\n",
      "Epoch 19, Train Loss: 0.0554, Val Loss: 0.0399\n",
      "Epoch 20, Train Loss: 0.0363, Val Loss: 0.0249\n",
      "Epoch 21, Train Loss: 0.0295, Val Loss: 0.0224\n",
      "Epoch 22, Train Loss: 0.0239, Val Loss: 0.0201\n",
      "Epoch 23, Train Loss: 0.0255, Val Loss: 0.0200\n",
      "Epoch 24, Train Loss: 0.0257, Val Loss: 0.0202\n",
      "Epoch 25, Train Loss: 0.0251, Val Loss: 0.0194\n",
      "Epoch 26, Train Loss: 0.0256, Val Loss: 0.0203\n",
      "Epoch 27, Train Loss: 0.0231, Val Loss: 0.0189\n",
      "Epoch 28, Train Loss: 0.0239, Val Loss: 0.0193\n",
      "Epoch 29, Train Loss: 0.0229, Val Loss: 0.0190\n",
      "Epoch 30, Train Loss: 0.0216, Val Loss: 0.0185\n",
      "Epoch 31, Train Loss: 0.0236, Val Loss: 0.0187\n",
      "Epoch 32, Train Loss: 0.0214, Val Loss: 0.0182\n",
      "Epoch 33, Train Loss: 0.0222, Val Loss: 0.0191\n",
      "Epoch 34, Train Loss: 0.0216, Val Loss: 0.0179\n",
      "Epoch 35, Train Loss: 0.0204, Val Loss: 0.0179\n",
      "Epoch 36, Train Loss: 0.0221, Val Loss: 0.0178\n",
      "Epoch 37, Train Loss: 0.0232, Val Loss: 0.0180\n",
      "Epoch 38, Train Loss: 0.0222, Val Loss: 0.0175\n",
      "Epoch 39, Train Loss: 0.0195, Val Loss: 0.0175\n",
      "Epoch 40, Train Loss: 0.0223, Val Loss: 0.0173\n",
      "Epoch 41, Train Loss: 0.0208, Val Loss: 0.0169\n",
      "Epoch 42, Train Loss: 0.0210, Val Loss: 0.0172\n",
      "Epoch 43, Train Loss: 0.0198, Val Loss: 0.0169\n",
      "Epoch 44, Train Loss: 0.0192, Val Loss: 0.0166\n",
      "Epoch 45, Train Loss: 0.0203, Val Loss: 0.0168\n",
      "Epoch 46, Train Loss: 0.0210, Val Loss: 0.0163\n",
      "Epoch 47, Train Loss: 0.0196, Val Loss: 0.0163\n",
      "Epoch 48, Train Loss: 0.0209, Val Loss: 0.0167\n",
      "Epoch 49, Train Loss: 0.0204, Val Loss: 0.0160\n",
      "Epoch 50, Train Loss: 0.0186, Val Loss: 0.0161\n",
      "Epoch 51, Train Loss: 0.0198, Val Loss: 0.0162\n",
      "Epoch 52, Train Loss: 0.0188, Val Loss: 0.0158\n",
      "Epoch 53, Train Loss: 0.0189, Val Loss: 0.0155\n",
      "Epoch 54, Train Loss: 0.0195, Val Loss: 0.0155\n",
      "Epoch 55, Train Loss: 0.0189, Val Loss: 0.0159\n",
      "Epoch 56, Train Loss: 0.0189, Val Loss: 0.0153\n",
      "Epoch 57, Train Loss: 0.0193, Val Loss: 0.0151\n",
      "Epoch 58, Train Loss: 0.0202, Val Loss: 0.0155\n",
      "Epoch 59, Train Loss: 0.0187, Val Loss: 0.0149\n",
      "Epoch 60, Train Loss: 0.0173, Val Loss: 0.0155\n",
      "Epoch 61, Train Loss: 0.0175, Val Loss: 0.0147\n",
      "Epoch 62, Train Loss: 0.0177, Val Loss: 0.0152\n",
      "Epoch 63, Train Loss: 0.0183, Val Loss: 0.0147\n",
      "Epoch 64, Train Loss: 0.0192, Val Loss: 0.0145\n",
      "Epoch 65, Train Loss: 0.0169, Val Loss: 0.0146\n",
      "Epoch 66, Train Loss: 0.0185, Val Loss: 0.0143\n",
      "Epoch 67, Train Loss: 0.0165, Val Loss: 0.0149\n",
      "Epoch 68, Train Loss: 0.0172, Val Loss: 0.0142\n",
      "Epoch 69, Train Loss: 0.0180, Val Loss: 0.0140\n",
      "Epoch 70, Train Loss: 0.0175, Val Loss: 0.0142\n",
      "Epoch 71, Train Loss: 0.0174, Val Loss: 0.0139\n",
      "Epoch 72, Train Loss: 0.0173, Val Loss: 0.0141\n",
      "Epoch 73, Train Loss: 0.0156, Val Loss: 0.0138\n",
      "Epoch 74, Train Loss: 0.0164, Val Loss: 0.0139\n",
      "Epoch 75, Train Loss: 0.0155, Val Loss: 0.0136\n",
      "Epoch 76, Train Loss: 0.0173, Val Loss: 0.0137\n",
      "Epoch 77, Train Loss: 0.0163, Val Loss: 0.0134\n",
      "Epoch 78, Train Loss: 0.0161, Val Loss: 0.0133\n",
      "Epoch 79, Train Loss: 0.0161, Val Loss: 0.0138\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[419], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     20\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 데이터셋 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 데이터 로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 및 검증 루프\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, labels_val in val_loader:\n",
    "            outputs = model(inputs_val)\n",
    "            loss = criterion(outputs, labels_val)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.9140, grad_fn=<SqueezeBackward0>),\n",
       " tensor([-0.1987, -0.0110, -0.0012, -0.0002]),\n",
       " tensor(-0.9959))"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model(inputs[5]),inputs[5],labels[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4344109892845154 tensor(0.5300) tensor(0.0813)\n",
      "-0.7787020802497864 tensor(-0.8143) tensor(-0.1826)\n",
      "-0.7724839448928833 tensor(-0.7121) tensor(-0.1762)\n",
      "0.33572620153427124 tensor(0.2991) tensor(0.0619)\n",
      "-0.988797128200531 tensor(-0.9991) tensor(-0.2238)\n",
      "0.2692647874355316 tensor(0.1643) tensor(0.0481)\n",
      "-0.3944154381752014 tensor(-0.3738) tensor(-0.0942)\n",
      "-0.8359125852584839 tensor(-0.7603) tensor(-0.1972)\n",
      "-0.8393959999084473 tensor(-0.9875) tensor(-0.1908)\n",
      "-1.0243483781814575 tensor(-0.9984) tensor(-0.2318)\n",
      "-0.5831440687179565 tensor(-0.4724) tensor(-0.1604)\n",
      "0.22117185592651367 tensor(0.1379) tensor(0.0388)\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(model(inputs_val[i]).item(),labels_val[i],inputs_val[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
