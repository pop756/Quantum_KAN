{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러 완화 결과: 0.0\n",
      "실제 결과: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# 양자 장치 설정\n",
    "dev = qml.device(\"default.mixed\", wires=2)\n",
    "\n",
    "\n",
    "def circuit_real(theta):\n",
    "    qml.RX(theta[0], wires=0)\n",
    "    qml.CNOT( wires=[0,1])\n",
    "    qml.RY(theta[1], wires=0)\n",
    "\n",
    "def circuit(theta,p):\n",
    "    qml.RX(theta, wires=0)\n",
    "    qml.DepolarizingChannel(p, wires=0)\n",
    "    qml.CNOT( wires=[0,1])\n",
    "    qml.DepolarizingChannel(p, wires=1)\n",
    "    qml.RY(theta/2, wires=0)\n",
    "    qml.DepolarizingChannel(p, wires=0)\n",
    "\n",
    "def adj_circuit(theta,p):\n",
    "    qml.adjoint(qml.RY)(theta/2, wires=0)\n",
    "    qml.DepolarizingChannel(p, wires=0)\n",
    "    qml.CNOT( wires=[0,1])\n",
    "    qml.DepolarizingChannel(p, wires=1)\n",
    "    qml.adjoint(qml.RX)(theta, wires=0)\n",
    "    qml.DepolarizingChannel(p, wires=0)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit_(theta):\n",
    "    circuit_real(theta)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def scale_circuit(theta,scale_factors,p):\n",
    "    for i in range(scale_factors):\n",
    "        circuit(theta,p)\n",
    "        adj_circuit(theta,p)\n",
    "    circuit(theta,p)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "# 에러 완화를 위한 서킷 실행 함수\n",
    "\n",
    "def execute_circuit(theta, scale_factor,p):\n",
    "    # 스케일 팩터에 따라 회로의 게이트 파라미터를 조정\n",
    "    noisy_result = scale_circuit(theta,scale_factor,p)\n",
    "    return noisy_result\n",
    "\n",
    "# ZNE를 사용하여 에러 완화 실행\n",
    "def mitigate_error(theta, scale_factors,p):\n",
    "    results = [execute_circuit(theta, s,p) for s in scale_factors]\n",
    "    # 선형 외삽을 사용하여 제로 노이즈 추정\n",
    "    fit = np.polyfit(scale_factors, results, 1)\n",
    "    zne_result = fit[1]-fit[0]*1/2\n",
    "    return zne_result,results\n",
    "\n",
    "# 파라미터와 스케일 팩터 설정\n",
    "theta = np.pi / 2\n",
    "scale_factors = [0, 1, 2,3]\n",
    "\n",
    "# 에러 완화 결과\n",
    "mitigated_result,results = mitigate_error(theta, scale_factors,0.1)\n",
    "real_result = circuit_(theta)\n",
    "print(\"에러 완화 결과:\", mitigated_result)\n",
    "print(\"실제 결과:\", real_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# 양자 장치 설정\n",
    "dev = qml.device('default.mixed', wires=2)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def random_circuit(params):\n",
    "    \"\"\"임의의 게이트를 포함하는 기본 회로\"\"\"\n",
    "\n",
    "    for i in range(10):\n",
    "        qml.RX(params[i*3+0], wires=0)\n",
    "        qml.RY(params[i*3+1], wires=1)\n",
    "        qml.CNOT(wires=[0, 1])\n",
    "        qml.RZ(params[i*3+2], wires=0)\n",
    "    return qml.expval(qml.PauliZ(0)@qml.PauliZ(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extra_polation(circ,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02):\n",
    "    \n",
    "    real_vaule = circ(theta)\n",
    "    dev = circ.device\n",
    "    ops = circ.qtape.operations\n",
    "    ops_inv = ops[::-1]\n",
    "\n",
    "\n",
    "    meas = circ.qtape.measurements[0]\n",
    "    def noise_circ():\n",
    "        for op in ops:\n",
    "            eval(f'qml.{op}')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    def noise_circ_inv():\n",
    "        for op in ops_inv:\n",
    "            eval(f'qml.adjoint(qml.{op})')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def extra_polation(factor):\n",
    "        Z = qml.PauliZ\n",
    "        Y = qml.PauliY\n",
    "        X = qml.PauliX\n",
    "        for i in range(factor):\n",
    "            noise_circ()\n",
    "            noise_circ_inv()\n",
    "        noise_circ()\n",
    "        return eval(f\"qml.{meas}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    res = []\n",
    "    for factor in noise_factor:\n",
    "        res.append(extra_polation(factor))\n",
    "    return res,real_vaule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 28/500 [00:27<07:27,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# 데이터셋 생성 클래스\n",
    "class QuantumData(Dataset):\n",
    "    def __init__(self, size=500):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _ in tqdm(range(size)):\n",
    "            theta = [np.random.uniform(0, 2 * np.pi) for i in range(30)]\n",
    "            noise_factor,result = extra_polation(random_circuit,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02)\n",
    "            self.data.append(np.array(noise_factor))\n",
    "            self.labels.append(result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "dataset = QuantumData()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return torch.squeeze(self.fc3(x))\n",
    "\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.2640, Val Loss: 0.1800\n",
      "Epoch 2, Train Loss: 0.2247, Val Loss: 0.1589\n",
      "Epoch 3, Train Loss: 0.2003, Val Loss: 0.1387\n",
      "Epoch 4, Train Loss: 0.1673, Val Loss: 0.1171\n",
      "Epoch 5, Train Loss: 0.1440, Val Loss: 0.0938\n",
      "Epoch 6, Train Loss: 0.1115, Val Loss: 0.0679\n",
      "Epoch 7, Train Loss: 0.0773, Val Loss: 0.0425\n",
      "Epoch 8, Train Loss: 0.0423, Val Loss: 0.0211\n",
      "Epoch 9, Train Loss: 0.0209, Val Loss: 0.0074\n",
      "Epoch 10, Train Loss: 0.0064, Val Loss: 0.0020\n",
      "Epoch 11, Train Loss: 0.0024, Val Loss: 0.0020\n",
      "Epoch 12, Train Loss: 0.0025, Val Loss: 0.0026\n",
      "Epoch 13, Train Loss: 0.0026, Val Loss: 0.0019\n",
      "Epoch 14, Train Loss: 0.0017, Val Loss: 0.0011\n",
      "Epoch 15, Train Loss: 0.0010, Val Loss: 0.0007\n",
      "Epoch 16, Train Loss: 0.0008, Val Loss: 0.0005\n",
      "Epoch 17, Train Loss: 0.0007, Val Loss: 0.0004\n",
      "Epoch 18, Train Loss: 0.0005, Val Loss: 0.0003\n",
      "Epoch 19, Train Loss: 0.0004, Val Loss: 0.0003\n",
      "Epoch 20, Train Loss: 0.0003, Val Loss: 0.0002\n",
      "Epoch 21, Train Loss: 0.0002, Val Loss: 0.0001\n",
      "Epoch 22, Train Loss: 0.0002, Val Loss: 0.0001\n",
      "Epoch 23, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 24, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 25, Train Loss: 0.0001, Val Loss: 0.0001\n",
      "Epoch 26, Train Loss: 0.0001, Val Loss: 0.0000\n",
      "Epoch 27, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 28, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 29, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 30, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 31, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 32, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 33, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 34, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 35, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 36, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 37, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 38, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 39, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 40, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 41, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 42, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 43, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 44, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 45, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 46, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 47, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 48, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 49, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 50, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 51, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 52, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 53, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 54, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 55, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 56, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 57, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 58, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 59, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 60, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 61, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 62, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 63, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 64, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 65, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 66, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 67, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 68, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 69, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 70, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 71, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 72, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 73, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 74, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 75, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 76, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 77, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 78, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 79, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 80, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 81, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 82, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 83, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 84, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 85, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 86, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 87, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 88, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 89, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 90, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 91, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 92, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 93, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 94, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 95, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 96, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 97, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 98, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 99, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 100, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 101, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 102, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 103, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 104, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 105, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 106, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 107, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 108, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 109, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 110, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 111, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 112, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 113, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 114, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 115, Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Epoch 116, Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     20\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[144], line 21\u001b[0m, in \u001b[0;36mQuantumData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 데이터셋 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 데이터 로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 및 검증 루프\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2495, grad_fn=<SqueezeBackward0>),\n",
       " tensor([0.1854, 0.1046, 0.0590, 0.0333]),\n",
       " tensor(0.2468))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs[5]),inputs[5],labels[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
