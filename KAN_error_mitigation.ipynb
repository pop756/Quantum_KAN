{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "# 양자 장치 설정\n",
    "dev = qml.device('default.mixed', wires=2)\n",
    "\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def random_circuit(params):\n",
    "    \"\"\"임의의 게이트를 포함하는 기본 회로\"\"\"\n",
    "\n",
    "    for i in range(10):\n",
    "        qml.RX(params[i*3+0], wires=0)\n",
    "        qml.RY(params[i*3+1], wires=1)\n",
    "        qml.CNOT(wires=[0, 1])\n",
    "        qml.RZ(params[i*3+2], wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def extra_polation(circ,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02):\n",
    "    \n",
    "    real_vaule = circ(theta)\n",
    "    dev = circ.device\n",
    "    ops = circ.qtape.operations\n",
    "    ops_inv = ops[::-1]\n",
    "\n",
    "\n",
    "    meas = circ.qtape.measurements[0]\n",
    "    def noise_circ():\n",
    "        for op in ops:\n",
    "            eval(f'qml.{op}')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    def noise_circ_inv():\n",
    "        for op in ops_inv:\n",
    "            eval(f'qml.adjoint(qml.{op})')\n",
    "            if len(op.wires)>1:\n",
    "                for wire in op.wires:\n",
    "                    qml.DepolarizingChannel(p1, wires=wire)\n",
    "                    qml.BitFlip(p2, wires=wire)\n",
    "            else:\n",
    "                qml.DepolarizingChannel(p1, wires=op.wires) \n",
    "                qml.BitFlip(p2, wires=op.wires)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def extra_polation(factor):\n",
    "        Z = qml.PauliZ\n",
    "        Y = qml.PauliY\n",
    "        X = qml.PauliX\n",
    "        for i in range(factor):\n",
    "            noise_circ()\n",
    "            noise_circ_inv()\n",
    "        noise_circ()\n",
    "        return eval(f\"qml.{meas}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    res = []\n",
    "    for factor in noise_factor:\n",
    "        res.append(extra_polation(factor))\n",
    "    return res,real_vaule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [07:23<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "# 데이터셋 생성 클래스\n",
    "class QuantumData(Dataset):\n",
    "    def __init__(self, size=500):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for _ in tqdm(range(size)):\n",
    "            theta = [np.random.uniform(0, 2 * np.pi) for i in range(30)]\n",
    "            noise_factor,result = extra_polation(random_circuit,theta,noise_factor=[0,1,2,3],p1=0.01,p2=0.02)\n",
    "            self.data.append(np.array(noise_factor))\n",
    "            self.labels.append(result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "dataset = QuantumData()\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 50)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        self.relu = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return torch.squeeze(self.fc3(x))\n",
    "\n",
    "model_MLP = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_MLP = torch.optim.Adam(model_MLP.parameters(), lr=0.01)\n",
    "\n",
    "from kan import KAN\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.KAN = KAN([4,1,1],grid=5)\n",
    "    def forward(self, x):\n",
    "        output = self.KAN(x)\n",
    "        #output = nn.Sigmoid()(output)\n",
    "        output = torch.squeeze(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "model = Model()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3629, Accuracy: 0.0/100 (0%)\n",
      "Epoch 1 Loss 0.156145 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3326, Accuracy: 0.0/100 (0%)\n",
      "Epoch 2 Loss 0.143109 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2539, Accuracy: 0.0/100 (0%)\n",
      "Epoch 3 Loss 0.102056 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.1701, Accuracy: 0.0/100 (0%)\n",
      "Epoch 4 Loss 0.063542 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0898, Accuracy: 0.0/100 (0%)\n",
      "Epoch 5 Loss 0.028758 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0330, Accuracy: 0.0/100 (0%)\n",
      "Epoch 6 Loss 0.011947 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0303, Accuracy: 0.0/100 (0%)\n",
      "Epoch 7 Loss 0.014591 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0357, Accuracy: 0.0/100 (0%)\n",
      "Epoch 8 Loss 0.013332 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0372, Accuracy: 0.0/100 (0%)\n",
      "Epoch 9 Loss 0.011517 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0257, Accuracy: 0.0/100 (0%)\n",
      "Epoch 10 Loss 0.010777 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0262, Accuracy: 0.0/100 (0%)\n",
      "Epoch 11 Loss 0.011063 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0267, Accuracy: 0.0/100 (0%)\n",
      "Epoch 12 Loss 0.009830 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0252, Accuracy: 0.0/100 (0%)\n",
      "Epoch 13 Loss 0.010229 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0263, Accuracy: 0.0/100 (0%)\n",
      "Epoch 14 Loss 0.009766 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0248, Accuracy: 0.0/100 (0%)\n",
      "Epoch 15 Loss 0.009860 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0253, Accuracy: 0.0/100 (0%)\n",
      "Epoch 16 Loss 0.010478 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0263, Accuracy: 0.0/100 (0%)\n",
      "Epoch 17 Loss 0.010012 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0238, Accuracy: 0.0/100 (0%)\n",
      "Epoch 18 Loss 0.009320 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0237, Accuracy: 0.0/100 (0%)\n",
      "Epoch 19 Loss 0.009649 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0260, Accuracy: 0.0/100 (0%)\n",
      "Epoch 20 Loss 0.009645 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0279, Accuracy: 0.0/100 (0%)\n",
      "Epoch 21 Loss 0.009333 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0241, Accuracy: 0.0/100 (0%)\n",
      "Epoch 22 Loss 0.009551 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0237, Accuracy: 0.0/100 (0%)\n",
      "Epoch 23 Loss 0.009346 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0250, Accuracy: 0.0/100 (0%)\n",
      "Epoch 24 Loss 0.009194 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0252, Accuracy: 0.0/100 (0%)\n",
      "Epoch 25 Loss 0.009653 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0243, Accuracy: 0.0/100 (0%)\n",
      "Epoch 26 Loss 0.008957 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0244, Accuracy: 0.0/100 (0%)\n",
      "Epoch 27 Loss 0.009812 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0237, Accuracy: 0.0/100 (0%)\n",
      "Epoch 28 Loss 0.009514 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0258, Accuracy: 0.0/100 (0%)\n",
      "Epoch 29 Loss 0.010557 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.0227, Accuracy: 0.0/100 (0%)\n",
      "Epoch 30 Loss 0.011134 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0224, Accuracy: 0.0/100 (0%)\n",
      "Epoch 31 Loss 0.009018 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0223, Accuracy: 0.0/100 (0%)\n",
      "Epoch 32 Loss 0.009775 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0244, Accuracy: 0.0/100 (0%)\n",
      "Epoch 33 Loss 0.009001 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0243, Accuracy: 0.0/100 (0%)\n",
      "Epoch 34 Loss 0.008825 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0230, Accuracy: 0.0/100 (0%)\n",
      "Epoch 35 Loss 0.008992 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0249, Accuracy: 0.0/100 (0%)\n",
      "Epoch 36 Loss 0.009114 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0233, Accuracy: 0.0/100 (0%)\n",
      "Epoch 37 Loss 0.008599 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0229, Accuracy: 0.0/100 (0%)\n",
      "Epoch 38 Loss 0.009819 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.0264, Accuracy: 0.0/100 (0%)\n",
      "Epoch 39 Loss 0.009096 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.0224, Accuracy: 0.0/100 (0%)\n",
      "Epoch 40 Loss 0.008478 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.0247, Accuracy: 0.0/100 (0%)\n",
      "Epoch 41 Loss 0.008443 acc : 0.000000 reg : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.0216, Accuracy: 0.0/100 (0%)\n",
      "Epoch 42 Loss 0.008336 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0216, Accuracy: 0.0/100 (0%)\n",
      "Epoch 43 Loss 0.008841 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0218, Accuracy: 0.0/100 (0%)\n",
      "Epoch 44 Loss 0.008591 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0232, Accuracy: 0.0/100 (0%)\n",
      "Epoch 45 Loss 0.008639 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0213, Accuracy: 0.0/100 (0%)\n",
      "Epoch 46 Loss 0.008386 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0213, Accuracy: 0.0/100 (0%)\n",
      "Epoch 47 Loss 0.008517 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0226, Accuracy: 0.0/100 (0%)\n",
      "Epoch 48 Loss 0.008376 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.0/100 (0%)\n",
      "Epoch 49 Loss 0.008259 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0214, Accuracy: 0.0/100 (0%)\n",
      "Epoch 50 Loss 0.008311 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0226, Accuracy: 0.0/100 (0%)\n",
      "Epoch 51 Loss 0.008215 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0209, Accuracy: 0.0/100 (0%)\n",
      "Epoch 52 Loss 0.008815 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0213, Accuracy: 0.0/100 (0%)\n",
      "Epoch 53 Loss 0.007642 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0231, Accuracy: 0.0/100 (0%)\n",
      "Epoch 54 Loss 0.008599 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0219, Accuracy: 0.0/100 (0%)\n",
      "Epoch 55 Loss 0.008236 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.0/100 (0%)\n",
      "Epoch 56 Loss 0.008170 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0203, Accuracy: 0.0/100 (0%)\n",
      "Epoch 57 Loss 0.008184 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0205, Accuracy: 0.0/100 (0%)\n",
      "Epoch 58 Loss 0.008152 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0212, Accuracy: 0.0/100 (0%)\n",
      "Epoch 59 Loss 0.007955 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0202, Accuracy: 0.0/100 (0%)\n",
      "Epoch 60 Loss 0.008533 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0202, Accuracy: 0.0/100 (0%)\n",
      "Epoch 61 Loss 0.009698 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0244, Accuracy: 0.0/100 (0%)\n",
      "Epoch 62 Loss 0.008652 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0258, Accuracy: 0.0/100 (0%)\n",
      "Epoch 63 Loss 0.007916 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0210, Accuracy: 0.0/100 (0%)\n",
      "Epoch 64 Loss 0.008039 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0208, Accuracy: 0.0/100 (0%)\n",
      "Epoch 65 Loss 0.007896 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0244, Accuracy: 0.0/100 (0%)\n",
      "Epoch 66 Loss 0.008851 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.0/100 (0%)\n",
      "Epoch 67 Loss 0.008191 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.0221, Accuracy: 0.0/100 (0%)\n",
      "Epoch 68 Loss 0.008497 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.0223, Accuracy: 0.0/100 (0%)\n",
      "Epoch 69 Loss 0.008189 acc : 0.000000 reg : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.0212, Accuracy: 0.0/100 (0%)\n",
      "Epoch 70 Loss 0.007638 acc : 0.000000 reg : 0.000000 stop count : 10"
     ]
    }
   ],
   "source": [
    "from functions.training import Early_stop_train_KAN\n",
    "\n",
    "train_seq = Early_stop_train_KAN(model,optimizer,criterion)\n",
    "\n",
    "train_seq.train_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixing (0,0,0) with sin, r2=0.9999704360961914\n",
      "fixing (0,1,0) with 1/x^4, r2=1.0000004768371582\n",
      "fixing (0,2,0) with exp, r2=0.9999726414680481\n",
      "fixing (0,3,0) with 1/x, r2=0.9396162629127502\n",
      "fixing (1,0,0) with abs, r2=0.9999914169311523\n"
     ]
    }
   ],
   "source": [
    "model.KAN.auto_symbolic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.76 - 0.29 \\left|{1.25 e^{10.0 x_{3}} + 4.99 \\sin{\\left(3.35 x_{1} + 9.39 \\right)} - 14.98 + \\frac{16.24}{\\left(1 - 0.16 x_{2}\\right)^{4}}}\\right|$"
      ],
      "text/plain": [
       "0.76 - 0.29*Abs(1.25*exp(10.0*x_3) + 4.99*sin(3.35*x_1 + 9.39) - 14.98 + 16.24/(1 - 0.16*x_2)**4)"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.KAN.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.KAN.plot()\n",
    "model.KAN = model.KAN.prune(threshold=0.07)\n",
    "model(inputs_val)\n",
    "model.KAN.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KAN' object has no attribute 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[565], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKAN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\kan\\KAN.py:709\u001b[0m, in \u001b[0;36mKAN.plot\u001b[1;34m(self, folder, beta, mask, mode, scale, tick, sample, in_vars, out_vars, title)\u001b[0m\n\u001b[0;32m    707\u001b[0m     alpha_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 709\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n) \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m/\u001b[39m n, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m N) \u001b[38;5;241m+\u001b[39m id_ \u001b[38;5;241m/\u001b[39m N], [l \u001b[38;5;241m*\u001b[39m y0, (l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m y0 \u001b[38;5;241m-\u001b[39m y1], color\u001b[38;5;241m=\u001b[39mcolor, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m scale, alpha\u001b[38;5;241m=\u001b[39malpha[l][j][i] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m[l][i]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][j]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    710\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m N) \u001b[38;5;241m+\u001b[39m id_ \u001b[38;5;241m/\u001b[39m N, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m n_next) \u001b[38;5;241m+\u001b[39m j \u001b[38;5;241m/\u001b[39m n_next], [(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m y0 \u001b[38;5;241m+\u001b[39m y1, (l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m y0], color\u001b[38;5;241m=\u001b[39mcolor, lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m scale, alpha\u001b[38;5;241m=\u001b[39malpha[l][j][i] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[l][i]\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m][j]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KAN' object has no attribute 'mask'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.KAN.plot(mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3939, Accuracy: 0.0/100 (0%)\n",
      "Epoch 1 Loss 0.178795 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3285, Accuracy: 0.0/100 (0%)\n",
      "Epoch 2 Loss 0.141259 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.1985, Accuracy: 0.0/100 (0%)\n",
      "Epoch 3 Loss 0.072215 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 4 Loss 0.015727 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0521, Accuracy: 0.0/100 (0%)\n",
      "Epoch 5 Loss 0.023369 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0267, Accuracy: 0.0/100 (0%)\n",
      "Epoch 6 Loss 0.015201 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0414, Accuracy: 0.0/100 (0%)\n",
      "Epoch 7 Loss 0.016004 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0389, Accuracy: 0.0/100 (0%)\n",
      "Epoch 8 Loss 0.012366 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0309, Accuracy: 0.0/100 (0%)\n",
      "Epoch 9 Loss 0.013695 acc : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0374, Accuracy: 0.0/100 (0%)\n",
      "Epoch 10 Loss 0.013405 acc : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0270, Accuracy: 0.0/100 (0%)\n",
      "Epoch 11 Loss 0.009784 acc : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0245, Accuracy: 0.0/100 (0%)\n",
      "Epoch 12 Loss 0.009125 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0242, Accuracy: 0.0/100 (0%)\n",
      "Epoch 13 Loss 0.009671 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0221, Accuracy: 0.0/100 (0%)\n",
      "Epoch 14 Loss 0.009472 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0230, Accuracy: 0.0/100 (0%)\n",
      "Epoch 15 Loss 0.008890 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.0/100 (0%)\n",
      "Epoch 16 Loss 0.008424 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0252, Accuracy: 0.0/100 (0%)\n",
      "Epoch 17 Loss 0.009260 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0320, Accuracy: 0.0/100 (0%)\n",
      "Epoch 18 Loss 0.009383 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0258, Accuracy: 0.0/100 (0%)\n",
      "Epoch 19 Loss 0.009677 acc : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0317, Accuracy: 0.0/100 (0%)\n",
      "Epoch 20 Loss 0.009442 acc : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0247, Accuracy: 0.0/100 (0%)\n",
      "Epoch 21 Loss 0.009265 acc : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0215, Accuracy: 0.0/100 (0%)\n",
      "Epoch 22 Loss 0.007963 acc : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.0188, Accuracy: 0.0/100 (0%)\n",
      "Epoch 23 Loss 0.007342 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0183, Accuracy: 0.0/100 (0%)\n",
      "Epoch 24 Loss 0.008024 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0192, Accuracy: 0.0/100 (0%)\n",
      "Epoch 25 Loss 0.006502 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0173, Accuracy: 0.0/100 (0%)\n",
      "Epoch 26 Loss 0.006940 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0175, Accuracy: 0.0/100 (0%)\n",
      "Epoch 27 Loss 0.007174 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0162, Accuracy: 0.0/100 (0%)\n",
      "Epoch 28 Loss 0.006858 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0162, Accuracy: 0.0/100 (0%)\n",
      "Epoch 29 Loss 0.006075 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0155, Accuracy: 0.0/100 (0%)\n",
      "Epoch 30 Loss 0.005870 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0162, Accuracy: 0.0/100 (0%)\n",
      "Epoch 31 Loss 0.005707 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0153, Accuracy: 0.0/100 (0%)\n",
      "Epoch 32 Loss 0.005412 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0168, Accuracy: 0.0/100 (0%)\n",
      "Epoch 33 Loss 0.005344 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0158, Accuracy: 0.0/100 (0%)\n",
      "Epoch 34 Loss 0.005282 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0219, Accuracy: 0.0/100 (0%)\n",
      "Epoch 35 Loss 0.006244 acc : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0227, Accuracy: 0.0/100 (0%)\n",
      "Epoch 36 Loss 0.006282 acc : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0109, Accuracy: 0.0/100 (0%)\n",
      "Epoch 37 Loss 0.005244 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0101, Accuracy: 0.0/100 (0%)\n",
      "Epoch 38 Loss 0.004408 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0096, Accuracy: 0.0/100 (0%)\n",
      "Epoch 39 Loss 0.004021 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0104, Accuracy: 0.0/100 (0%)\n",
      "Epoch 40 Loss 0.003629 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0088, Accuracy: 0.0/100 (0%)\n",
      "Epoch 41 Loss 0.003393 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0088, Accuracy: 0.0/100 (0%)\n",
      "Epoch 42 Loss 0.003260 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0071, Accuracy: 0.0/100 (0%)\n",
      "Epoch 43 Loss 0.003567 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0112, Accuracy: 0.0/100 (0%)\n",
      "Epoch 44 Loss 0.003635 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0067, Accuracy: 0.0/100 (0%)\n",
      "Epoch 45 Loss 0.003053 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0060, Accuracy: 0.0/100 (0%)\n",
      "Epoch 46 Loss 0.003378 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0107, Accuracy: 0.0/100 (0%)\n",
      "Epoch 47 Loss 0.003297 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0056, Accuracy: 0.0/100 (0%)\n",
      "Epoch 48 Loss 0.003557 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0067, Accuracy: 0.0/100 (0%)\n",
      "Epoch 49 Loss 0.003284 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0092, Accuracy: 0.0/100 (0%)\n",
      "Epoch 50 Loss 0.003520 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0053, Accuracy: 0.0/100 (0%)\n",
      "Epoch 51 Loss 0.002992 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0049, Accuracy: 0.0/100 (0%)\n",
      "Epoch 52 Loss 0.002828 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0090, Accuracy: 0.0/100 (0%)\n",
      "Epoch 53 Loss 0.003284 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0048, Accuracy: 0.0/100 (0%)\n",
      "Epoch 54 Loss 0.002739 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0060, Accuracy: 0.0/100 (0%)\n",
      "Epoch 55 Loss 0.003247 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0063, Accuracy: 0.0/100 (0%)\n",
      "Epoch 56 Loss 0.002976 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0059, Accuracy: 0.0/100 (0%)\n",
      "Epoch 57 Loss 0.003020 acc : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0046, Accuracy: 0.0/100 (0%)\n",
      "Epoch 58 Loss 0.002992 acc : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.0106, Accuracy: 0.0/100 (0%)\n",
      "Epoch 59 Loss 0.003543 acc : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.0081, Accuracy: 0.0/100 (0%)\n",
      "Epoch 60 Loss 0.003926 acc : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.0082, Accuracy: 0.0/100 (0%)\n",
      "Epoch 61 Loss 0.003065 acc : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.0053, Accuracy: 0.0/100 (0%)\n",
      "Epoch 62 Loss 0.003374 acc : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.0049, Accuracy: 0.0/100 (0%)\n",
      "Epoch 63 Loss 0.003156 acc : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.0087, Accuracy: 0.0/100 (0%)\n",
      "Epoch 64 Loss 0.003297 acc : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.0064, Accuracy: 0.0/100 (0%)\n",
      "Epoch 65 Loss 0.003480 acc : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.0062, Accuracy: 0.0/100 (0%)\n",
      "Epoch 66 Loss 0.004166 acc : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.0171, Accuracy: 0.0/100 (0%)\n",
      "Epoch 67 Loss 0.004695 acc : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.0049, Accuracy: 0.0/100 (0%)\n",
      "Epoch 68 Loss 0.003258 acc : 0.000000 stop count : 10"
     ]
    }
   ],
   "source": [
    "from functions.training import Early_stop_train\n",
    "\n",
    "train_seq = Early_stop_train(model_MLP,optimizer_MLP,criterion)\n",
    "\n",
    "train_seq.train_model(train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9624, Accuracy: 0.0/100 (0%)\n",
      "Epoch 1 Loss 0.354825 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.6017, Accuracy: 0.0/100 (0%)\n",
      "Epoch 2 Loss 0.229636 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.4565, Accuracy: 0.0/100 (0%)\n",
      "Epoch 3 Loss 0.200533 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.4135, Accuracy: 0.0/100 (0%)\n",
      "Epoch 4 Loss 0.187401 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.4004, Accuracy: 0.0/100 (0%)\n",
      "Epoch 5 Loss 0.188833 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3952, Accuracy: 0.0/100 (0%)\n",
      "Epoch 6 Loss 0.182674 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3932, Accuracy: 0.0/100 (0%)\n",
      "Epoch 7 Loss 0.188731 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3919, Accuracy: 0.0/100 (0%)\n",
      "Epoch 8 Loss 0.190261 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3911, Accuracy: 0.0/100 (0%)\n",
      "Epoch 9 Loss 0.183973 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3900, Accuracy: 0.0/100 (0%)\n",
      "Epoch 10 Loss 0.183471 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3885, Accuracy: 0.0/100 (0%)\n",
      "Epoch 11 Loss 0.182851 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3858, Accuracy: 0.0/100 (0%)\n",
      "Epoch 12 Loss 0.184937 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3793, Accuracy: 0.0/100 (0%)\n",
      "Epoch 13 Loss 0.176720 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3689, Accuracy: 0.0/100 (0%)\n",
      "Epoch 14 Loss 0.156077 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3377, Accuracy: 0.0/100 (0%)\n",
      "Epoch 15 Loss 0.139501 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.3050, Accuracy: 0.0/100 (0%)\n",
      "Epoch 16 Loss 0.111112 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2773, Accuracy: 0.0/100 (0%)\n",
      "Epoch 17 Loss 0.110784 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2631, Accuracy: 0.0/100 (0%)\n",
      "Epoch 18 Loss 0.109787 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2606, Accuracy: 0.0/100 (0%)\n",
      "Epoch 19 Loss 0.099748 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2572, Accuracy: 0.0/100 (0%)\n",
      "Epoch 20 Loss 0.099554 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2593, Accuracy: 0.0/100 (0%)\n",
      "Epoch 21 Loss 0.098546 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2549, Accuracy: 0.0/100 (0%)\n",
      "Epoch 22 Loss 0.102863 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2570, Accuracy: 0.0/100 (0%)\n",
      "Epoch 23 Loss 0.108682 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2541, Accuracy: 0.0/100 (0%)\n",
      "Epoch 24 Loss 0.102128 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2555, Accuracy: 0.0/100 (0%)\n",
      "Epoch 25 Loss 0.107636 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2540, Accuracy: 0.0/100 (0%)\n",
      "Epoch 26 Loss 0.101853 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2540, Accuracy: 0.0/100 (0%)\n",
      "Epoch 27 Loss 0.098897 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2541, Accuracy: 0.0/100 (0%)\n",
      "Epoch 28 Loss 0.087555 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2541, Accuracy: 0.0/100 (0%)\n",
      "Epoch 29 Loss 0.098581 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2535, Accuracy: 0.0/100 (0%)\n",
      "Epoch 30 Loss 0.093711 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2542, Accuracy: 0.0/100 (0%)\n",
      "Epoch 31 Loss 0.089225 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2534, Accuracy: 0.0/100 (0%)\n",
      "Epoch 32 Loss 0.094247 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2549, Accuracy: 0.0/100 (0%)\n",
      "Epoch 33 Loss 0.091057 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2533, Accuracy: 0.0/100 (0%)\n",
      "Epoch 34 Loss 0.095199 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2528, Accuracy: 0.0/100 (0%)\n",
      "Epoch 35 Loss 0.093449 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2551, Accuracy: 0.0/100 (0%)\n",
      "Epoch 36 Loss 0.097163 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2536, Accuracy: 0.0/100 (0%)\n",
      "Epoch 37 Loss 0.102683 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2547, Accuracy: 0.0/100 (0%)\n",
      "Epoch 38 Loss 0.097902 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2527, Accuracy: 0.0/100 (0%)\n",
      "Epoch 39 Loss 0.093211 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2530, Accuracy: 0.0/100 (0%)\n",
      "Epoch 40 Loss 0.097964 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2536, Accuracy: 0.0/100 (0%)\n",
      "Epoch 41 Loss 0.091185 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2526, Accuracy: 0.0/100 (0%)\n",
      "Epoch 42 Loss 0.109998 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2533, Accuracy: 0.0/100 (0%)\n",
      "Epoch 43 Loss 0.087504 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2529, Accuracy: 0.0/100 (0%)\n",
      "Epoch 44 Loss 0.096497 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2526, Accuracy: 0.0/100 (0%)\n",
      "Epoch 45 Loss 0.093567 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2530, Accuracy: 0.0/100 (0%)\n",
      "Epoch 46 Loss 0.094665 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2543, Accuracy: 0.0/100 (0%)\n",
      "Epoch 47 Loss 0.089805 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2523, Accuracy: 0.0/100 (0%)\n",
      "Epoch 48 Loss 0.094244 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2527, Accuracy: 0.0/100 (0%)\n",
      "Epoch 49 Loss 0.089249 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2542, Accuracy: 0.0/100 (0%)\n",
      "Epoch 50 Loss 0.097657 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2526, Accuracy: 0.0/100 (0%)\n",
      "Epoch 51 Loss 0.086474 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2536, Accuracy: 0.0/100 (0%)\n",
      "Epoch 52 Loss 0.104775 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2537, Accuracy: 0.0/100 (0%)\n",
      "Epoch 53 Loss 0.099166 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 54 Loss 0.099587 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2531, Accuracy: 0.0/100 (0%)\n",
      "Epoch 55 Loss 0.090040 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.2527, Accuracy: 0.0/100 (0%)\n",
      "Epoch 56 Loss 0.092640 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.2521, Accuracy: 0.0/100 (0%)\n",
      "Epoch 57 Loss 0.095484 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2524, Accuracy: 0.0/100 (0%)\n",
      "Epoch 58 Loss 0.096525 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2529, Accuracy: 0.0/100 (0%)\n",
      "Epoch 59 Loss 0.093810 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2518, Accuracy: 0.0/100 (0%)\n",
      "Epoch 60 Loss 0.097248 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2536, Accuracy: 0.0/100 (0%)\n",
      "Epoch 61 Loss 0.091326 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2520, Accuracy: 0.0/100 (0%)\n",
      "Epoch 62 Loss 0.090002 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2521, Accuracy: 0.0/100 (0%)\n",
      "Epoch 63 Loss 0.095542 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2530, Accuracy: 0.0/100 (0%)\n",
      "Epoch 64 Loss 0.088616 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2519, Accuracy: 0.0/100 (0%)\n",
      "Epoch 65 Loss 0.092829 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 66 Loss 0.087901 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2523, Accuracy: 0.0/100 (0%)\n",
      "Epoch 67 Loss 0.091725 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.2528, Accuracy: 0.0/100 (0%)\n",
      "Epoch 68 Loss 0.094146 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 69 Loss 0.095047 acc : 0.000000 reg : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.2518, Accuracy: 0.0/100 (0%)\n",
      "Epoch 70 Loss 0.086463 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2530, Accuracy: 0.0/100 (0%)\n",
      "Epoch 71 Loss 0.095502 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2520, Accuracy: 0.0/100 (0%)\n",
      "Epoch 72 Loss 0.101247 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2553, Accuracy: 0.0/100 (0%)\n",
      "Epoch 73 Loss 0.096089 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2518, Accuracy: 0.0/100 (0%)\n",
      "Epoch 74 Loss 0.093340 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2521, Accuracy: 0.0/100 (0%)\n",
      "Epoch 75 Loss 0.102834 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 76 Loss 0.094972 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2518, Accuracy: 0.0/100 (0%)\n",
      "Epoch 77 Loss 0.094419 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2544, Accuracy: 0.0/100 (0%)\n",
      "Epoch 78 Loss 0.097537 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2519, Accuracy: 0.0/100 (0%)\n",
      "Epoch 79 Loss 0.094437 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2523, Accuracy: 0.0/100 (0%)\n",
      "Epoch 80 Loss 0.090989 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2515, Accuracy: 0.0/100 (0%)\n",
      "Epoch 81 Loss 0.089153 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2519, Accuracy: 0.0/100 (0%)\n",
      "Epoch 82 Loss 0.094241 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2522, Accuracy: 0.0/100 (0%)\n",
      "Epoch 83 Loss 0.091447 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2524, Accuracy: 0.0/100 (0%)\n",
      "Epoch 84 Loss 0.095147 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2528, Accuracy: 0.0/100 (0%)\n",
      "Epoch 85 Loss 0.091792 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2522, Accuracy: 0.0/100 (0%)\n",
      "Epoch 86 Loss 0.096592 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 87 Loss 0.093005 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2513, Accuracy: 0.0/100 (0%)\n",
      "Epoch 88 Loss 0.094882 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 89 Loss 0.088761 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2543, Accuracy: 0.0/100 (0%)\n",
      "Epoch 90 Loss 0.092745 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2514, Accuracy: 0.0/100 (0%)\n",
      "Epoch 91 Loss 0.094223 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2528, Accuracy: 0.0/100 (0%)\n",
      "Epoch 92 Loss 0.090662 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2523, Accuracy: 0.0/100 (0%)\n",
      "Epoch 93 Loss 0.095967 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2517, Accuracy: 0.0/100 (0%)\n",
      "Epoch 94 Loss 0.101833 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 95 Loss 0.098857 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 96 Loss 0.094610 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 97 Loss 0.092111 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 98 Loss 0.092956 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2525, Accuracy: 0.0/100 (0%)\n",
      "Epoch 99 Loss 0.102333 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2513, Accuracy: 0.0/100 (0%)\n",
      "Epoch 100 Loss 0.093072 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2544, Accuracy: 0.0/100 (0%)\n",
      "Epoch 101 Loss 0.095423 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2520, Accuracy: 0.0/100 (0%)\n",
      "Epoch 102 Loss 0.089993 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2534, Accuracy: 0.0/100 (0%)\n",
      "Epoch 103 Loss 0.096678 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2527, Accuracy: 0.0/100 (0%)\n",
      "Epoch 104 Loss 0.095626 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2515, Accuracy: 0.0/100 (0%)\n",
      "Epoch 105 Loss 0.088738 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.2536, Accuracy: 0.0/100 (0%)\n",
      "Epoch 106 Loss 0.091202 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.2513, Accuracy: 0.0/100 (0%)\n",
      "Epoch 107 Loss 0.090218 acc : 0.000000 reg : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.2508, Accuracy: 0.0/100 (0%)\n",
      "Epoch 108 Loss 0.102035 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2514, Accuracy: 0.0/100 (0%)\n",
      "Epoch 109 Loss 0.102987 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2508, Accuracy: 0.0/100 (0%)\n",
      "Epoch 110 Loss 0.093020 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2509, Accuracy: 0.0/100 (0%)\n",
      "Epoch 111 Loss 0.095344 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2538, Accuracy: 0.0/100 (0%)\n",
      "Epoch 112 Loss 0.094876 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2511, Accuracy: 0.0/100 (0%)\n",
      "Epoch 113 Loss 0.099470 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2509, Accuracy: 0.0/100 (0%)\n",
      "Epoch 114 Loss 0.099823 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2539, Accuracy: 0.0/100 (0%)\n",
      "Epoch 115 Loss 0.096224 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2537, Accuracy: 0.0/100 (0%)\n",
      "Epoch 116 Loss 0.090679 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2576, Accuracy: 0.0/100 (0%)\n",
      "Epoch 117 Loss 0.093888 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.2506, Accuracy: 0.0/100 (0%)\n",
      "Epoch 118 Loss 0.095726 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2506, Accuracy: 0.0/100 (0%)\n",
      "Epoch 119 Loss 0.092359 acc : 0.000000 reg : 0.000000 stop count : 0\n",
      "Test set: Average loss: 0.2529, Accuracy: 0.0/100 (0%)\n",
      "Epoch 120 Loss 0.088652 acc : 0.000000 reg : 0.000000 stop count : 1\n",
      "Test set: Average loss: 0.2508, Accuracy: 0.0/100 (0%)\n",
      "Epoch 121 Loss 0.090630 acc : 0.000000 reg : 0.000000 stop count : 2\n",
      "Test set: Average loss: 0.2515, Accuracy: 0.0/100 (0%)\n",
      "Epoch 122 Loss 0.090242 acc : 0.000000 reg : 0.000000 stop count : 3\n",
      "Test set: Average loss: 0.2522, Accuracy: 0.0/100 (0%)\n",
      "Epoch 123 Loss 0.090711 acc : 0.000000 reg : 0.000000 stop count : 4\n",
      "Test set: Average loss: 0.2508, Accuracy: 0.0/100 (0%)\n",
      "Epoch 124 Loss 0.099420 acc : 0.000000 reg : 0.000000 stop count : 5\n",
      "Test set: Average loss: 0.2507, Accuracy: 0.0/100 (0%)\n",
      "Epoch 125 Loss 0.096067 acc : 0.000000 reg : 0.000000 stop count : 6\n",
      "Test set: Average loss: 0.2517, Accuracy: 0.0/100 (0%)\n",
      "Epoch 126 Loss 0.091826 acc : 0.000000 reg : 0.000000 stop count : 7\n",
      "Test set: Average loss: 0.2507, Accuracy: 0.0/100 (0%)\n",
      "Epoch 127 Loss 0.096926 acc : 0.000000 reg : 0.000000 stop count : 8\n",
      "Test set: Average loss: 0.2517, Accuracy: 0.0/100 (0%)\n",
      "Epoch 128 Loss 0.088443 acc : 0.000000 reg : 0.000000 stop count : 9\n",
      "Test set: Average loss: 0.2510, Accuracy: 0.0/100 (0%)\n",
      "Epoch 129 Loss 0.090810 acc : 0.000000 reg : 0.000000 stop count : 10"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 모델, 손실 함수, 옵티마이저 초기화\\nmodel = RegressionModel()\\ncriterion = nn.MSELoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\\n\\n# 훈련 및 검증 루프\\nnum_epochs = 5000\\nfor epoch in range(num_epochs):\\n    model.train()\\n    train_loss = 0\\n    for inputs, labels in train_loader:\\n        optimizer.zero_grad()\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n        train_loss += loss.item()\\n    \\n    train_loss /= len(train_loader)\\n    \\n    model.eval()\\n    val_loss = 0\\n    with torch.no_grad():\\n        for inputs_val, labels_val in val_loader:\\n            outputs = model(inputs_val)\\n            loss = criterion(outputs, labels_val)\\n            val_loss += loss.item()\\n    \\n    val_loss /= len(val_loader)\\n    \\n    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")'"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "# 데이터셋 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 데이터 로더\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "from functions.training import Early_stop_train_KAN\n",
    "\n",
    "train_seq = Early_stop_train_KAN(model,optimizer,criterion)\n",
    "\n",
    "train_seq.train_model(train_loader,val_loader)\n",
    "\n",
    "\"\"\"\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = RegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 훈련 및 검증 루프\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, labels_val in val_loader:\n",
    "            outputs = model(inputs_val)\n",
    "            loss = criterion(outputs, labels_val)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pad33\\anaconda3\\envs\\KAN\\lib\\site-packages\\kan\\KAN.py:327: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  self.acts_scale_std.append(torch.std(postacts, dim=0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP ZEN dist : 0.14955700933933258\n",
      "RAW dist : 0.1532641053199768\n",
      "Line dist : 0.16159355640411377\n",
      "MLP ZEN dist : 0.10771334171295166\n",
      "RAW dist : 0.18776066601276398\n",
      "Line dist : 0.1903919279575348\n",
      "MLP ZEN dist : 0.052278101444244385\n",
      "RAW dist : 0.3093665838241577\n",
      "Line dist : 0.32062798738479614\n",
      "MLP ZEN dist : 0.05065947771072388\n",
      "RAW dist : 0.19946980476379395\n",
      "Line dist : 0.2069738656282425\n",
      "MLP ZEN dist : 0.05468130111694336\n",
      "RAW dist : 0.36115068197250366\n",
      "Line dist : 0.37305277585983276\n",
      "MLP ZEN dist : 0.12824797630310059\n",
      "RAW dist : 0.02719835191965103\n",
      "Line dist : 0.03183937072753906\n",
      "MLP ZEN dist : 0.03492026776075363\n",
      "RAW dist : 0.049839843064546585\n",
      "Line dist : 0.05169253051280975\n",
      "MLP ZEN dist : 0.11827421188354492\n",
      "RAW dist : 0.41442620754241943\n",
      "Line dist : 0.43196535110473633\n",
      "MLP ZEN dist : 0.17221030592918396\n",
      "RAW dist : 0.3034900426864624\n",
      "Line dist : 0.3105599880218506\n",
      "MLP ZEN dist : 0.11819145083427429\n",
      "RAW dist : 0.4393290579319\n",
      "Line dist : 0.45247897505760193\n",
      "MLP ZEN dist : 0.032307446002960205\n",
      "RAW dist : 0.11609996110200882\n",
      "Line dist : 0.12030977755784988\n",
      "MLP ZEN dist : 0.1427748203277588\n",
      "RAW dist : 0.23731189966201782\n",
      "Line dist : 0.24260038137435913\n",
      "MLP ZEN dist : 0.0909387469291687\n",
      "RAW dist : 0.16927671432495117\n",
      "Line dist : 0.17632928490638733\n",
      "MLP ZEN dist : 0.024570494890213013\n",
      "RAW dist : 0.3109288215637207\n",
      "Line dist : 0.3218638598918915\n",
      "MLP ZEN dist : 0.04563826322555542\n",
      "RAW dist : 0.603407621383667\n",
      "Line dist : 0.6252816915512085\n",
      "MLP ZEN dist : 0.10334193706512451\n",
      "RAW dist : 0.6374573707580566\n",
      "Line dist : 0.656478762626648\n",
      "MLP ZEN dist : 0.08158594369888306\n",
      "RAW dist : 0.5568429231643677\n",
      "Line dist : 0.5760285258293152\n",
      "MLP ZEN dist : 0.046204566955566406\n",
      "RAW dist : 0.09361881017684937\n",
      "Line dist : 0.09660457074642181\n",
      "MLP ZEN dist : 0.09904465079307556\n",
      "RAW dist : 0.3450169563293457\n",
      "Line dist : 0.3538544476032257\n",
      "MLP ZEN dist : 0.0798335075378418\n",
      "RAW dist : 0.2616311013698578\n",
      "Line dist : 0.2693658173084259\n",
      "MLP ZEN dist : 0.11823352426290512\n",
      "RAW dist : 0.06850054860115051\n",
      "Line dist : 0.07282914966344833\n",
      "MLP ZEN dist : 0.021610617637634277\n",
      "RAW dist : 0.03946840018033981\n",
      "Line dist : 0.04224691540002823\n",
      "MLP ZEN dist : 0.06837934255599976\n",
      "RAW dist : 0.37028399109840393\n",
      "Line dist : 0.3800749182701111\n",
      "MLP ZEN dist : 0.009640395641326904\n",
      "RAW dist : 0.34477320313453674\n",
      "Line dist : 0.3577074110507965\n",
      "MLP ZEN dist : 0.06327062845230103\n",
      "RAW dist : 0.2639192044734955\n",
      "Line dist : 0.270005464553833\n",
      "MLP ZEN dist : 0.14223068952560425\n",
      "RAW dist : 0.2531580924987793\n",
      "Line dist : 0.2587539553642273\n",
      "MLP ZEN dist : 0.10185322910547256\n",
      "RAW dist : 0.08920112997293472\n",
      "Line dist : 0.09090933203697205\n",
      "MLP ZEN dist : 0.01770263910293579\n",
      "RAW dist : 0.4502626061439514\n",
      "Line dist : 0.46750590205192566\n",
      "MLP ZEN dist : 0.06925658881664276\n",
      "RAW dist : 0.17501692473888397\n",
      "Line dist : 0.180092915892601\n",
      "MLP ZEN dist : 0.13174480199813843\n",
      "RAW dist : 0.5209341645240784\n",
      "Line dist : 0.5362905859947205\n",
      "MLP ZEN dist : 0.09179827570915222\n",
      "RAW dist : 0.37445253133773804\n",
      "Line dist : 0.38663429021835327\n",
      "MLP ZEN dist : 0.05678778886795044\n",
      "RAW dist : 0.25808730721473694\n",
      "Line dist : 0.2680938243865967\n",
      "MLP ZEN dist : 0.08863338828086853\n",
      "RAW dist : 0.3519994914531708\n",
      "Line dist : 0.3608737587928772\n",
      "MLP ZEN dist : 0.02260488271713257\n",
      "RAW dist : 0.4676644802093506\n",
      "Line dist : 0.48400768637657166\n",
      "MLP ZEN dist : 0.05629312992095947\n",
      "RAW dist : 0.3816167116165161\n",
      "Line dist : 0.3946557641029358\n",
      "MLP ZEN dist : 0.12667429447174072\n",
      "RAW dist : 0.142200767993927\n",
      "Line dist : 0.14458292722702026\n"
     ]
    }
   ],
   "source": [
    "res_MLP = []\n",
    "res_RAW = []\n",
    "res_line_list = []\n",
    "\n",
    "for i in range(36):\n",
    "    \n",
    "    print(f'MLP ZEN dist : {abs(model(inputs_val[i,None]).item()-labels_val[i])}')\n",
    "    res_MLP.append(abs(model(inputs_val[i,None]).item()-labels_val[i]))\n",
    "    print(f'RAW dist : {abs(inputs_val[i][0]-labels_val[i])}')\n",
    "    res_RAW.append(abs(inputs_val[i][0]-labels_val[i]))\n",
    "    res_line = np.polyfit([1,3,5,7], inputs_val[i], 1)[1]\n",
    "    print(f'Line dist : {abs(res_line.item()-labels_val[i])}')\n",
    "    res_line_list.append(abs(res_line.item()-labels_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.08110244, 0.28690073, 0.29625443)"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(res_MLP),np.mean(res_RAW),np.mean(res_line_list)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
