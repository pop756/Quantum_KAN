{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane\n",
        "!pip install pykan\n",
        "!git clone https://github.com/pop756/Quantum_machine.git\n",
        "%cd Quantum_machine"
      ],
      "metadata": {
        "id": "kP7p3x26kiZ2",
        "outputId": "ffe9c30b-83e8-4e63-9d7b-8837392664ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.14.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.6.9)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
            "Requirement already satisfied: pennylane-lightning>=0.36 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.11.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
            "Requirement already satisfied: pykan in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Cloning into 'Quantum_machine'...\n",
            "remote: Enumerating objects: 481, done.\u001b[K\n",
            "remote: Counting objects: 100% (481/481), done.\u001b[K\n",
            "remote: Compressing objects: 100% (469/469), done.\u001b[K\n",
            "remote: Total 481 (delta 26), reused 457 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (481/481), 8.95 MiB | 26.88 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/Quantum_machine/Quantum_machine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "vJl4rYpKBAEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "PCA_dim = 8\n",
        "CLS_num = 2\n",
        "\n",
        "\n",
        "\n",
        "with open('./data.pkl','rb') as file:\n",
        "    data = pickle.load(file)\n",
        "X = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "def Fit_to_quantum(X,PCA_dim):\n",
        "    pca = PCA(n_components=PCA_dim)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "class Feature_data_loader(Dataset):\n",
        "    def __init__(self,x_train,y_train):\n",
        "        self.feature1 = x_train\n",
        "        temp = copy.deepcopy(x_train)\n",
        "        shuffle = torch.randperm(len(temp))\n",
        "        self.feature2 = temp[shuffle]\n",
        "        self.y1 = y_train\n",
        "        temp_y = copy.deepcopy(y_train)\n",
        "        self.y2 = temp_y[shuffle]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature1)\n",
        "    def __getitem__(self,idx):\n",
        "        input1 = self.feature1[idx]\n",
        "        input2 = self.feature2[idx]\n",
        "        if self.y1[idx] == self.y2[idx]:\n",
        "            label = torch.tensor(1.).float()\n",
        "        else:\n",
        "            label = torch.tensor(0.).float()\n",
        "        return [input1,input2],label\n",
        "\n",
        "\n",
        "# DataLoader 생성\n",
        "\n",
        "\n",
        "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
        "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
        "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "N4Y4dnYJBAEt"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred, true):\n",
        "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
        "    pred = pred.detach().cpu()\n",
        "    true = true.cpu()\n",
        "    try:\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "    except:\n",
        "        pred_labels = torch.round(pred)\n",
        "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
        "    correct = (pred_labels == true).sum()\n",
        "    # 정확도를 계산\n",
        "    acc = correct / true.size(0)\n",
        "    return acc.item()\n",
        "\n",
        "class Early_stop_train():\n",
        "    def __init__(self,model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "\n",
        "        self.loss_list = [1e100]\n",
        "        self.stop_count = 0\n",
        "\n",
        "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            if self.stop_count>=res:\n",
        "                break\n",
        "            loss_val,_ = self.test(test_loader)\n",
        "            self.loss_list.append(loss_val)\n",
        "\n",
        "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
        "                self.stop_count+=1\n",
        "            else:\n",
        "                self.stop_count = 0\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "            for X_train,y_train in train_loader:\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(X_train)\n",
        "\n",
        "                loss = self.criterion(output.squeeze(), y_train)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_list.append(loss.item())\n",
        "                acc = accuracy(output,y_train)\n",
        "                acc_list.append(acc)\n",
        "\n",
        "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} stop count : {self.stop_count}\")\n",
        "\n",
        "\n",
        "    def test(self,test_loader):\n",
        "        if test_loader is None:\n",
        "            return 0,0\n",
        "        else:\n",
        "            #self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in test_loader:\n",
        "                    data, target = data, target\n",
        "                    output = self.model(data)\n",
        "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
        "\n",
        "                    correct += accuracy(output,target)*len(output)\n",
        "\n",
        "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "            return test_loss,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3d3GfMBAEv",
        "outputId": "dce7ef57-3f8c-4efd-a9f9-b7b100cc40b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 12.2966, Accuracy: 153.99999904632568/300 (51%)\n",
            "Epoch 1 Loss 1.682097 acc : 0.491572 stop count : 0\n",
            "Test set: Average loss: 5.2292, Accuracy: 147.0/300 (49%)\n",
            "Epoch 2 Loss 0.869834 acc : 0.544981 stop count : 0\n",
            "Test set: Average loss: 4.4403, Accuracy: 168.00000047683716/300 (56%)\n",
            "Epoch 3 Loss 0.753308 acc : 0.607292 stop count : 0\n",
            "Test set: Average loss: 3.8160, Accuracy: 174.99999952316284/300 (58%)\n",
            "Epoch 4 Loss 0.683022 acc : 0.611458 stop count : 0\n",
            "Test set: Average loss: 3.4449, Accuracy: 183.99999928474426/300 (61%)\n",
            "Epoch 5 Loss 0.635906 acc : 0.648106 stop count : 0\n",
            "Test set: Average loss: 3.1995, Accuracy: 200.00000023841858/300 (67%)\n",
            "Epoch 6 Loss 0.580724 acc : 0.686837 stop count : 0\n",
            "Test set: Average loss: 3.0139, Accuracy: 212.00000023841858/300 (71%)\n",
            "Epoch 7 Loss 0.540716 acc : 0.713068 stop count : 0\n",
            "Test set: Average loss: 2.8277, Accuracy: 213.00000095367432/300 (71%)\n",
            "Epoch 8 Loss 0.514822 acc : 0.732765 stop count : 0\n",
            "Test set: Average loss: 2.6826, Accuracy: 218.00000023841858/300 (73%)\n",
            "Epoch 9 Loss 0.492053 acc : 0.745928 stop count : 0\n",
            "Test set: Average loss: 2.6286, Accuracy: 219.99999928474426/300 (73%)\n",
            "Epoch 10 Loss 0.474526 acc : 0.754261 stop count : 0\n",
            "Test set: Average loss: 2.5752, Accuracy: 221.99999928474426/300 (74%)\n",
            "Epoch 11 Loss 0.467558 acc : 0.764110 stop count : 0\n",
            "Test set: Average loss: 2.5328, Accuracy: 223.00000095367432/300 (74%)\n",
            "Epoch 12 Loss 0.457190 acc : 0.767140 stop count : 0\n",
            "Test set: Average loss: 2.4869, Accuracy: 221.99999928474426/300 (74%)\n",
            "Epoch 13 Loss 0.439019 acc : 0.778788 stop count : 0\n",
            "Test set: Average loss: 2.4839, Accuracy: 224.00000095367432/300 (75%)\n",
            "Epoch 14 Loss 0.439389 acc : 0.770928 stop count : 0\n",
            "Test set: Average loss: 2.4436, Accuracy: 224.0/300 (75%)\n",
            "Epoch 15 Loss 0.432668 acc : 0.779735 stop count : 0\n",
            "Test set: Average loss: 2.4427, Accuracy: 223.00000095367432/300 (74%)\n",
            "Epoch 16 Loss 0.427737 acc : 0.790814 stop count : 0\n",
            "Test set: Average loss: 2.4437, Accuracy: 225.00000071525574/300 (75%)\n",
            "Epoch 17 Loss 0.413013 acc : 0.781439 stop count : 1\n",
            "Test set: Average loss: 2.4466, Accuracy: 225.0/300 (75%)\n",
            "Epoch 18 Loss 0.412440 acc : 0.796875 stop count : 2\n",
            "Test set: Average loss: 2.4452, Accuracy: 234.0/300 (78%)\n",
            "Epoch 19 Loss 0.406212 acc : 0.791288 stop count : 3\n",
            "Test set: Average loss: 2.4623, Accuracy: 227.0/300 (76%)\n",
            "Epoch 20 Loss 0.392523 acc : 0.795549 stop count : 4\n",
            "Test set: Average loss: 2.3970, Accuracy: 227.0/300 (76%)\n",
            "Epoch 21 Loss 0.379461 acc : 0.814110 stop count : 0\n",
            "Test set: Average loss: 2.4163, Accuracy: 230.0/300 (77%)\n",
            "Epoch 22 Loss 0.371534 acc : 0.808428 stop count : 1\n",
            "Test set: Average loss: 2.3942, Accuracy: 228.0/300 (76%)\n",
            "Epoch 23 Loss 0.363717 acc : 0.824242 stop count : 0\n",
            "Test set: Average loss: 2.3549, Accuracy: 230.0/300 (77%)\n",
            "Epoch 24 Loss 0.356108 acc : 0.827367 stop count : 0\n",
            "Test set: Average loss: 2.4663, Accuracy: 227.0/300 (76%)\n",
            "Epoch 25 Loss 0.356356 acc : 0.824242 stop count : 1\n",
            "Test set: Average loss: 2.4339, Accuracy: 230.0/300 (77%)\n",
            "Epoch 26 Loss 0.342372 acc : 0.837311 stop count : 2\n",
            "Test set: Average loss: 2.4594, Accuracy: 232.0/300 (77%)\n",
            "Epoch 27 Loss 0.329740 acc : 0.844413 stop count : 3\n",
            "Test set: Average loss: 2.3894, Accuracy: 230.0/300 (77%)\n",
            "Epoch 28 Loss 0.330076 acc : 0.834280 stop count : 4\n",
            "Test set: Average loss: 2.4855, Accuracy: 231.0/300 (77%)\n",
            "Epoch 29 Loss 0.329079 acc : 0.831723 stop count : 5\n",
            "Test set: Average loss: 2.5659, Accuracy: 229.0/300 (76%)\n",
            "Epoch 30 Loss 0.316222 acc : 0.851799 stop count : 6\n",
            "Test set: Average loss: 2.4931, Accuracy: 231.0/300 (77%)\n",
            "Epoch 31 Loss 0.311560 acc : 0.848958 stop count : 7\n",
            "Test set: Average loss: 2.4564, Accuracy: 233.0/300 (78%)\n",
            "Epoch 32 Loss 0.310154 acc : 0.865530 stop count : 8\n",
            "Test set: Average loss: 2.5356, Accuracy: 239.00000071525574/300 (80%)\n",
            "Epoch 33 Loss 0.296948 acc : 0.850000 stop count : 9\n",
            "Test set: Average loss: 2.4811, Accuracy: 230.0/300 (77%)\n",
            "Epoch 34 Loss 0.297066 acc : 0.857197 stop count : 10\n",
            "Test set: Average loss: 2.3623, Accuracy: 237.00000071525574/300 (79%)\n",
            "Epoch 35 Loss 0.301975 acc : 0.875852 stop count : 11\n",
            "Test set: Average loss: 2.3808, Accuracy: 236.9999988079071/300 (79%)\n",
            "Epoch 36 Loss 0.322858 acc : 0.856818 stop count : 12\n",
            "Test set: Average loss: 2.6059, Accuracy: 239.99999976158142/300 (80%)\n",
            "Epoch 37 Loss 0.293142 acc : 0.873011 stop count : 13\n",
            "Test set: Average loss: 2.2884, Accuracy: 240.9999988079071/300 (80%)\n",
            "Epoch 38 Loss 0.277391 acc : 0.878314 stop count : 0\n",
            "Test set: Average loss: 2.4994, Accuracy: 240.9999988079071/300 (80%)\n",
            "Epoch 39 Loss 0.277782 acc : 0.883049 stop count : 1\n",
            "Test set: Average loss: 2.2860, Accuracy: 239.99999976158142/300 (80%)\n",
            "Epoch 40 Loss 0.271083 acc : 0.886837 stop count : 0\n",
            "Test set: Average loss: 2.5734, Accuracy: 241.00000047683716/300 (80%)\n",
            "Epoch 41 Loss 0.255161 acc : 0.894223 stop count : 1\n",
            "Test set: Average loss: 2.4738, Accuracy: 240.9999988079071/300 (80%)\n",
            "Epoch 42 Loss 0.251182 acc : 0.885701 stop count : 2\n",
            "Test set: Average loss: 2.2804, Accuracy: 241.00000047683716/300 (80%)\n",
            "Epoch 43 Loss 0.251438 acc : 0.894318 stop count : 0\n",
            "Test set: Average loss: 2.3264, Accuracy: 245.9999988079071/300 (82%)\n",
            "Epoch 44 Loss 0.244706 acc : 0.901136 stop count : 1\n",
            "Test set: Average loss: 2.4082, Accuracy: 242.9999988079071/300 (81%)\n",
            "Epoch 45 Loss 0.236644 acc : 0.897348 stop count : 2\n",
            "Test set: Average loss: 2.3920, Accuracy: 239.99999976158142/300 (80%)\n",
            "Epoch 46 Loss 0.244753 acc : 0.898580 stop count : 3\n",
            "Test set: Average loss: 2.3730, Accuracy: 245.00000047683716/300 (82%)\n",
            "Epoch 47 Loss 0.234487 acc : 0.897254 stop count : 4\n",
            "Test set: Average loss: 2.6896, Accuracy: 237.00000047683716/300 (79%)\n",
            "Epoch 48 Loss 0.250064 acc : 0.894318 stop count : 5\n",
            "Test set: Average loss: 2.2315, Accuracy: 245.00000047683716/300 (82%)\n",
            "Epoch 49 Loss 0.228235 acc : 0.908617 stop count : 0\n",
            "Test set: Average loss: 2.2742, Accuracy: 241.9999988079071/300 (81%)\n",
            "Epoch 50 Loss 0.222432 acc : 0.908523 stop count : 1\n",
            "\n",
            " Test start \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 3.8670, Accuracy: 113.99999988079071/300 (38%)\n",
            "Epoch 1 Loss 0.665669 acc : 0.630398 stop count : 0\n",
            "Test set: Average loss: 2.7574, Accuracy: 279.0000002384186/300 (93%)\n",
            "Epoch 2 Loss 0.447840 acc : 0.957008 stop count : 0\n",
            "Test set: Average loss: 1.9493, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 3 Loss 0.310415 acc : 0.940057 stop count : 0\n",
            "Test set: Average loss: 1.5682, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 4 Loss 0.248329 acc : 0.945644 stop count : 0\n",
            "Test set: Average loss: 1.4119, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 5 Loss 0.217384 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.3491, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 6 Loss 0.203682 acc : 0.945833 stop count : 0\n",
            "Test set: Average loss: 1.3166, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 7 Loss 0.195112 acc : 0.944602 stop count : 0\n",
            "Test set: Average loss: 1.2968, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 8 Loss 0.190444 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.2871, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 9 Loss 0.185748 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.2791, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 10 Loss 0.182699 acc : 0.945549 stop count : 0\n",
            "Test set: Average loss: 1.2772, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 11 Loss 0.178446 acc : 0.947443 stop count : 0\n",
            "Test set: Average loss: 1.2703, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 12 Loss 0.175399 acc : 0.944129 stop count : 0\n",
            "Test set: Average loss: 1.2608, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 13 Loss 0.173258 acc : 0.942803 stop count : 0\n",
            "Test set: Average loss: 1.2428, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 14 Loss 0.170695 acc : 0.944318 stop count : 0\n",
            "Test set: Average loss: 1.2381, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 15 Loss 0.168875 acc : 0.944223 stop count : 0\n",
            "Test set: Average loss: 1.2290, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 16 Loss 0.167433 acc : 0.944223 stop count : 0\n",
            "Test set: Average loss: 1.2107, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 17 Loss 0.165578 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.1922, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 18 Loss 0.165054 acc : 0.945549 stop count : 0\n",
            "Test set: Average loss: 1.1812, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 19 Loss 0.162807 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.1678, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 20 Loss 0.161219 acc : 0.945644 stop count : 0\n",
            "Test set: Average loss: 1.1640, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 21 Loss 0.160910 acc : 0.945644 stop count : 0\n",
            "Test set: Average loss: 1.1633, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 22 Loss 0.159986 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.1532, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 23 Loss 0.159800 acc : 0.945549 stop count : 0\n",
            "Test set: Average loss: 1.1509, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 24 Loss 0.158661 acc : 0.947159 stop count : 0\n",
            "Test set: Average loss: 1.1486, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 25 Loss 0.158851 acc : 0.948485 stop count : 0\n",
            "Test set: Average loss: 1.1439, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 26 Loss 0.158211 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.1438, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 27 Loss 0.158361 acc : 0.944129 stop count : 0\n",
            "Test set: Average loss: 1.1436, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 28 Loss 0.158501 acc : 0.945549 stop count : 0\n",
            "Test set: Average loss: 1.1436, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 29 Loss 0.157998 acc : 0.945549 stop count : 0\n",
            "Test set: Average loss: 1.1390, Accuracy: 281.99999928474426/300 (94%)\n",
            "Epoch 30 Loss 0.158230 acc : 0.946970 stop count : 0\n",
            "Test set: Average loss: 1.1367, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 31 Loss 0.157855 acc : 0.944602 stop count : 0\n",
            "Test set: Average loss: 1.1406, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 32 Loss 0.157935 acc : 0.943939 stop count : 1\n",
            "Test set: Average loss: 1.1375, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 33 Loss 0.157081 acc : 0.945739 stop count : 2\n",
            "Test set: Average loss: 1.1361, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 34 Loss 0.157528 acc : 0.945455 stop count : 0\n",
            "Test set: Average loss: 1.1352, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 35 Loss 0.156610 acc : 0.944413 stop count : 0\n",
            "Test set: Average loss: 1.1320, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 36 Loss 0.157894 acc : 0.943750 stop count : 0\n",
            "Test set: Average loss: 1.1300, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 37 Loss 0.156820 acc : 0.947254 stop count : 0\n",
            "Test set: Average loss: 1.1244, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 38 Loss 0.156526 acc : 0.944223 stop count : 0\n",
            "Test set: Average loss: 1.1276, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 39 Loss 0.156201 acc : 0.944223 stop count : 1\n",
            "Test set: Average loss: 1.1240, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 40 Loss 0.155973 acc : 0.944318 stop count : 0\n",
            "Test set: Average loss: 1.1230, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 41 Loss 0.155770 acc : 0.944508 stop count : 0\n",
            "Test set: Average loss: 1.1198, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 42 Loss 0.155627 acc : 0.944129 stop count : 0\n",
            "Test set: Average loss: 1.1161, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 43 Loss 0.155650 acc : 0.944129 stop count : 0\n",
            "Test set: Average loss: 1.1127, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 44 Loss 0.154463 acc : 0.944413 stop count : 0\n",
            "Test set: Average loss: 1.1069, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 45 Loss 0.154387 acc : 0.944318 stop count : 0\n",
            "Test set: Average loss: 1.1029, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 46 Loss 0.153908 acc : 0.944034 stop count : 0\n",
            "Test set: Average loss: 1.1008, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 47 Loss 0.153889 acc : 0.943939 stop count : 0\n",
            "Test set: Average loss: 1.0970, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 48 Loss 0.152591 acc : 0.945739 stop count : 0\n",
            "Test set: Average loss: 1.0826, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 49 Loss 0.151984 acc : 0.947064 stop count : 0\n",
            "Test set: Average loss: 1.0760, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 50 Loss 0.150366 acc : 0.945928 stop count : 0\n",
            "Test set: Average loss: 1.0702, Accuracy: 279.99999928474426/300 (93%)\n",
            "Test Accuracy: 280.00\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "result_list_classical = []\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "\"\"\"\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=PCA_dim)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.PCA_dim, random_state=seed)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
        "\n",
        "# Pennylane 장치 설정\n",
        "dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
        "\n",
        "\n",
        "def ZZFeatureMapLayer(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ZZFeatureMapLayer_fixed(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "    index=0\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index])*(features[:,index+1]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ansatz(params):\n",
        "    for j in range(len(params)):\n",
        "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
        "        for i in range(len(params[0])):\n",
        "            qml.RY(params[j, i, 0], wires=i)\n",
        "            qml.RZ(params[j, i, 1], wires=i)\n",
        "\n",
        "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
        "        if j == len(params)-1:\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(len(params[0])-1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "\n",
        "# 양자 레이어 정의\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def QuantumLayer(features,params):\n",
        "    ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
        "    ansatz(params)\n",
        "    return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
        "\n",
        "\n",
        "## 양자 커널\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal(features1,features2):\n",
        "    ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal_fix(features1,features2):\n",
        "    ZZFeatureMapLayer_fixed(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer_fixed)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "\n",
        "class Feature_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Feature_model,self).__init__()\n",
        "        self.cls = nn.Sequential(OrderedDict([('cls1', nn.Linear(PCA_dim,PCA_dim*8)),\n",
        "                                              ('relu1', nn.ReLU()),('cls2', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu2', nn.ReLU()),('cls3', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu3', nn.ReLU()),('cls4', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu4', nn.ReLU()),('cls5', nn.Linear(PCA_dim*8,PCA_dim*2-1)),\n",
        "                                              ('sigmoid', nn.ReLU())]))\n",
        "        self.Kernal = Kernal\n",
        "    def forward(self,inputs):\n",
        "        epsilon = 1e-6\n",
        "        input1 = inputs[0]\n",
        "        input2 = inputs[1]\n",
        "        input1 = self.cls(input1)*np.pi\n",
        "        input2 = self.cls(input2)*np.pi\n",
        "        output = self.Kernal(input1,input2)\n",
        "        output = output.type(torch.float32)\n",
        "        return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 하이브리드 모델 정의\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cls = feature_model.cls\n",
        "\n",
        "        self.quantum_layer = QuantumLayer\n",
        "        self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
        "    def forward(self, x):\n",
        "        x = self.cls(x)*np.pi\n",
        "        #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
        "        quantum_output = self.quantum_layer(x,self.Q_params)\n",
        "        quantum_output = quantum_output.type(torch.float32)\n",
        "        return torch.log(quantum_output)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cls_layer_1 = nn.Linear(PCA_dim,PCA_dim*PCA_dim)\n",
        "        self.cls_layer_2 = nn.Linear(PCA_dim*PCA_dim,PCA_dim*PCA_dim-1)\n",
        "        self.output_layer = nn.Linear(PCA_dim*PCA_dim-1,PCA_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.cls_layer_1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.cls_layer_2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "# 모델, 손실 함수, 최적화 설정\n",
        "\n",
        "\n",
        "feature_model = Feature_model(); criterion = nn.BCELoss()\n",
        "#model = Model(); criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(feature_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
        "train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15)\n",
        "\n",
        "model = HybridModel(); criterion = nn.NLLLoss()\n",
        "for param in model.cls.parameters():\n",
        "    param.requires_grad = False\n",
        "#model.load_state_dict(para_dict)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "print('\\n\\n Test start \\n\\n')\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=5)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "result_list_classical.append(acc)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fltgmdndkgx0",
        "outputId": "62dc3fa9-daad-4d8d-9226-2c659471af99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 700/700 [02:34<00:00,  4.54it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "x_train = torch.tensor(x_train).float()\n",
        "num_data = x_train.shape[0]\n",
        "kernel_matrix = torch.zeros((num_data, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_data)\n",
        "    output = feature_model([data,x_train])\n",
        "    kernel_matrix[i] = output.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "jgvLiqvOkgx3",
        "outputId": "2645aaf8-6657-42bf-f1bc-e32218adf955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-d2f3af1b991f>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(y_train).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11515.3555, grad_fn=<AddBackward0>) tensor(40000., grad_fn=<MulBackward0>)\n",
            "tensor(-11580.0527, grad_fn=<AddBackward0>) tensor(16900.4004, grad_fn=<MulBackward0>)\n",
            "tensor(-24597.5625, grad_fn=<AddBackward0>) tensor(3879.0286, grad_fn=<MulBackward0>)\n",
            "tensor(-28473.1816, grad_fn=<AddBackward0>) tensor(0.1113, grad_fn=<MulBackward0>)\n",
            "tensor(-25924.6230, grad_fn=<AddBackward0>) tensor(2546.2891, grad_fn=<MulBackward0>)\n",
            "tensor(-21271.7109, grad_fn=<AddBackward0>) tensor(7198.0054, grad_fn=<MulBackward0>)\n",
            "tensor(-18226.7461, grad_fn=<AddBackward0>) tensor(10242.9287, grad_fn=<MulBackward0>)\n",
            "tensor(-18086.0586, grad_fn=<AddBackward0>) tensor(10384.4570, grad_fn=<MulBackward0>)\n",
            "tensor(-20238.4824, grad_fn=<AddBackward0>) tensor(8233.4902, grad_fn=<MulBackward0>)\n",
            "tensor(-23379.7461, grad_fn=<AddBackward0>) tensor(5094.1099, grad_fn=<MulBackward0>)\n",
            "tensor(-26266.3730, grad_fn=<AddBackward0>) tensor(2209.6345, grad_fn=<MulBackward0>)\n",
            "tensor(-28057.1016, grad_fn=<AddBackward0>) tensor(421.2160, grad_fn=<MulBackward0>)\n",
            "tensor(-28451.8730, grad_fn=<AddBackward0>) tensor(28.7811, grad_fn=<MulBackward0>)\n",
            "tensor(-27690.6016, grad_fn=<AddBackward0>) tensor(792.2875, grad_fn=<MulBackward0>)\n",
            "tensor(-26392.5820, grad_fn=<AddBackward0>) tensor(2092.3252, grad_fn=<MulBackward0>)\n",
            "tensor(-25260.5625, grad_fn=<AddBackward0>) tensor(3226.0540, grad_fn=<MulBackward0>)\n",
            "tensor(-24783.4727, grad_fn=<AddBackward0>) tensor(3704.4927, grad_fn=<MulBackward0>)\n",
            "tensor(-25087.6094, grad_fn=<AddBackward0>) tensor(3401.3501, grad_fn=<MulBackward0>)\n",
            "tensor(-25973.4648, grad_fn=<AddBackward0>) tensor(2516.1863, grad_fn=<MulBackward0>)\n",
            "tensor(-27063.6992, grad_fn=<AddBackward0>) tensor(1426.4170, grad_fn=<MulBackward0>)\n",
            "tensor(-27973.3809, grad_fn=<AddBackward0>) tensor(517.0764, grad_fn=<MulBackward0>)\n",
            "tensor(-28444.9082, grad_fn=<AddBackward0>) tensor(45.8683, grad_fn=<MulBackward0>)\n",
            "tensor(-28419.7012, grad_fn=<AddBackward0>) tensor(71.4740, grad_fn=<MulBackward0>)\n",
            "tensor(-28032.5547, grad_fn=<AddBackward0>) tensor(459.1822, grad_fn=<MulBackward0>)\n",
            "tensor(-27534.1621, grad_fn=<AddBackward0>) tensor(958.3555, grad_fn=<MulBackward0>)\n",
            "tensor(-27175.4766, grad_fn=<AddBackward0>) tensor(1318.0806, grad_fn=<MulBackward0>)\n",
            "tensor(-27106.6660, grad_fn=<AddBackward0>) tensor(1388.1910, grad_fn=<MulBackward0>)\n",
            "tensor(-27333.6270, grad_fn=<AddBackward0>) tensor(1162.7683, grad_fn=<MulBackward0>)\n",
            "tensor(-27740.5625, grad_fn=<AddBackward0>) tensor(757.5703, grad_fn=<MulBackward0>)\n",
            "tensor(-28156.7422, grad_fn=<AddBackward0>) tensor(343.2774, grad_fn=<MulBackward0>)\n",
            "tensor(-28433.3750, grad_fn=<AddBackward0>) tensor(68.6222, grad_fn=<MulBackward0>)\n",
            "tensor(-28500.0879, grad_fn=<AddBackward0>) tensor(3.9192, grad_fn=<MulBackward0>)\n",
            "tensor(-28382.5879, grad_fn=<AddBackward0>) tensor(123.3902, grad_fn=<MulBackward0>)\n",
            "tensor(-28178.6133, grad_fn=<AddBackward0>) tensor(329.2476, grad_fn=<MulBackward0>)\n",
            "tensor(-28006.0039, grad_fn=<AddBackward0>) tensor(503.6158, grad_fn=<MulBackward0>)\n",
            "tensor(-27948.7910, grad_fn=<AddBackward0>) tensor(562.4406, grad_fn=<MulBackward0>)\n",
            "tensor(-28025.7500, grad_fn=<AddBackward0>) tensor(486.9535, grad_fn=<MulBackward0>)\n",
            "tensor(-28192.5332, grad_fn=<AddBackward0>) tensor(321.5168, grad_fn=<MulBackward0>)\n",
            "tensor(-28371.1875, grad_fn=<AddBackward0>) tensor(144.1337, grad_fn=<MulBackward0>)\n",
            "tensor(-28490.0352, grad_fn=<AddBackward0>) tensor(26.5258, grad_fn=<MulBackward0>)\n",
            "tensor(-28514.8594, grad_fn=<AddBackward0>) tensor(2.9615, grad_fn=<MulBackward0>)\n",
            "tensor(-28458.9961, grad_fn=<AddBackward0>) tensor(60.1540, grad_fn=<MulBackward0>)\n",
            "tensor(-28370.2129, grad_fn=<AddBackward0>) tensor(150.3698, grad_fn=<MulBackward0>)\n",
            "tensor(-28303.3398, grad_fn=<AddBackward0>) tensor(218.8065, grad_fn=<MulBackward0>)\n",
            "tensor(-28293.3809, grad_fn=<AddBackward0>) tensor(230.4662, grad_fn=<MulBackward0>)\n",
            "tensor(-28341.9102, grad_fn=<AddBackward0>) tensor(183.7639, grad_fn=<MulBackward0>)\n",
            "tensor(-28421.6289, grad_fn=<AddBackward0>) tensor(105.9794, grad_fn=<MulBackward0>)\n",
            "tensor(-28494.0820, grad_fn=<AddBackward0>) tensor(35.5374, grad_fn=<MulBackward0>)\n",
            "tensor(-28530.1836, grad_fn=<AddBackward0>) tensor(1.4885, grad_fn=<MulBackward0>)\n",
            "tensor(-28523.1133, grad_fn=<AddBackward0>) tensor(10.6176, grad_fn=<MulBackward0>)\n",
            "tensor(-28488.3301, grad_fn=<AddBackward0>) tensor(47.4309, grad_fn=<MulBackward0>)\n",
            "tensor(-28452.4121, grad_fn=<AddBackward0>) tensor(85.3293, grad_fn=<MulBackward0>)\n",
            "tensor(-28437.8477, grad_fn=<AddBackward0>) tensor(101.8094, grad_fn=<MulBackward0>)\n",
            "tensor(-28452.1895, grad_fn=<AddBackward0>) tensor(89.3188, grad_fn=<MulBackward0>)\n",
            "tensor(-28486.5293, grad_fn=<AddBackward0>) tensor(56.7781, grad_fn=<MulBackward0>)\n",
            "tensor(-28522.7773, grad_fn=<AddBackward0>) tensor(22.2980, grad_fn=<MulBackward0>)\n",
            "tensor(-28544.6191, grad_fn=<AddBackward0>) tensor(2.2214, grad_fn=<MulBackward0>)\n",
            "tensor(-28545.8789, grad_fn=<AddBackward0>) tensor(2.7450, grad_fn=<MulBackward0>)\n",
            "tensor(-28532.2578, grad_fn=<AddBackward0>) tensor(18.2010, grad_fn=<MulBackward0>)\n",
            "tensor(-28516.3613, grad_fn=<AddBackward0>) tensor(35.9948, grad_fn=<MulBackward0>)\n",
            "tensor(-28509.8047, grad_fn=<AddBackward0>) tensor(44.5232, grad_fn=<MulBackward0>)\n",
            "tensor(-28517.0039, grad_fn=<AddBackward0>) tensor(39.3749, grad_fn=<MulBackward0>)\n",
            "tensor(-28533.9180, grad_fn=<AddBackward0>) tensor(24.5790, grad_fn=<MulBackward0>)\n",
            "tensor(-28551.6602, grad_fn=<AddBackward0>) tensor(9.0024, grad_fn=<MulBackward0>)\n",
            "tensor(-28562.2891, grad_fn=<AddBackward0>) tensor(0.5745, grad_fn=<MulBackward0>)\n",
            "tensor(-28563.1191, grad_fn=<AddBackward0>) tensor(1.9561, grad_fn=<MulBackward0>)\n",
            "tensor(-28557.4258, grad_fn=<AddBackward0>) tensor(9.8567, grad_fn=<MulBackward0>)\n",
            "tensor(-28551.6016, grad_fn=<AddBackward0>) tensor(17.8713, grad_fn=<MulBackward0>)\n",
            "tensor(-28550.9668, grad_fn=<AddBackward0>) tensor(20.6672, grad_fn=<MulBackward0>)\n",
            "tensor(-28556.8711, grad_fn=<AddBackward0>) tensor(16.9011, grad_fn=<MulBackward0>)\n",
            "tensor(-28566.5410, grad_fn=<AddBackward0>) tensor(9.3508, grad_fn=<MulBackward0>)\n",
            "tensor(-28575.4121, grad_fn=<AddBackward0>) tensor(2.5939, grad_fn=<MulBackward0>)\n",
            "tensor(-28580.1270, grad_fn=<AddBackward0>) tensor(0.0016, grad_fn=<MulBackward0>)\n",
            "tensor(-28580.3516, grad_fn=<AddBackward0>) tensor(1.9220, grad_fn=<MulBackward0>)\n",
            "tensor(-28578.4844, grad_fn=<AddBackward0>) tensor(5.9737, grad_fn=<MulBackward0>)\n",
            "tensor(-28577.7480, grad_fn=<AddBackward0>) tensor(8.9384, grad_fn=<MulBackward0>)\n",
            "tensor(-28580.1016, grad_fn=<AddBackward0>) tensor(8.8568, grad_fn=<MulBackward0>)\n",
            "tensor(-28585.2480, grad_fn=<AddBackward0>) tensor(6.0265, grad_fn=<MulBackward0>)\n",
            "tensor(-28591.1973, grad_fn=<AddBackward0>) tensor(2.4313, grad_fn=<MulBackward0>)\n",
            "tensor(-28595.7812, grad_fn=<AddBackward0>) tensor(0.2261, grad_fn=<MulBackward0>)\n",
            "tensor(-28598.0488, grad_fn=<AddBackward0>) tensor(0.3520, grad_fn=<MulBackward0>)\n",
            "tensor(-28598.6582, grad_fn=<AddBackward0>) tensor(2.1403, grad_fn=<MulBackward0>)\n",
            "tensor(-28599.1973, grad_fn=<AddBackward0>) tensor(3.9968, grad_fn=<MulBackward0>)\n",
            "tensor(-28601.0117, grad_fn=<AddBackward0>) tensor(4.5694, grad_fn=<MulBackward0>)\n",
            "tensor(-28604.3945, grad_fn=<AddBackward0>) tensor(3.5690, grad_fn=<MulBackward0>)\n",
            "tensor(-28608.5684, grad_fn=<AddBackward0>) tensor(1.7738, grad_fn=<MulBackward0>)\n",
            "tensor(-28612.3750, grad_fn=<AddBackward0>) tensor(0.3522, grad_fn=<MulBackward0>)\n",
            "tensor(-28615.0938, grad_fn=<AddBackward0>) tensor(0.0314, grad_fn=<MulBackward0>)\n",
            "tensor(-28616.8418, grad_fn=<AddBackward0>) tensor(0.7030, grad_fn=<MulBackward0>)\n",
            "tensor(-28618.3535, grad_fn=<AddBackward0>) tensor(1.6344, grad_fn=<MulBackward0>)\n",
            "tensor(-28620.4004, grad_fn=<AddBackward0>) tensor(2.0647, grad_fn=<MulBackward0>)\n",
            "tensor(-28623.2559, grad_fn=<AddBackward0>) tensor(1.7156, grad_fn=<MulBackward0>)\n",
            "tensor(-28626.6113, grad_fn=<AddBackward0>) tensor(0.8951, grad_fn=<MulBackward0>)\n",
            "tensor(-28629.8711, grad_fn=<AddBackward0>) tensor(0.1890, grad_fn=<MulBackward0>)\n",
            "tensor(-28632.6191, grad_fn=<AddBackward0>) tensor(0.0131, grad_fn=<MulBackward0>)\n",
            "tensor(-28634.8613, grad_fn=<AddBackward0>) tensor(0.3544, grad_fn=<MulBackward0>)\n",
            "tensor(-28636.9551, grad_fn=<AddBackward0>) tensor(0.8504, grad_fn=<MulBackward0>)\n",
            "tensor(-28639.3008, grad_fn=<AddBackward0>) tensor(1.0972, grad_fn=<MulBackward0>)\n",
            "tensor(-28642.0605, grad_fn=<AddBackward0>) tensor(0.9310, grad_fn=<MulBackward0>)\n",
            "tensor(-28645.0879, grad_fn=<AddBackward0>) tensor(0.5021, grad_fn=<MulBackward0>)\n",
            "tensor(-28648.0801, grad_fn=<AddBackward0>) tensor(0.1189, grad_fn=<MulBackward0>)\n",
            "tensor(-28650.8164, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28653.3066, grad_fn=<AddBackward0>) tensor(0.1485, grad_fn=<MulBackward0>)\n",
            "tensor(-28655.7402, grad_fn=<AddBackward0>) tensor(0.3735, grad_fn=<MulBackward0>)\n",
            "tensor(-28658.3145, grad_fn=<AddBackward0>) tensor(0.4738, grad_fn=<MulBackward0>)\n",
            "tensor(-28661.1133, grad_fn=<AddBackward0>) tensor(0.3796, grad_fn=<MulBackward0>)\n",
            "tensor(-28664.0371, grad_fn=<AddBackward0>) tensor(0.1778, grad_fn=<MulBackward0>)\n",
            "tensor(-28666.9297, grad_fn=<AddBackward0>) tensor(0.0242, grad_fn=<MulBackward0>)\n",
            "tensor(-28669.6914, grad_fn=<AddBackward0>) tensor(0.0150, grad_fn=<MulBackward0>)\n",
            "tensor(-28672.3457, grad_fn=<AddBackward0>) tensor(0.1255, grad_fn=<MulBackward0>)\n",
            "tensor(-28674.9980, grad_fn=<AddBackward0>) tensor(0.2471, grad_fn=<MulBackward0>)\n",
            "tensor(-28677.7441, grad_fn=<AddBackward0>) tensor(0.2812, grad_fn=<MulBackward0>)\n",
            "tensor(-28680.6035, grad_fn=<AddBackward0>) tensor(0.2082, grad_fn=<MulBackward0>)\n",
            "tensor(-28683.5195, grad_fn=<AddBackward0>) tensor(0.0904, grad_fn=<MulBackward0>)\n",
            "tensor(-28686.4082, grad_fn=<AddBackward0>) tensor(0.0106, grad_fn=<MulBackward0>)\n",
            "tensor(-28689.2305, grad_fn=<AddBackward0>) tensor(0.0083, grad_fn=<MulBackward0>)\n",
            "tensor(-28692.0176, grad_fn=<AddBackward0>) tensor(0.0584, grad_fn=<MulBackward0>)\n",
            "tensor(-28694.8262, grad_fn=<AddBackward0>) tensor(0.1033, grad_fn=<MulBackward0>)\n",
            "tensor(-28697.6992, grad_fn=<AddBackward0>) tensor(0.1020, grad_fn=<MulBackward0>)\n",
            "tensor(-28700.6348, grad_fn=<AddBackward0>) tensor(0.0595, grad_fn=<MulBackward0>)\n",
            "tensor(-28703.5859, grad_fn=<AddBackward0>) tensor(0.0138, grad_fn=<MulBackward0>)\n",
            "tensor(-28706.5176, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28709.4219, grad_fn=<AddBackward0>) tensor(0.0253, grad_fn=<MulBackward0>)\n",
            "tensor(-28712.3281, grad_fn=<AddBackward0>) tensor(0.0621, grad_fn=<MulBackward0>)\n",
            "tensor(-28715.2637, grad_fn=<AddBackward0>) tensor(0.0800, grad_fn=<MulBackward0>)\n",
            "tensor(-28718.2383, grad_fn=<AddBackward0>) tensor(0.0660, grad_fn=<MulBackward0>)\n",
            "tensor(-28721.2461, grad_fn=<AddBackward0>) tensor(0.0336, grad_fn=<MulBackward0>)\n",
            "tensor(-28724.2559, grad_fn=<AddBackward0>) tensor(0.0068, grad_fn=<MulBackward0>)\n",
            "tensor(-28727.2539, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28730.2578, grad_fn=<AddBackward0>) tensor(0.0106, grad_fn=<MulBackward0>)\n",
            "tensor(-28733.2695, grad_fn=<AddBackward0>) tensor(0.0221, grad_fn=<MulBackward0>)\n",
            "tensor(-28736.3086, grad_fn=<AddBackward0>) tensor(0.0226, grad_fn=<MulBackward0>)\n",
            "tensor(-28739.3770, grad_fn=<AddBackward0>) tensor(0.0126, grad_fn=<MulBackward0>)\n",
            "tensor(-28742.4531, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28745.5391, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28748.6230, grad_fn=<AddBackward0>) tensor(0.0098, grad_fn=<MulBackward0>)\n",
            "tensor(-28751.7129, grad_fn=<AddBackward0>) tensor(0.0208, grad_fn=<MulBackward0>)\n",
            "tensor(-28754.8281, grad_fn=<AddBackward0>) tensor(0.0253, grad_fn=<MulBackward0>)\n",
            "tensor(-28757.9609, grad_fn=<AddBackward0>) tensor(0.0200, grad_fn=<MulBackward0>)\n",
            "tensor(-28761.1094, grad_fn=<AddBackward0>) tensor(0.0098, grad_fn=<MulBackward0>)\n",
            "tensor(-28764.2676, grad_fn=<AddBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n",
            "tensor(-28767.4297, grad_fn=<AddBackward0>) tensor(7.2853e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28770.6035, grad_fn=<AddBackward0>) tensor(0.0024, grad_fn=<MulBackward0>)\n",
            "tensor(-28773.7910, grad_fn=<AddBackward0>) tensor(0.0044, grad_fn=<MulBackward0>)\n",
            "tensor(-28776.9922, grad_fn=<AddBackward0>) tensor(0.0037, grad_fn=<MulBackward0>)\n",
            "tensor(-28780.2109, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28783.4414, grad_fn=<AddBackward0>) tensor(1.3481e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28786.6816, grad_fn=<AddBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n",
            "tensor(-28789.9316, grad_fn=<AddBackward0>) tensor(0.0059, grad_fn=<MulBackward0>)\n",
            "tensor(-28793.1914, grad_fn=<AddBackward0>) tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "tensor(-28796.4668, grad_fn=<AddBackward0>) tensor(0.0088, grad_fn=<MulBackward0>)\n",
            "tensor(-28799.7559, grad_fn=<AddBackward0>) tensor(0.0060, grad_fn=<MulBackward0>)\n",
            "tensor(-28803.0586, grad_fn=<AddBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n",
            "tensor(-28806.3691, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28809.6934, grad_fn=<AddBackward0>) tensor(1.9245e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28813.0273, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28816.3750, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28819.7363, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28823.1074, grad_fn=<AddBackward0>) tensor(5.3226e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28826.4922, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28829.8848, grad_fn=<AddBackward0>) tensor(0.0024, grad_fn=<MulBackward0>)\n",
            "tensor(-28833.2930, grad_fn=<AddBackward0>) tensor(0.0037, grad_fn=<MulBackward0>)\n",
            "tensor(-28836.7109, grad_fn=<AddBackward0>) tensor(0.0041, grad_fn=<MulBackward0>)\n",
            "tensor(-28840.1426, grad_fn=<AddBackward0>) tensor(0.0033, grad_fn=<MulBackward0>)\n",
            "tensor(-28843.5859, grad_fn=<AddBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n",
            "tensor(-28847.0391, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28850.5039, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28853.9844, grad_fn=<AddBackward0>) tensor(1.1462e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28857.4727, grad_fn=<AddBackward0>) tensor(1.3097e-06, grad_fn=<MulBackward0>)\n",
            "tensor(-28860.9727, grad_fn=<AddBackward0>) tensor(3.5527e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28864.4863, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28868.0117, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28871.5469, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28875.0938, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28878.6523, grad_fn=<AddBackward0>) tensor(0.0022, grad_fn=<MulBackward0>)\n",
            "tensor(-28882.2227, grad_fn=<AddBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n",
            "tensor(-28885.8047, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28889.4004, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28893.0059, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28896.6250, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28900.2520, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28903.8906, grad_fn=<AddBackward0>) tensor(0.0003, grad_fn=<MulBackward0>)\n",
            "tensor(-28907.5430, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28911.2051, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28914.8789, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28918.5645, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28922.2617, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28925.9648, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28929.6875, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28933.4180, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28937.1602, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28940.9141, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28944.6777, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28948.4551, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28952.2402, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28956.0391, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28959.8477, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28963.6680, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28967.4980, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28971.3418, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28975.1973, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28979.0605, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-28982.9375, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-28986.8242, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28990.7207, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-28994.6309, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28998.5527, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29002.4844, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-29006.4277, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-29010.3809, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-29014.3438, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-29018.3203, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29022.3047, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29026.3027, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29030.3105, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-29034.3320, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29038.3613, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29042.4023, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29046.4531, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29050.5176, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-29054.5898, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-29058.6758, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29062.7715, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29066.8750, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29070.9922, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29075.1230, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29079.2617, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29083.4082, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29087.5723, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29091.7422, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29095.9238, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29100.1172, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29104.3184, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29108.5332, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29112.7559, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29116.9922, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29121.2402, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29125.4961, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29129.7637, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29134.0430, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29138.3301, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29142.6309, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29146.9414, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29151.2617, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29155.5918, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29159.9355, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29164.2871, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29168.6504, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29173.0254, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29177.4082, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29181.8066, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29186.2090, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29190.6250, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29195.0527, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29199.4883, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29203.9375, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29208.3965, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29212.8652, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29217.3457, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29221.8340, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29226.3359, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29230.8477, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29235.3672, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29239.9004, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29244.4414, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29248.9961, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29253.5586, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29258.1328, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29262.7148, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29267.3125, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29271.9180, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29276.5312, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29281.1562, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29285.7949, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29290.4414, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29295.0977, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29299.7656, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29304.4434, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29309.1309, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29313.8301, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29318.5391, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29323.2578, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29327.9863, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29332.7285, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29337.4785, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29342.2422, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29347.0137, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29351.7930, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29356.5859, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29361.3887, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29366.1992, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29371.0215, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29375.8574, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29380.6992, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29385.5547, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29390.4180, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29395.2930, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29400.1777, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29405.0723, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29409.9785, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29414.8926, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29419.8203, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29424.7559, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29429.7031, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29434.6562, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29439.6270, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29444.6035, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29449.5918, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29454.5879, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29459.5977, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29464.6152, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29469.6426, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29474.6836, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29479.7305, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29484.7910, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29489.8594, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29494.9375, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29500.0293, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29505.1309, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29510.2383, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29515.3594, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29520.4902, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29525.6328, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29530.7812, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29535.9434, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29541.1152, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29546.2988, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29551.4902, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29556.6914, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29561.9023, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29567.1270, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29572.3574, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29577.6016, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29582.8535, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29588.1191, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29593.3926, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-29598.6738, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29603.9688, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29609.2734, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29614.5859, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29619.9102, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29625.2461, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29630.5879, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29635.9453, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29641.3105, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29646.6836, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29652.0684, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29657.4648, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29662.8711, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29668.2852, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29673.7129, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29679.1484, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29684.5957, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29690.0527, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29695.5176, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29700.9961, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29706.4805, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29711.9785, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29717.4844, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29723.0020, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29728.5312, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29734.0664, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29739.6152, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29745.1719, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29750.7402, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29756.3184, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29761.9043, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29767.5039, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29773.1133, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29778.7324, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29784.3594, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29789.9980, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29795.6484, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29801.3086, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29806.9766, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29812.6543, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29818.3457, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29824.0469, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29829.7539, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29835.4746, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29841.2070, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29846.9453, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29852.6953, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29858.4551, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29864.2266, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29870.0059, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29875.7969, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29881.5996, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29887.4102, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29893.2305, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29899.0645, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29904.9043, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29910.7539, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29916.6172, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29922.4902, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29928.3730, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29934.2637, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29940.1680, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29946.0801, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29952.0020, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29957.9355, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29963.8770, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29969.8301, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29975.7930, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29981.7656, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29987.7500, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29993.7422, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-29999.7461, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30005.7617, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30011.7852, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30017.8184, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30023.8652, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30029.9180, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30035.9844, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30042.0586, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30048.1426, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30054.2383, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30060.3438, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30066.4609, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30072.5840, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30078.7207, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30084.8672, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30091.0215, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30097.1875, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30103.3652, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30109.5527, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30115.7500, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30121.9570, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30128.1738, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30134.4004, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30140.6387, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30146.8867, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30153.1426, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30159.4121, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30165.6895, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30171.9785, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30178.2754, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30184.5859, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30190.9043, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30197.2344, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30203.5742, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30209.9219, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30216.2852, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30222.6543, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30229.0352, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30235.4238, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30241.8262, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30248.2363, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30254.6562, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30261.0898, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30267.5293, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30273.9824, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30280.4434, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30286.9180, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30293.3984, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30299.8906, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30306.3945, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30312.9062, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30319.4316, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30325.9648, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30332.5078, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30339.0625, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30345.6289, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30352.2031, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30358.7852, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30365.3828, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30371.9863, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30378.6035, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30385.2305, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30391.8633, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30398.5117, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30405.1699, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30411.8359, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30418.5137, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30425.2012, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30431.8984, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30438.6074, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30445.3262, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30452.0566, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30458.7930, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30465.5430, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30472.3047, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30479.0723, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30485.8535, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30492.6484, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30499.4473, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30506.2598, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30513.0801, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30519.9121, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30526.7539, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30533.6074, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30540.4707, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30547.3457, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-30554.2266, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30561.1230, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30568.0273, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30574.9434, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30581.8672, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30588.8027, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-30595.7480, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "Optimized alphas: tensor([0.5160, 0.5148, 0.5131, 0.5207, 0.5211, 0.5207, 0.5159, 0.5218, 0.5218,\n",
            "        0.5161, 0.5154, 0.5143, 0.5226, 0.5217, 0.5142, 0.5217, 0.5213, 0.5144,\n",
            "        0.5156, 0.5218, 0.5160, 0.5213, 0.5155, 0.5189, 0.5152, 0.5152, 0.5214,\n",
            "        0.5132, 0.5150, 0.5219, 0.5218, 0.5201, 0.5127, 0.5218, 0.5153, 0.5210,\n",
            "        0.5215, 0.5132, 0.5213, 0.5153, 0.5218, 0.5159, 0.5218, 0.5218, 0.5218,\n",
            "        0.5198, 0.5157, 0.5216, 0.5158, 0.5218, 0.5203, 0.5160, 0.5219, 0.5164,\n",
            "        0.5217, 0.5218, 0.5219, 0.5203, 0.5153, 0.5216, 0.5217, 0.5160, 0.5209,\n",
            "        0.5203, 0.5111, 0.5190, 0.5151, 0.5156, 0.5166, 0.5152, 0.5159, 0.5160,\n",
            "        0.5219, 0.5149, 0.5155, 0.5216, 0.5135, 0.5156, 0.5208, 0.5203, 0.5159,\n",
            "        0.5217, 0.5217, 0.5210, 0.5219, 0.5161, 0.5126, 0.5218, 0.5214, 0.5216,\n",
            "        0.5205, 0.5144, 0.5186, 0.5147, 0.5215, 0.5140, 0.5148, 0.5218, 0.5148,\n",
            "        0.5208, 0.5210, 0.5155, 0.5216, 0.5218, 0.5216, 0.5159, 0.5162, 0.5218,\n",
            "        0.5190, 0.5160, 0.5217, 0.5151, 0.5181, 0.5212, 0.5217, 0.5209, 0.5199,\n",
            "        0.5218, 0.5218, 0.5217, 0.5159, 0.5216, 0.5218, 0.5140, 0.5156, 0.5155,\n",
            "        0.5214, 0.5159, 0.5157, 0.5216, 0.5217, 0.5156, 0.5220, 0.5213, 0.5161,\n",
            "        0.5160, 0.5151, 0.5208, 0.5159, 0.5145, 0.5203, 0.5218, 0.5157, 0.5180,\n",
            "        0.5152, 0.5213, 0.5155, 0.5160, 0.5159, 0.5135, 0.5156, 0.5217, 0.5225,\n",
            "        0.5218, 0.5157, 0.5150, 0.5206, 0.5127, 0.5146, 0.5161, 0.5222, 0.5213,\n",
            "        0.5145, 0.5215, 0.5212, 0.5200, 0.5214, 0.5161, 0.5143, 0.5153, 0.5217,\n",
            "        0.5212, 0.5092, 0.5161, 0.5134, 0.5148, 0.5140, 0.5161, 0.5161, 0.5216,\n",
            "        0.5126, 0.5157, 0.5158, 0.5161, 0.5217, 0.5161, 0.5210, 0.5160, 0.5158,\n",
            "        0.5198, 0.5154, 0.5216, 0.5159, 0.5159, 0.5155, 0.5154, 0.5159, 0.5217,\n",
            "        0.5218, 0.5218, 0.5207, 0.5223, 0.5157, 0.5218, 0.5161, 0.5221, 0.5216,\n",
            "        0.5138, 0.5215, 0.5161, 0.5220, 0.5161, 0.5217, 0.5085, 0.5207, 0.5215,\n",
            "        0.5218, 0.5204, 0.5161, 0.5157, 0.5161, 0.5157, 0.5197, 0.5216, 0.5161,\n",
            "        0.5218, 0.5212, 0.5212, 0.5162, 0.5218, 0.5152, 0.5133, 0.5159, 0.5217,\n",
            "        0.5130, 0.5161, 0.5218, 0.5134, 0.5214, 0.5163, 0.5145, 0.5215, 0.5160,\n",
            "        0.5155, 0.5151, 0.5215, 0.5219, 0.5216, 0.5153, 0.5148, 0.5217, 0.5150,\n",
            "        0.5122, 0.5217, 0.5155, 0.5214, 0.5149, 0.5215, 0.5201, 0.5161, 0.5217,\n",
            "        0.5161, 0.5216, 0.5194, 0.5218, 0.5219, 0.5147, 0.5216, 0.5217, 0.5146,\n",
            "        0.5121, 0.5202, 0.5199, 0.5218, 0.5109, 0.5209, 0.5157, 0.5157, 0.5163,\n",
            "        0.5160, 0.5149, 0.5204, 0.5147, 0.5160, 0.5215, 0.5161, 0.5209, 0.5141,\n",
            "        0.5218, 0.5219, 0.5160, 0.5153, 0.5156, 0.5159, 0.5159, 0.5218, 0.5217,\n",
            "        0.5212, 0.5156, 0.5156, 0.5160, 0.5218, 0.5156, 0.5201, 0.5144, 0.5216,\n",
            "        0.5217, 0.5148, 0.5160, 0.5159, 0.5161, 0.5158, 0.5159, 0.5213, 0.5155,\n",
            "        0.5166, 0.5161, 0.5134, 0.5143, 0.5160, 0.5213, 0.5218, 0.5212, 0.5160,\n",
            "        0.5218, 0.5159, 0.5216, 0.5150, 0.5151, 0.5160, 0.5132, 0.5150, 0.5163,\n",
            "        0.5154, 0.5215, 0.5217, 0.5145, 0.5212, 0.5156, 0.5160, 0.5157, 0.5159,\n",
            "        0.5217, 0.5212, 0.5153, 0.5212, 0.5214, 0.5217, 0.5217, 0.5218, 0.5213,\n",
            "        0.5212, 0.5211, 0.5214, 0.5147, 0.5157, 0.5158, 0.5163, 0.5201, 0.5217,\n",
            "        0.5161, 0.5152, 0.5158, 0.5218, 0.5159, 0.5220, 0.5158, 0.5160, 0.5215,\n",
            "        0.5230, 0.5157, 0.5159, 0.5161, 0.5146, 0.5159, 0.5159, 0.5159, 0.5153,\n",
            "        0.5217, 0.5159, 0.5129, 0.5157, 0.5153, 0.5218, 0.5159, 0.5159, 0.5217,\n",
            "        0.5156, 0.5193, 0.5218, 0.5214, 0.5203, 0.5161, 0.5218, 0.5217, 0.5149,\n",
            "        0.5220, 0.5148, 0.5144, 0.5225, 0.5141, 0.5163, 0.5155, 0.5215, 0.5158,\n",
            "        0.5221, 0.5154, 0.5160, 0.5160, 0.5217, 0.5206, 0.5159, 0.5174, 0.5216,\n",
            "        0.5204, 0.5210, 0.5140, 0.5158, 0.5213, 0.5222, 0.5218, 0.5158, 0.5124,\n",
            "        0.5222, 0.5155, 0.5217, 0.5204, 0.5140, 0.5157, 0.5199, 0.5153, 0.5210,\n",
            "        0.5160, 0.5217, 0.5160, 0.5215, 0.5214, 0.5154, 0.5216, 0.5161, 0.5205,\n",
            "        0.5214, 0.5154, 0.5149, 0.5159, 0.5203, 0.5207, 0.5142, 0.5160, 0.5161,\n",
            "        0.5155, 0.5151, 0.5218, 0.5220, 0.5217, 0.5218, 0.5218, 0.5217, 0.5209,\n",
            "        0.5157, 0.5218, 0.5141, 0.5216, 0.5213, 0.5148, 0.5183, 0.5184, 0.5161,\n",
            "        0.5125, 0.5217, 0.5146, 0.5150, 0.5154, 0.5217, 0.5214, 0.5159, 0.5197,\n",
            "        0.5158, 0.5216, 0.5163, 0.5118, 0.5218, 0.5159, 0.5118, 0.5157, 0.5209,\n",
            "        0.5218, 0.5135, 0.5148, 0.5152, 0.5217, 0.5148, 0.5156, 0.5217, 0.5158,\n",
            "        0.5159, 0.5148, 0.5220, 0.5151, 0.5228, 0.5200, 0.5147, 0.5215, 0.5132,\n",
            "        0.5225, 0.5127, 0.5157, 0.5159, 0.5136, 0.5128, 0.5213, 0.5213, 0.5216,\n",
            "        0.5216, 0.5211, 0.5227, 0.5119, 0.5139, 0.5158, 0.5153, 0.5214, 0.5216,\n",
            "        0.5158, 0.5215, 0.5154, 0.5161, 0.5222, 0.5187, 0.5201, 0.5215, 0.5218,\n",
            "        0.5203, 0.5155, 0.5163, 0.5218, 0.5147, 0.5160, 0.5161, 0.5158, 0.5144,\n",
            "        0.5206, 0.5161, 0.5216, 0.5216, 0.5216, 0.5215, 0.5151, 0.5157, 0.5217,\n",
            "        0.5157, 0.5145, 0.5217, 0.5215, 0.5216, 0.5216, 0.5216, 0.5206, 0.5218,\n",
            "        0.5207, 0.5208, 0.5203, 0.5153, 0.5217, 0.5098, 0.5181, 0.5208, 0.5152,\n",
            "        0.5160, 0.5211, 0.5152, 0.5164, 0.5211, 0.5218, 0.5151, 0.5217, 0.5212,\n",
            "        0.5215, 0.5209, 0.5213, 0.5154, 0.5158, 0.5177, 0.5155, 0.5145, 0.5158,\n",
            "        0.5160, 0.5213, 0.5147, 0.5144, 0.5160, 0.5141, 0.5217, 0.5217, 0.5162,\n",
            "        0.5167, 0.5218, 0.5153, 0.5221, 0.5159, 0.5152, 0.5212, 0.5159, 0.5156,\n",
            "        0.5216, 0.5150, 0.5160, 0.5212, 0.5161, 0.5211, 0.5160, 0.5211, 0.5219,\n",
            "        0.5148, 0.5159, 0.5203, 0.5200, 0.5182, 0.5216, 0.5158, 0.5215, 0.5216,\n",
            "        0.5216, 0.5206, 0.5160, 0.5158, 0.5148, 0.5217, 0.5152, 0.5144, 0.5161,\n",
            "        0.5215, 0.5152, 0.5142, 0.5153, 0.5155, 0.5160, 0.5218, 0.5216, 0.5160,\n",
            "        0.5150, 0.5150, 0.5218, 0.5161, 0.5157, 0.5217, 0.5160, 0.5213, 0.5218,\n",
            "        0.5222, 0.5212, 0.5158, 0.5216, 0.5152, 0.5202, 0.5158, 0.5153, 0.5158,\n",
            "        0.5218, 0.5160, 0.5229, 0.5151, 0.5177, 0.5154, 0.5218, 0.5157, 0.5146,\n",
            "        0.5217, 0.5141, 0.5222, 0.5216, 0.5218, 0.5218, 0.5163, 0.5151, 0.5216,\n",
            "        0.5205, 0.5179, 0.5200, 0.5159, 0.5152, 0.5213, 0.5146, 0.5215, 0.5159,\n",
            "        0.5217, 0.5140, 0.5146, 0.5149, 0.5222, 0.5218, 0.5213, 0.5154, 0.5161,\n",
            "        0.5163, 0.5210, 0.5202, 0.5218, 0.5159, 0.5158, 0.5216])\n"
          ]
        }
      ],
      "source": [
        "labels = torch.tensor(y_train).float()\n",
        "labels = 2*labels-1\n",
        "alpha = torch.tensor([0.5]*num_data,requires_grad=True)\n",
        "optimizer = torch.optim.Adam([alpha], lr=0.001)\n",
        "def objective_function(alpha, kernel_matrix, labels):\n",
        "    \"\"\"SVM의 쌍대 목적 함수\"\"\"\n",
        "    L = 0.5 * torch.dot(alpha, torch.mv(kernel_matrix, alpha)) - torch.sum(alpha)\n",
        "    # 제약 조건을 유지하기 위해 레이블과 alpha의 곱의 합은 0이어야 합니다.\n",
        "    constraint = torch.dot(alpha, labels)\n",
        "    loss = -L + 1e4 * constraint ** 2\n",
        "    print(loss,1e4 * constraint ** 2)\n",
        "    return loss  # 제약조건에 큰 페널티를 적용\n",
        "\n",
        "# 훈련 과정\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective_function(alpha, kernel_matrix, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    alpha.data.clamp_(0)  # alpha는 0 이상이어야 함\n",
        "\n",
        "print(\"Optimized alphas:\", alpha.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "fm6T_J2-kgx4",
        "outputId": "e148967c-146a-4fc4-d020-e30e23df3246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 700/700 [01:22<00:00,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tensor([-1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1., -1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1.,  1.], grad_fn=<SignBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pennylane as qml\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 테스트 데이터와 훈련 데이터 간의 양자 커널 행렬 계산\n",
        "x_test = torch.tensor(x_test).float()\n",
        "num_test = x_test.size(0)\n",
        "test_kernel_matrix = torch.zeros((num_test, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_test)\n",
        "    output = feature_model([data,x_test])\n",
        "    test_kernel_matrix[:,i] = output.detach().cpu()\n",
        "\n",
        "# 훈련된 모델을 사용하여 테스트 데이터의 클래스 예측\n",
        "predictions = torch.sign(torch.mv(test_kernel_matrix, alpha * labels))\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = (predictions+1)/2"
      ],
      "metadata": {
        "id": "tqKJfT74pDjk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(predictions,y_test)"
      ],
      "metadata": {
        "id": "zUkNq4ZhpAyC",
        "outputId": "07814bee-5a1d-4762-ec20-1e5058ef5731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9366666674613953"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "281/300"
      ],
      "metadata": {
        "id": "9uQYsztupJdA",
        "outputId": "b2fe6a1f-5196-40d0-ab27-1565d1948d10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9366666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "dJ0Bjl-KtiW4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Kernal added\""
      ],
      "metadata": {
        "id": "K1Z9kvQ2xYkt",
        "outputId": "b5c19833-bee3-4185-ec54-a1dff324da97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author identity unknown\n",
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@3773fe15e751.(none)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "ZX6ySGowxc4K",
        "outputId": "f795e6dc-0406-4820-d116-4c37578aeee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3uzOdNWxd9s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}