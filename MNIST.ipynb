{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "# 데이터 전처리 설정\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋 로드\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
    "train_mask = (train_dataset.targets >= 0) & (train_dataset.targets <= 8-1)\n",
    "test_mask = (test_dataset.targets >= 0) & (test_dataset.targets <= 8-1)\n",
    "\n",
    "train_dataset.data = train_dataset.data[train_mask]\n",
    "train_dataset.targets = train_dataset.targets[train_mask]\n",
    "test_dataset.data = test_dataset.data[test_mask]\n",
    "test_dataset.targets = test_dataset.targets[test_mask]\n",
    "\n",
    "# 데이터를 NumPy 배열로 변환\n",
    "x_train = train_dataset.data.numpy().reshape(-1, 784)\n",
    "x_test = test_dataset.data.numpy().reshape(-1, 784)\n",
    "y_train = train_dataset.targets.numpy()\n",
    "y_test = test_dataset.targets.numpy()\n",
    "\n",
    "# PCA로 feature 축소\n",
    "pca = PCA(n_components=8)\n",
    "x_train_pca = pca.fit_transform(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "\n",
    "# 정규화 (표준 스케일러 사용)\n",
    "scaler = StandardScaler()\n",
    "x_train_pca = scaler.fit_transform(x_train_pca)\n",
    "x_test_pca = scaler.transform(x_test_pca)\n",
    "\n",
    "# PyTorch Tensor로 변환\n",
    "x_train_pca, y_train = torch.tensor(x_train_pca, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_pca, y_test = torch.tensor(x_test_pca, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "class Feature_data_loader(Dataset):\n",
    "    def __init__(self,x_train,y_train):\n",
    "        self.feature1 = x_train\n",
    "        temp = copy.deepcopy(x_train)\n",
    "        shuffle = torch.randperm(len(temp))\n",
    "        self.feature2 = temp[shuffle]\n",
    "        \n",
    "        self.y1 = y_train\n",
    "        temp_y = copy.deepcopy(y_train)\n",
    "        self.y2 = temp_y[shuffle]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature1)\n",
    "    def __getitem__(self,idx):\n",
    "        input1 = self.feature1[idx]\n",
    "        input2 = self.feature2[idx]\n",
    "        if self.y1[idx] == self.y2[idx]:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return [input1,input2],label\n",
    "\n",
    "\n",
    "# DataLoader 생성\n",
    "\n",
    "\n",
    "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train),batch_size=batch_size,shuffle=True)\n",
    "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN, create_dataset\n",
    "def reg(acts_scale,KAN_layer, factor=1,lamb_l1=1.,lamb_entropy=2.,lamb_coef=0.,lamb_coefdiff=0.):\n",
    "\n",
    "    def nonlinear(x, th=1e-16):\n",
    "        return (x < th) * x * factor + (x > th) * (x + (factor - 1) * th)\n",
    "\n",
    "    reg_ = 0.\n",
    "    for i in range(len(acts_scale)):\n",
    "        vec = acts_scale[i].reshape(-1, )\n",
    "\n",
    "        p = vec / torch.sum(vec)\n",
    "        l1 = torch.sum(nonlinear(vec))\n",
    "        entropy = - torch.sum(p * torch.log2(p + 1e-4))\n",
    "        reg_ += lamb_l1 * l1 + lamb_entropy * entropy  # both l1 and entropy\n",
    "\n",
    "    # regularize coefficient to encourage spline to be zero\n",
    "    for i in range(len(KAN_layer.act_fun)):\n",
    "        coeff_l1 = torch.sum(torch.mean(torch.abs(KAN_layer.act_fun[i].coef), dim=1))\n",
    "        coeff_diff_l1 = torch.sum(torch.mean(torch.abs(torch.diff(KAN_layer.act_fun[i].coef)), dim=1))\n",
    "        reg_ += lamb_coef * coeff_l1 + lamb_coefdiff * coeff_diff_l1\n",
    "\n",
    "    return reg_\n",
    "def accuracy(pred, true):\n",
    "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
    "    pred = pred.detach().cpu()\n",
    "    true = true.cpu()\n",
    "    try:\n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "    except:\n",
    "        pred_labels = torch.round(pred)\n",
    "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
    "    correct = (pred_labels == true).sum()\n",
    "    # 정확도를 계산\n",
    "    acc = correct / true.size(0)\n",
    "    return acc.item() \n",
    "\n",
    "class Early_stop_train():\n",
    "    def __init__(self,model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "\n",
    "        \n",
    "        self.loss_list = [1e100]\n",
    "        self.acc_list = []\n",
    "        self.stop_count = 0\n",
    "        \n",
    "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10):\n",
    "        #self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            if self.stop_count>=res:\n",
    "                break\n",
    "            loss_val,_ = self.test(test_loader)\n",
    "            self.loss_list.append(loss_val)\n",
    "            \n",
    "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
    "                self.stop_count+=1\n",
    "            else:\n",
    "                self.stop_count = 0\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            for X_train,y_train in train_loader:\n",
    "                y_train = y_train.type(torch.float32)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(X_train)\n",
    "                reg_ = 0.1*reg(self.model.KAN.acts_scale,self.model.KAN)\n",
    "                try:\n",
    "                    y_train = y_train.type(torch.float32)\n",
    "                    loss = self.criterion(output.squeeze(), y_train)+reg_\n",
    "                except:\n",
    "                    y_train = y_train.long()\n",
    "                    loss = self.criterion(output.squeeze(), y_train)+reg_\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss_list.append(loss.item())\n",
    "                acc = accuracy(output,y_train)\n",
    "                acc_list.append(acc)\n",
    "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} reg : {reg_:4f} stop count : {self.stop_count}\")\n",
    "\n",
    "    def test(self,test_loader):\n",
    "        if test_loader is None:\n",
    "            return 0,0\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data, target\n",
    "                    output = self.model(data)\n",
    "                    test_loss += nn.functional.cross_entropy(output, target, reduction='sum').item()\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "            return test_loss,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 2.184938 acc : 0.789909 reg : 1.058526 stop count : 0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# 모델 학습 및 평가\u001b[39;00m\n\u001b[0;32m    139\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(feature_model, optimizer, criterion)\n\u001b[1;32m--> 140\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m para_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[1;32mIn[41], line 66\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res)\u001b[0m\n\u001b[0;32m     64\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m reg_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mreg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mKAN\u001b[38;5;241m.\u001b[39macts_scale,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mKAN)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 91\u001b[0m, in \u001b[0;36mFeature_model.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     89\u001b[0m input1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKAN(input1)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#input1 = torch.concat([input1,input1_copy],dim=1)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m input2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#input2 = torch.concat([input2,input2_copy],dim=1)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKernal(input1,input2)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\KAN.py:311\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts\u001b[38;5;241m.\u001b[39mappend(x)  \u001b[38;5;66;03m# acts shape: (batch, width[l])\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m--> 311\u001b[0m     x_numerical, preacts, postacts_numerical, postspline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_enabled \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m         x_symbolic, postacts_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_fun[l](x)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\KANLayer.py:172\u001b[0m, in \u001b[0;36mKANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    170\u001b[0m preacts \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)\n\u001b[0;32m    171\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_fun(x)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (batch, size)\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcoef2curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_sharing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_sharing\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (size, batch)\u001b[39;00m\n\u001b[0;32m    173\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (batch, size)\u001b[39;00m\n\u001b[0;32m    174\u001b[0m postspline \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mreshape(batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\spline.py:100\u001b[0m, in \u001b[0;36mcoef2curve\u001b[1;34m(x_eval, grid, coef, k, device)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mconverting B-spline coefficients to B-spline curves. Evaluate x on B-spline curves (summing up B_batch results over B-spline basis).\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03mtorch.Size([5, 100])\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# x_eval: (size, batch), grid: (size, grid), coef: (size, coef)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# coef: (size, coef), B_batch: (size, coef, batch), summer over coef\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m y_eval \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mij,ijk->ik\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_eval\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\functional.py:198\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;66;03m# Overwriting reason:\u001b[39;00m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# This dispatches to two ATen functions depending on the type of\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# split_size_or_sections. The branching code is in _tensor.py, which we\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# call here.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msplit(split_size_or_sections, dim)\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"einsum(equation, *operands) -> Tensor\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Sums the product of the elements of the input :attr:`operands` along dimensions specified using a notation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m                [ 0.3311,  5.5201, -3.0356]])\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_einsum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mopt_einsum\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "result_list_classical = []\n",
    "for seed in [115,151,123,8,25]:\n",
    "    # 데이터 로드 및 전처리\n",
    "    \"\"\"\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=8)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.8, random_state=seed)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
    "\n",
    "    # Pennylane 장치 설정\n",
    "    dev = qml.device(\"default.qubit\", wires=8)\n",
    "\n",
    "\n",
    "    def ZZFeatureMapLayer(features, wires):\n",
    "        \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
    "        index = 0\n",
    "        for i in wires:\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(features[:,index], wires=i)\n",
    "            index += 1\n",
    "\n",
    "        for j in range(0, len(wires)-1):\n",
    "            qml.CNOT(wires=[j, j+1])\n",
    "            qml.RZ((features[:,index]), wires=j+1)\n",
    "            qml.CNOT(wires=[j, j+1])\n",
    "            index+=1\n",
    "    \n",
    "    def ansatz(params):\n",
    "        for j in range(len(params)):\n",
    "            # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
    "            for i in range(len(params[0])):\n",
    "                qml.RY(params[j, i, 0], wires=i)\n",
    "                qml.RZ(params[j, i, 1], wires=i)\n",
    "                \n",
    "            # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
    "            if j == len(params)-1:\n",
    "                pass\n",
    "            else:\n",
    "                for i in range(len(params[0])-1):\n",
    "                    qml.CNOT(wires=[i, i+1])\n",
    "\n",
    "\n",
    "    # 양자 레이어 정의\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def QuantumLayer(features,params):\n",
    "        ZZFeatureMapLayer(features, wires=range(8))\n",
    "        ansatz(params)\n",
    "        return qml.probs(wires=[0,1,2])\n",
    "\n",
    "    \n",
    "    ## 양자 커널\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def Kernal(features1,features2):\n",
    "        ZZFeatureMapLayer(features1, wires=range(8))\n",
    "        qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(8))\n",
    "        return qml.probs(wires=range(8))\n",
    "    \n",
    "    \n",
    "    class Feature_model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Feature_model,self).__init__()\n",
    "            self.KAN = KAN([8,5,8*2-1],grid=10)\n",
    "            self.Kernal = Kernal\n",
    "        def forward(self,inputs):\n",
    "            input1 = inputs[0]\n",
    "            #input1_copy = input1.clone().detach().requires_grad_(True)\n",
    "            input2 = inputs[1]\n",
    "            #input2_copy = input2.clone().detach().requires_grad_(True)\n",
    "            input1 = self.KAN(input1)\n",
    "            #input1 = torch.concat([input1,input1_copy],dim=1)\n",
    "            input2 = self.KAN(input2)\n",
    "            #input2 = torch.concat([input2,input2_copy],dim=1)\n",
    "            output = self.Kernal(input1,input2)\n",
    "            output = output.type(torch.float32)\n",
    "            return output[:,0]\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    # 하이브리드 모델 정의\n",
    "    class HybridModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(HybridModel, self).__init__()\n",
    "            self.KAN = feature_model.KAN\n",
    "            \n",
    "            self.quantum_layer = QuantumLayer\n",
    "            self.Q_params = nn.Parameter((torch.rand([8,8,2])*2-1)*np.pi,requires_grad=True)\n",
    "        def forward(self, x):\n",
    "            x = self.KAN(x)\n",
    "            #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
    "            quantum_output = self.quantum_layer(x,self.Q_params)\n",
    "            quantum_output = quantum_output.type(torch.float32)\n",
    "            return torch.log(quantum_output)\n",
    "        \n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.cls_layer_1 = nn.Linear(8,8*2)\n",
    "            self.cls_layer_2 = nn.Linear(8*2,8*2-1)\n",
    "            self.output_layer = nn.Linear(8*2-1,8)\n",
    "        def forward(self, x):\n",
    "            x = self.cls_layer_1(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.cls_layer_2(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            output = self.output_layer(x)\n",
    "            return output\n",
    "    # 모델, 손실 함수, 최적화 설정\n",
    "    \n",
    "    \n",
    "    feature_model = Feature_model(); criterion = nn.BCELoss()\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(feature_model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    # 모델 학습 및 평가\n",
    "    train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
    "    train_process.train_model(feature_loader,epochs=5)\n",
    "    \n",
    "    para_dict = {}\n",
    "\n",
    "    for key in model.state_dict().keys():\n",
    "        try:\n",
    "            para_dict[key]=feature_model.state_dict()[key]\n",
    "        except:\n",
    "            para_dict[key]=model.state_dict()[key]\n",
    "    \n",
    "    model = HybridModel(); criterion = nn.NLLLoss()\n",
    "\n",
    "    for param in model.KAN.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.KAN.parameters():\n",
    "        print(param.requires_grad)\n",
    "    train_process = Early_stop_train(model, optimizer, criterion)\n",
    "    train_process.train_model(feature_loader,epochs=15)\n",
    "    \n",
    "    _,accuracy = train_process.test(test_loader)\n",
    "    result_list_classical.append(accuracy)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 41 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.KAN.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_circuit(params):\n",
    "    for j in range(len(params)):\n",
    "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
    "        for i in range(len(params[0])):\n",
    "            qml.RX(params[0, i, 0], wires=i)\n",
    "            qml.RY(params[0, i, 1], wires=i)\n",
    "            qml.RZ(params[0, i, 2], wires=i)\n",
    "        \n",
    "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
    "        for i in range(len(params[0])-1):\n",
    "            qml.CNOT(wires=[i, i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_32884\\2305901559.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.6606189012527466 acc : 0.6612486839294434 stop count : 0\n",
      "Epoch 2 Loss 0.6599441766738892 acc : 0.6607721447944641 stop count : 0\n",
      "Epoch 3 Loss 0.6594443917274475 acc : 0.6602181792259216 stop count : 0\n",
      "Epoch 4 Loss 0.6588974595069885 acc : 0.6594597697257996 stop count : 0\n",
      "Epoch 5 Loss 0.6581461429595947 acc : 0.6583967804908752 stop count : 0\n",
      "Epoch 6 Loss 0.6570683717727661 acc : 0.6569231152534485 stop count : 0\n",
      "Epoch 7 Loss 0.6555571556091309 acc : 0.6548988223075867 stop count : 0\n",
      "Epoch 8 Loss 0.6534753441810608 acc : 0.6522341370582581 stop count : 0\n",
      "Epoch 9 Loss 0.650797426700592 acc : 0.6490319967269897 stop count : 0\n",
      "Epoch 10 Loss 0.6477214694023132 acc : 0.6455845236778259 stop count : 0\n",
      "Epoch 11 Loss 0.6444721817970276 acc : 0.6420226693153381 stop count : 0\n",
      "Epoch 12 Loss 0.6409582495689392 acc : 0.6382325291633606 stop count : 0\n",
      "Epoch 13 Loss 0.6370148062705994 acc : 0.6341482400894165 stop count : 0\n",
      "Epoch 14 Loss 0.6326273083686829 acc : 0.6297394037246704 stop count : 0\n",
      "Epoch 15 Loss 0.6278442740440369 acc : 0.6250247955322266 stop count : 0\n",
      "Epoch 16 Loss 0.622768223285675 acc : 0.6200156211853027 stop count : 0\n",
      "Epoch 17 Loss 0.6174346208572388 acc : 0.614687442779541 stop count : 0\n",
      "Epoch 18 Loss 0.6117743253707886 acc : 0.6091455221176147 stop count : 0\n",
      "Epoch 19 Loss 0.6058481931686401 acc : 0.6034901142120361 stop count : 0\n",
      "Epoch 20 Loss 0.5997894406318665 acc : 0.59766685962677 stop count : 0\n",
      "Epoch 21 Loss 0.5936314463615417 acc : 0.591729998588562 stop count : 0\n",
      "Epoch 22 Loss 0.5873993039131165 acc : 0.5858304500579834 stop count : 0\n",
      "Epoch 23 Loss 0.581192135810852 acc : 0.5800281763076782 stop count : 0\n",
      "Epoch 24 Loss 0.575114905834198 acc : 0.5743480324745178 stop count : 0\n",
      "Epoch 25 Loss 0.5692067742347717 acc : 0.5687410831451416 stop count : 0\n",
      "Epoch 26 Loss 0.5634355545043945 acc : 0.5632423162460327 stop count : 0\n",
      "Epoch 27 Loss 0.5578117966651917 acc : 0.5579398274421692 stop count : 0\n",
      "Epoch 28 Loss 0.5523291230201721 acc : 0.552808940410614 stop count : 0\n",
      "Epoch 29 Loss 0.5468897223472595 acc : 0.5478862524032593 stop count : 0\n",
      "Epoch 30 Loss 0.5416029095649719 acc : 0.5430936813354492 stop count : 0\n",
      "Epoch 31 Loss 0.5365378260612488 acc : 0.5383283495903015 stop count : 0\n",
      "Epoch 32 Loss 0.5316660404205322 acc : 0.5336647033691406 stop count : 0\n",
      "Epoch 33 Loss 0.5270023345947266 acc : 0.5291677713394165 stop count : 0\n",
      "Epoch 34 Loss 0.5225812792778015 acc : 0.5248632431030273 stop count : 0\n",
      "Epoch 35 Loss 0.5183647871017456 acc : 0.520845890045166 stop count : 0\n",
      "Epoch 36 Loss 0.5143898129463196 acc : 0.5170431733131409 stop count : 0\n",
      "Epoch 37 Loss 0.5106319189071655 acc : 0.513437032699585 stop count : 0\n",
      "Epoch 38 Loss 0.5071292519569397 acc : 0.5100735425949097 stop count : 0\n",
      "Epoch 39 Loss 0.5038573741912842 acc : 0.5069648623466492 stop count : 0\n",
      "Epoch 40 Loss 0.5008257627487183 acc : 0.5040097832679749 stop count : 0\n",
      "Epoch 41 Loss 0.4979765713214874 acc : 0.5012133121490479 stop count : 0\n",
      "Epoch 42 Loss 0.4952704608440399 acc : 0.4985962212085724 stop count : 0\n",
      "Epoch 43 Loss 0.4927263557910919 acc : 0.49611204862594604 stop count : 0\n",
      "Epoch 44 Loss 0.4903261661529541 acc : 0.4938204884529114 stop count : 0\n",
      "Epoch 45 Loss 0.4880700409412384 acc : 0.4917469620704651 stop count : 0\n",
      "Epoch 46 Loss 0.4859386086463928 acc : 0.48986750841140747 stop count : 0\n",
      "Epoch 47 Loss 0.4839206337928772 acc : 0.4881282150745392 stop count : 0\n",
      "Epoch 48 Loss 0.48201245069503784 acc : 0.4865334630012512 stop count : 0\n",
      "Epoch 49 Loss 0.480177640914917 acc : 0.48503023386001587 stop count : 0\n",
      "Epoch 50 Loss 0.47841885685920715 acc : 0.4836023449897766 stop count : 0\n",
      "Epoch 51 Loss 0.4767254889011383 acc : 0.48222389817237854 stop count : 0\n",
      "Epoch 52 Loss 0.47509342432022095 acc : 0.48091423511505127 stop count : 0\n",
      "Epoch 53 Loss 0.4735146164894104 acc : 0.4795938730239868 stop count : 0\n",
      "Epoch 54 Loss 0.472041517496109 acc : 0.4785092771053314 stop count : 0\n",
      "Epoch 55 Loss 0.47077611088752747 acc : 0.4777134358882904 stop count : 0\n",
      "Epoch 56 Loss 0.46985381841659546 acc : 0.4773959219455719 stop count : 0\n",
      "Epoch 57 Loss 0.4693354368209839 acc : 0.4772930145263672 stop count : 0\n",
      "Epoch 58 Loss 0.4690910577774048 acc : 0.47725677490234375 stop count : 0\n",
      "Epoch 59 Loss 0.4688970148563385 acc : 0.47717833518981934 stop count : 0\n",
      "Epoch 60 Loss 0.4685794711112976 acc : 0.47690826654434204 stop count : 0\n",
      "Epoch 61 Loss 0.4680851995944977 acc : 0.47652509808540344 stop count : 0\n",
      "Epoch 62 Loss 0.4674450755119324 acc : 0.4761492609977722 stop count : 0\n",
      "Epoch 63 Loss 0.46674707531929016 acc : 0.4758349359035492 stop count : 0\n",
      "Epoch 64 Loss 0.4661113917827606 acc : 0.4757338762283325 stop count : 0\n",
      "Epoch 65 Loss 0.4656505584716797 acc : 0.4758545160293579 stop count : 1\n",
      "Epoch 66 Loss 0.4654116928577423 acc : 0.47610995173454285 stop count : 2\n",
      "Epoch 67 Loss 0.46534860134124756 acc : 0.47644367814064026 stop count : 3\n",
      "Epoch 68 Loss 0.46534866094589233 acc : 0.4767075777053833 stop count : 4\n",
      "Epoch 69 Loss 0.46530455350875854 acc : 0.47685787081718445 stop count : 5\n",
      "Epoch 70 Loss 0.4651567339897156 acc : 0.47689586877822876 stop count : 6\n",
      "Epoch 71 Loss 0.46491217613220215 acc : 0.4768514335155487 stop count : 7\n",
      "Epoch 72 Loss 0.46461963653564453 acc : 0.4767790734767914 stop count : 8\n",
      "Epoch 73 Loss 0.46433383226394653 acc : 0.4767535924911499 stop count : 9\n",
      "Epoch 74 Loss 0.4641152024269104 acc : 0.4768269956111908 stop count : 10\n",
      "Test Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_32884\\2305901559.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.6701057553291321 acc : 0.6370547413825989 stop count : 0\n",
      "Epoch 2 Loss 0.6694309115409851 acc : 0.6364783644676208 stop count : 0\n",
      "Epoch 3 Loss 0.6687676310539246 acc : 0.6356559991836548 stop count : 0\n",
      "Epoch 4 Loss 0.6678234338760376 acc : 0.6345279812812805 stop count : 0\n",
      "Epoch 5 Loss 0.6665349006652832 acc : 0.6331040263175964 stop count : 0\n",
      "Epoch 6 Loss 0.6648843884468079 acc : 0.6313164234161377 stop count : 0\n",
      "Epoch 7 Loss 0.662781298160553 acc : 0.6290525197982788 stop count : 0\n",
      "Epoch 8 Loss 0.6601089239120483 acc : 0.6262944340705872 stop count : 0\n",
      "Epoch 9 Loss 0.6568499207496643 acc : 0.6230124235153198 stop count : 0\n",
      "Epoch 10 Loss 0.652980387210846 acc : 0.6191859841346741 stop count : 0\n",
      "Epoch 11 Loss 0.6484526991844177 acc : 0.6149014234542847 stop count : 0\n",
      "Epoch 12 Loss 0.6432909369468689 acc : 0.6102950572967529 stop count : 0\n",
      "Epoch 13 Loss 0.6376293301582336 acc : 0.6054434180259705 stop count : 0\n",
      "Epoch 14 Loss 0.6316206455230713 acc : 0.6004497408866882 stop count : 0\n",
      "Epoch 15 Loss 0.6254319548606873 acc : 0.5954256057739258 stop count : 0\n",
      "Epoch 16 Loss 0.619145929813385 acc : 0.5903146266937256 stop count : 0\n",
      "Epoch 17 Loss 0.6126651167869568 acc : 0.5851422548294067 stop count : 0\n",
      "Epoch 18 Loss 0.6060743927955627 acc : 0.5798653364181519 stop count : 0\n",
      "Epoch 19 Loss 0.599431574344635 acc : 0.5744584798812866 stop count : 0\n",
      "Epoch 20 Loss 0.5927533507347107 acc : 0.5689217448234558 stop count : 0\n",
      "Epoch 21 Loss 0.5860486626625061 acc : 0.5633494853973389 stop count : 0\n",
      "Epoch 22 Loss 0.5794268250465393 acc : 0.5576591491699219 stop count : 0\n",
      "Epoch 23 Loss 0.5728334188461304 acc : 0.551883339881897 stop count : 0\n",
      "Epoch 24 Loss 0.5662962794303894 acc : 0.5461716651916504 stop count : 0\n",
      "Epoch 25 Loss 0.5599123239517212 acc : 0.5405001640319824 stop count : 0\n",
      "Epoch 26 Loss 0.5536925792694092 acc : 0.5349289774894714 stop count : 0\n",
      "Epoch 27 Loss 0.5477873682975769 acc : 0.5295302867889404 stop count : 0\n",
      "Epoch 28 Loss 0.542277991771698 acc : 0.5242688655853271 stop count : 0\n",
      "Epoch 29 Loss 0.537097156047821 acc : 0.5191141963005066 stop count : 0\n",
      "Epoch 30 Loss 0.5322456359863281 acc : 0.5139120221138 stop count : 0\n",
      "Epoch 31 Loss 0.5275838375091553 acc : 0.5088491439819336 stop count : 0\n",
      "Epoch 32 Loss 0.523162841796875 acc : 0.5040557384490967 stop count : 0\n",
      "Epoch 33 Loss 0.519024670124054 acc : 0.4995362162590027 stop count : 0\n",
      "Epoch 34 Loss 0.5150855779647827 acc : 0.4952913820743561 stop count : 0\n",
      "Epoch 35 Loss 0.5113362669944763 acc : 0.49130597710609436 stop count : 0\n",
      "Epoch 36 Loss 0.507745623588562 acc : 0.48752546310424805 stop count : 0\n",
      "Epoch 37 Loss 0.5042775869369507 acc : 0.4839099049568176 stop count : 0\n",
      "Epoch 38 Loss 0.5009027123451233 acc : 0.48044028878211975 stop count : 0\n",
      "Epoch 39 Loss 0.49763190746307373 acc : 0.47716495394706726 stop count : 0\n",
      "Epoch 40 Loss 0.49449580907821655 acc : 0.47420772910118103 stop count : 0\n",
      "Epoch 41 Loss 0.491685688495636 acc : 0.4717838168144226 stop count : 0\n",
      "Epoch 42 Loss 0.4893634617328644 acc : 0.4699978232383728 stop count : 0\n",
      "Epoch 43 Loss 0.487638920545578 acc : 0.46861377358436584 stop count : 0\n",
      "Epoch 44 Loss 0.4863124191761017 acc : 0.4674083888530731 stop count : 0\n",
      "Epoch 45 Loss 0.4851241707801819 acc : 0.4662049114704132 stop count : 0\n",
      "Epoch 46 Loss 0.483882337808609 acc : 0.4650034010410309 stop count : 0\n",
      "Epoch 47 Loss 0.4825737476348877 acc : 0.46376943588256836 stop count : 0\n",
      "Epoch 48 Loss 0.48119449615478516 acc : 0.46246740221977234 stop count : 0\n",
      "Epoch 49 Loss 0.4797144830226898 acc : 0.461216002702713 stop count : 0\n",
      "Epoch 50 Loss 0.4782804548740387 acc : 0.46004071831703186 stop count : 0\n",
      "Epoch 51 Loss 0.47696971893310547 acc : 0.45909157395362854 stop count : 0\n",
      "Epoch 52 Loss 0.4759208559989929 acc : 0.4585193395614624 stop count : 0\n",
      "Epoch 53 Loss 0.47525665163993835 acc : 0.4582415521144867 stop count : 0\n",
      "Epoch 54 Loss 0.47487708926200867 acc : 0.4581417143344879 stop count : 0\n",
      "Epoch 55 Loss 0.47468021512031555 acc : 0.45801228284835815 stop count : 0\n",
      "Epoch 56 Loss 0.4744609296321869 acc : 0.4577898681163788 stop count : 0\n",
      "Epoch 57 Loss 0.4741552174091339 acc : 0.45746853947639465 stop count : 0\n",
      "Epoch 58 Loss 0.47376665472984314 acc : 0.457080602645874 stop count : 0\n",
      "Epoch 59 Loss 0.47331827878952026 acc : 0.4567202627658844 stop count : 0\n",
      "Epoch 60 Loss 0.47291460633277893 acc : 0.4564524292945862 stop count : 0\n",
      "Epoch 61 Loss 0.4726021885871887 acc : 0.4563354551792145 stop count : 0\n",
      "Epoch 62 Loss 0.47243303060531616 acc : 0.4563150703907013 stop count : 0\n",
      "Epoch 63 Loss 0.4723720848560333 acc : 0.45631587505340576 stop count : 1\n",
      "Epoch 64 Loss 0.47233131527900696 acc : 0.4562799036502838 stop count : 0\n",
      "Epoch 65 Loss 0.4722606837749481 acc : 0.4561699330806732 stop count : 0\n",
      "Epoch 66 Loss 0.47212156653404236 acc : 0.4560159742832184 stop count : 0\n",
      "Epoch 67 Loss 0.4719477593898773 acc : 0.45586636662483215 stop count : 0\n",
      "Epoch 68 Loss 0.47177061438560486 acc : 0.45575541257858276 stop count : 0\n",
      "Epoch 69 Loss 0.4716280698776245 acc : 0.45570608973503113 stop count : 0\n",
      "Epoch 70 Loss 0.4715506434440613 acc : 0.45570823550224304 stop count : 1\n",
      "Epoch 71 Loss 0.47152388095855713 acc : 0.45571961998939514 stop count : 2\n",
      "Epoch 72 Loss 0.47150322794914246 acc : 0.45570796728134155 stop count : 3\n",
      "Epoch 73 Loss 0.47145941853523254 acc : 0.4556519389152527 stop count : 0\n",
      "Epoch 74 Loss 0.4713729918003082 acc : 0.45557069778442383 stop count : 0\n",
      "Epoch 75 Loss 0.47125059366226196 acc : 0.45548346638679504 stop count : 0\n",
      "Epoch 76 Loss 0.47111448645591736 acc : 0.45541301369667053 stop count : 0\n",
      "Epoch 77 Loss 0.4709944725036621 acc : 0.45537322759628296 stop count : 0\n",
      "Epoch 78 Loss 0.4708934426307678 acc : 0.4553588032722473 stop count : 0\n",
      "Epoch 79 Loss 0.4708133339881897 acc : 0.45534783601760864 stop count : 0\n",
      "Epoch 80 Loss 0.4707305133342743 acc : 0.4553319811820984 stop count : 0\n",
      "Epoch 81 Loss 0.4706265926361084 acc : 0.45529595017433167 stop count : 0\n",
      "Epoch 82 Loss 0.47049808502197266 acc : 0.45526123046875 stop count : 0\n",
      "Epoch 83 Loss 0.47034958004951477 acc : 0.45523887872695923 stop count : 0\n",
      "Epoch 84 Loss 0.4701916575431824 acc : 0.4552466869354248 stop count : 1\n",
      "Epoch 85 Loss 0.47003814578056335 acc : 0.45529091358184814 stop count : 2\n",
      "Epoch 86 Loss 0.46989262104034424 acc : 0.45535948872566223 stop count : 3\n",
      "Epoch 87 Loss 0.4697597920894623 acc : 0.45544669032096863 stop count : 4\n",
      "Epoch 88 Loss 0.46963825821876526 acc : 0.4555361568927765 stop count : 5\n",
      "Epoch 89 Loss 0.46952030062675476 acc : 0.4556305706501007 stop count : 6\n",
      "Epoch 90 Loss 0.46940234303474426 acc : 0.4557251036167145 stop count : 7\n",
      "Epoch 91 Loss 0.46927961707115173 acc : 0.45582830905914307 stop count : 8\n",
      "Epoch 92 Loss 0.4691596031188965 acc : 0.45591476559638977 stop count : 9\n",
      "Epoch 93 Loss 0.4690489172935486 acc : 0.45599326491355896 stop count : 10\n",
      "Test Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_32884\\2305901559.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.6551796793937683 acc : 0.6718658804893494 stop count : 0\n",
      "Epoch 2 Loss 0.6539228558540344 acc : 0.6707038879394531 stop count : 0\n",
      "Epoch 3 Loss 0.65277099609375 acc : 0.6693840026855469 stop count : 0\n",
      "Epoch 4 Loss 0.6514339447021484 acc : 0.6678219437599182 stop count : 0\n",
      "Epoch 5 Loss 0.6497986912727356 acc : 0.666073203086853 stop count : 0\n",
      "Epoch 6 Loss 0.6478617787361145 acc : 0.6642065644264221 stop count : 0\n",
      "Epoch 7 Loss 0.6456918120384216 acc : 0.6622459888458252 stop count : 0\n",
      "Epoch 8 Loss 0.6433862447738647 acc : 0.6601547598838806 stop count : 0\n",
      "Epoch 9 Loss 0.6410398483276367 acc : 0.6578838229179382 stop count : 0\n",
      "Epoch 10 Loss 0.6386851668357849 acc : 0.6553741097450256 stop count : 0\n",
      "Epoch 11 Loss 0.6362760663032532 acc : 0.6525833606719971 stop count : 0\n",
      "Epoch 12 Loss 0.6337364315986633 acc : 0.64952552318573 stop count : 0\n",
      "Epoch 13 Loss 0.6310027837753296 acc : 0.6462193727493286 stop count : 0\n",
      "Epoch 14 Loss 0.6280413866043091 acc : 0.6427146792411804 stop count : 0\n",
      "Epoch 15 Loss 0.6248241662979126 acc : 0.6390780806541443 stop count : 0\n",
      "Epoch 16 Loss 0.621371328830719 acc : 0.6353786587715149 stop count : 0\n",
      "Epoch 17 Loss 0.617749035358429 acc : 0.6316695213317871 stop count : 0\n",
      "Epoch 18 Loss 0.6140454411506653 acc : 0.6280116438865662 stop count : 0\n",
      "Epoch 19 Loss 0.6103376150131226 acc : 0.6244308948516846 stop count : 0\n",
      "Epoch 20 Loss 0.6066863536834717 acc : 0.6209425926208496 stop count : 0\n",
      "Epoch 21 Loss 0.6031102538108826 acc : 0.6175467371940613 stop count : 0\n",
      "Epoch 22 Loss 0.5996319055557251 acc : 0.6142043471336365 stop count : 0\n",
      "Epoch 23 Loss 0.5962349772453308 acc : 0.6108636856079102 stop count : 0\n",
      "Epoch 24 Loss 0.5928733348846436 acc : 0.6074501872062683 stop count : 0\n",
      "Epoch 25 Loss 0.5894439816474915 acc : 0.6039076447486877 stop count : 0\n",
      "Epoch 26 Loss 0.5858796834945679 acc : 0.6002358794212341 stop count : 0\n",
      "Epoch 27 Loss 0.5821715593338013 acc : 0.5964588522911072 stop count : 0\n",
      "Epoch 28 Loss 0.5783290863037109 acc : 0.592587411403656 stop count : 0\n",
      "Epoch 29 Loss 0.5743939280509949 acc : 0.5887179374694824 stop count : 0\n",
      "Epoch 30 Loss 0.5704562067985535 acc : 0.5850036144256592 stop count : 0\n",
      "Epoch 31 Loss 0.5666952729225159 acc : 0.5816951394081116 stop count : 0\n",
      "Epoch 32 Loss 0.5633401870727539 acc : 0.5790607929229736 stop count : 0\n",
      "Epoch 33 Loss 0.5606518983840942 acc : 0.5771008133888245 stop count : 0\n",
      "Epoch 34 Loss 0.5585865378379822 acc : 0.5754331350326538 stop count : 0\n",
      "Epoch 35 Loss 0.5567238926887512 acc : 0.5737758278846741 stop count : 0\n",
      "Epoch 36 Loss 0.5547987818717957 acc : 0.5721191763877869 stop count : 0\n",
      "Epoch 37 Loss 0.5528547763824463 acc : 0.5705083012580872 stop count : 0\n",
      "Epoch 38 Loss 0.550979495048523 acc : 0.5690010190010071 stop count : 0\n",
      "Epoch 39 Loss 0.5492329001426697 acc : 0.5675787329673767 stop count : 0\n",
      "Epoch 40 Loss 0.5476415157318115 acc : 0.5661460757255554 stop count : 0\n",
      "Epoch 41 Loss 0.5461514592170715 acc : 0.5647076368331909 stop count : 0\n",
      "Epoch 42 Loss 0.5447067618370056 acc : 0.5633187294006348 stop count : 0\n",
      "Epoch 43 Loss 0.5433247685432434 acc : 0.5620672106742859 stop count : 0\n",
      "Epoch 44 Loss 0.5420516133308411 acc : 0.560910701751709 stop count : 0\n",
      "Epoch 45 Loss 0.540852427482605 acc : 0.5597428679466248 stop count : 0\n",
      "Epoch 46 Loss 0.5395979881286621 acc : 0.5584786534309387 stop count : 0\n",
      "Epoch 47 Loss 0.5381566286087036 acc : 0.5571073889732361 stop count : 0\n",
      "Epoch 48 Loss 0.5364595055580139 acc : 0.5556516647338867 stop count : 0\n",
      "Epoch 49 Loss 0.5345209240913391 acc : 0.5540614724159241 stop count : 0\n",
      "Epoch 50 Loss 0.5323382616043091 acc : 0.5521513819694519 stop count : 0\n",
      "Epoch 51 Loss 0.5297776460647583 acc : 0.5496658086776733 stop count : 0\n",
      "Epoch 52 Loss 0.5266805291175842 acc : 0.5464733839035034 stop count : 0\n",
      "Epoch 53 Loss 0.522972583770752 acc : 0.5425603985786438 stop count : 0\n",
      "Epoch 54 Loss 0.5186756253242493 acc : 0.5380418300628662 stop count : 0\n",
      "Epoch 55 Loss 0.5139327049255371 acc : 0.5330958962440491 stop count : 0\n",
      "Epoch 56 Loss 0.5089800357818604 acc : 0.5280345678329468 stop count : 0\n",
      "Epoch 57 Loss 0.5041255354881287 acc : 0.5231514573097229 stop count : 0\n",
      "Epoch 58 Loss 0.4996374547481537 acc : 0.5186000466346741 stop count : 0\n",
      "Epoch 59 Loss 0.4955005645751953 acc : 0.5142247676849365 stop count : 0\n",
      "Epoch 60 Loss 0.49156853556632996 acc : 0.5100366473197937 stop count : 0\n",
      "Epoch 61 Loss 0.4878635108470917 acc : 0.5060871243476868 stop count : 0\n",
      "Epoch 62 Loss 0.48451781272888184 acc : 0.5024065971374512 stop count : 0\n",
      "Epoch 63 Loss 0.4815487861633301 acc : 0.4988405108451843 stop count : 0\n",
      "Epoch 64 Loss 0.47887134552001953 acc : 0.4954235553741455 stop count : 0\n",
      "Epoch 65 Loss 0.4764614403247833 acc : 0.4922594726085663 stop count : 0\n",
      "Epoch 66 Loss 0.47446638345718384 acc : 0.4894135296344757 stop count : 0\n",
      "Epoch 67 Loss 0.47282907366752625 acc : 0.4866842031478882 stop count : 0\n",
      "Epoch 68 Loss 0.4713464081287384 acc : 0.48416316509246826 stop count : 0\n",
      "Epoch 69 Loss 0.47003084421157837 acc : 0.4820556938648224 stop count : 0\n",
      "Epoch 70 Loss 0.46899867057800293 acc : 0.4803824722766876 stop count : 0\n",
      "Epoch 71 Loss 0.46819448471069336 acc : 0.4790615141391754 stop count : 0\n",
      "Epoch 72 Loss 0.46748217940330505 acc : 0.47801870107650757 stop count : 0\n",
      "Epoch 73 Loss 0.46680596470832825 acc : 0.4772251844406128 stop count : 0\n",
      "Epoch 74 Loss 0.4662007987499237 acc : 0.47662732005119324 stop count : 0\n",
      "Epoch 75 Loss 0.46568164229393005 acc : 0.4761807322502136 stop count : 0\n",
      "Epoch 76 Loss 0.465226948261261 acc : 0.475860595703125 stop count : 0\n",
      "Epoch 77 Loss 0.46484097838401794 acc : 0.47565916180610657 stop count : 0\n",
      "Epoch 78 Loss 0.4645157754421234 acc : 0.4754563570022583 stop count : 0\n",
      "Epoch 79 Loss 0.46416959166526794 acc : 0.4753260612487793 stop count : 0\n",
      "Epoch 80 Loss 0.4638271927833557 acc : 0.47530806064605713 stop count : 0\n",
      "Epoch 81 Loss 0.4635624289512634 acc : 0.4753239154815674 stop count : 1\n",
      "Epoch 82 Loss 0.46331167221069336 acc : 0.47529709339141846 stop count : 0\n",
      "Epoch 83 Loss 0.4629952907562256 acc : 0.4752924144268036 stop count : 0\n",
      "Epoch 84 Loss 0.46269866824150085 acc : 0.47535401582717896 stop count : 1\n",
      "Epoch 85 Loss 0.4624520242214203 acc : 0.4754299521446228 stop count : 2\n",
      "Epoch 86 Loss 0.46221065521240234 acc : 0.47553685307502747 stop count : 3\n",
      "Epoch 87 Loss 0.4619801342487335 acc : 0.47567257285118103 stop count : 4\n",
      "Epoch 88 Loss 0.4617837965488434 acc : 0.47580182552337646 stop count : 5\n",
      "Epoch 89 Loss 0.4615953266620636 acc : 0.4759478271007538 stop count : 6\n",
      "Epoch 90 Loss 0.4614311754703522 acc : 0.4760536849498749 stop count : 7\n",
      "Epoch 91 Loss 0.4613068401813507 acc : 0.47614169120788574 stop count : 8\n",
      "Epoch 92 Loss 0.4611683487892151 acc : 0.47622501850128174 stop count : 9\n",
      "Epoch 93 Loss 0.46103155612945557 acc : 0.4763358533382416 stop count : 10\n",
      "Test Accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_32884\\2305901559.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.6544307470321655 acc : 0.6603696346282959 stop count : 0\n",
      "Epoch 2 Loss 0.6510183215141296 acc : 0.6555991768836975 stop count : 0\n",
      "Epoch 3 Loss 0.6465136408805847 acc : 0.6496330499649048 stop count : 0\n",
      "Epoch 4 Loss 0.6408785581588745 acc : 0.6425679326057434 stop count : 0\n",
      "Epoch 5 Loss 0.6342251896858215 acc : 0.6346235871315002 stop count : 0\n",
      "Epoch 6 Loss 0.6266855001449585 acc : 0.6261028051376343 stop count : 0\n",
      "Epoch 7 Loss 0.6184796094894409 acc : 0.6172933578491211 stop count : 0\n",
      "Epoch 8 Loss 0.6098747849464417 acc : 0.6083110570907593 stop count : 0\n",
      "Epoch 9 Loss 0.6009758114814758 acc : 0.5990184545516968 stop count : 0\n",
      "Epoch 10 Loss 0.5916915535926819 acc : 0.5894243717193604 stop count : 0\n",
      "Epoch 11 Loss 0.5820196866989136 acc : 0.5797485709190369 stop count : 0\n",
      "Epoch 12 Loss 0.5722246766090393 acc : 0.5701069831848145 stop count : 0\n",
      "Epoch 13 Loss 0.5625175833702087 acc : 0.5607330799102783 stop count : 0\n",
      "Epoch 14 Loss 0.5530955195426941 acc : 0.5519550442695618 stop count : 0\n",
      "Epoch 15 Loss 0.5442397594451904 acc : 0.5438744425773621 stop count : 0\n",
      "Epoch 16 Loss 0.5361776947975159 acc : 0.5363674759864807 stop count : 0\n",
      "Epoch 17 Loss 0.5287798643112183 acc : 0.5291644334793091 stop count : 0\n",
      "Epoch 18 Loss 0.5217447876930237 acc : 0.5222156643867493 stop count : 0\n",
      "Epoch 19 Loss 0.5149784088134766 acc : 0.5157319903373718 stop count : 0\n",
      "Epoch 20 Loss 0.5085833072662354 acc : 0.5100018978118896 stop count : 0\n",
      "Epoch 21 Loss 0.5027629733085632 acc : 0.5052008032798767 stop count : 0\n",
      "Epoch 22 Loss 0.49768680334091187 acc : 0.5013695955276489 stop count : 0\n",
      "Epoch 23 Loss 0.49339357018470764 acc : 0.49821895360946655 stop count : 0\n",
      "Epoch 24 Loss 0.48973312973976135 acc : 0.49555251002311707 stop count : 0\n",
      "Epoch 25 Loss 0.4865587651729584 acc : 0.49329257011413574 stop count : 0\n",
      "Epoch 26 Loss 0.4838208556175232 acc : 0.4913944900035858 stop count : 0\n",
      "Epoch 27 Loss 0.48151665925979614 acc : 0.4899192452430725 stop count : 0\n",
      "Epoch 28 Loss 0.47960370779037476 acc : 0.48874765634536743 stop count : 0\n",
      "Epoch 29 Loss 0.4779549539089203 acc : 0.4877038300037384 stop count : 0\n",
      "Epoch 30 Loss 0.4764593541622162 acc : 0.48661667108535767 stop count : 0\n",
      "Epoch 31 Loss 0.47507423162460327 acc : 0.4856167733669281 stop count : 0\n",
      "Epoch 32 Loss 0.4738558530807495 acc : 0.48470836877822876 stop count : 0\n",
      "Epoch 33 Loss 0.47281819581985474 acc : 0.4840501844882965 stop count : 0\n",
      "Epoch 34 Loss 0.4719615578651428 acc : 0.48333096504211426 stop count : 0\n",
      "Epoch 35 Loss 0.47122475504875183 acc : 0.48264843225479126 stop count : 0\n",
      "Epoch 36 Loss 0.47052058577537537 acc : 0.48201560974121094 stop count : 0\n",
      "Epoch 37 Loss 0.46982377767562866 acc : 0.48143893480300903 stop count : 0\n",
      "Epoch 38 Loss 0.46914348006248474 acc : 0.48100459575653076 stop count : 0\n",
      "Epoch 39 Loss 0.468533456325531 acc : 0.4806186258792877 stop count : 0\n",
      "Epoch 40 Loss 0.46802079677581787 acc : 0.48025551438331604 stop count : 0\n",
      "Epoch 41 Loss 0.46758127212524414 acc : 0.4800219237804413 stop count : 0\n",
      "Epoch 42 Loss 0.46721309423446655 acc : 0.4798217713832855 stop count : 0\n",
      "Epoch 43 Loss 0.4668937623500824 acc : 0.4796229898929596 stop count : 0\n",
      "Epoch 44 Loss 0.4665738642215729 acc : 0.47950688004493713 stop count : 0\n",
      "Epoch 45 Loss 0.4662524163722992 acc : 0.47940197587013245 stop count : 0\n",
      "Epoch 46 Loss 0.46596410870552063 acc : 0.47937047481536865 stop count : 0\n",
      "Epoch 47 Loss 0.46574103832244873 acc : 0.47936904430389404 stop count : 0\n",
      "Epoch 48 Loss 0.46555060148239136 acc : 0.47928428649902344 stop count : 0\n",
      "Epoch 49 Loss 0.46537351608276367 acc : 0.4792717695236206 stop count : 0\n",
      "Epoch 50 Loss 0.4652116894721985 acc : 0.4792570471763611 stop count : 0\n",
      "Epoch 51 Loss 0.46506550908088684 acc : 0.4792034327983856 stop count : 0\n",
      "Epoch 52 Loss 0.46492302417755127 acc : 0.4792080819606781 stop count : 1\n",
      "Epoch 53 Loss 0.46478134393692017 acc : 0.47919052839279175 stop count : 0\n",
      "Epoch 54 Loss 0.46464744210243225 acc : 0.4791909456253052 stop count : 1\n",
      "Epoch 55 Loss 0.4645345211029053 acc : 0.47919484972953796 stop count : 2\n",
      "Epoch 56 Loss 0.46442171931266785 acc : 0.4791482388973236 stop count : 0\n",
      "Epoch 57 Loss 0.4642846882343292 acc : 0.4791094660758972 stop count : 0\n",
      "Epoch 58 Loss 0.46414440870285034 acc : 0.4790521264076233 stop count : 0\n",
      "Epoch 59 Loss 0.46402785181999207 acc : 0.4790308177471161 stop count : 0\n",
      "Epoch 60 Loss 0.463922917842865 acc : 0.4790109694004059 stop count : 0\n",
      "Epoch 61 Loss 0.4638241231441498 acc : 0.4789770543575287 stop count : 0\n",
      "Epoch 62 Loss 0.4637317657470703 acc : 0.47898975014686584 stop count : 1\n",
      "Epoch 63 Loss 0.4636439085006714 acc : 0.4789590537548065 stop count : 0\n",
      "Epoch 64 Loss 0.4635518193244934 acc : 0.47894203662872314 stop count : 0\n",
      "Epoch 65 Loss 0.46345287561416626 acc : 0.4789107143878937 stop count : 0\n",
      "Epoch 66 Loss 0.4633561074733734 acc : 0.47891297936439514 stop count : 1\n",
      "Epoch 67 Loss 0.46326276659965515 acc : 0.47890397906303406 stop count : 0\n",
      "Epoch 68 Loss 0.4631618857383728 acc : 0.4789077043533325 stop count : 1\n",
      "Epoch 69 Loss 0.4630548655986786 acc : 0.47892430424690247 stop count : 2\n",
      "Epoch 70 Loss 0.46295714378356934 acc : 0.47892114520072937 stop count : 3\n",
      "Epoch 71 Loss 0.4628784656524658 acc : 0.4789507985115051 stop count : 4\n",
      "Epoch 72 Loss 0.46281740069389343 acc : 0.4789430797100067 stop count : 5\n",
      "Epoch 73 Loss 0.4627542197704315 acc : 0.47898516058921814 stop count : 6\n",
      "Epoch 74 Loss 0.4626840651035309 acc : 0.4789724051952362 stop count : 7\n",
      "Epoch 75 Loss 0.4626077115535736 acc : 0.4790164828300476 stop count : 8\n",
      "Epoch 76 Loss 0.4625304341316223 acc : 0.47899818420410156 stop count : 9\n",
      "Epoch 77 Loss 0.4624498188495636 acc : 0.4790094494819641 stop count : 10\n",
      "Test Accuracy: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_32884\\2305901559.py:63: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.667733907699585 acc : 0.643064558506012 stop count : 0\n",
      "Epoch 2 Loss 0.666910707950592 acc : 0.642401933670044 stop count : 0\n",
      "Epoch 3 Loss 0.6661825776100159 acc : 0.6415269374847412 stop count : 0\n",
      "Epoch 4 Loss 0.6652516722679138 acc : 0.6403061747550964 stop count : 0\n",
      "Epoch 5 Loss 0.6640011668205261 acc : 0.6386641263961792 stop count : 0\n",
      "Epoch 6 Loss 0.6623743772506714 acc : 0.6365231275558472 stop count : 0\n",
      "Epoch 7 Loss 0.6603052616119385 acc : 0.6337823867797852 stop count : 0\n",
      "Epoch 8 Loss 0.6576862931251526 acc : 0.6303697228431702 stop count : 0\n",
      "Epoch 9 Loss 0.6544212698936462 acc : 0.6262366771697998 stop count : 0\n",
      "Epoch 10 Loss 0.6504656076431274 acc : 0.6213874220848083 stop count : 0\n",
      "Epoch 11 Loss 0.6458415389060974 acc : 0.6158982515335083 stop count : 0\n",
      "Epoch 12 Loss 0.640623927116394 acc : 0.6098947525024414 stop count : 0\n",
      "Epoch 13 Loss 0.6348841786384583 acc : 0.6034853458404541 stop count : 0\n",
      "Epoch 14 Loss 0.6286410093307495 acc : 0.5967549681663513 stop count : 0\n",
      "Epoch 15 Loss 0.6218599081039429 acc : 0.5897446274757385 stop count : 0\n",
      "Epoch 16 Loss 0.6145448684692383 acc : 0.5825140476226807 stop count : 0\n",
      "Epoch 17 Loss 0.606823742389679 acc : 0.5751517415046692 stop count : 0\n",
      "Epoch 18 Loss 0.5988597869873047 acc : 0.5677996277809143 stop count : 0\n",
      "Epoch 19 Loss 0.5908058881759644 acc : 0.5605634450912476 stop count : 0\n",
      "Epoch 20 Loss 0.5827987790107727 acc : 0.5535176396369934 stop count : 0\n",
      "Epoch 21 Loss 0.5749741196632385 acc : 0.5467244386672974 stop count : 0\n",
      "Epoch 22 Loss 0.5674331784248352 acc : 0.5403106808662415 stop count : 0\n",
      "Epoch 23 Loss 0.5602731108665466 acc : 0.5343254804611206 stop count : 0\n",
      "Epoch 24 Loss 0.5535593032836914 acc : 0.5288422107696533 stop count : 0\n",
      "Epoch 25 Loss 0.5473340153694153 acc : 0.5238563418388367 stop count : 0\n",
      "Epoch 26 Loss 0.5416160821914673 acc : 0.5192326903343201 stop count : 0\n",
      "Epoch 27 Loss 0.536332368850708 acc : 0.5148515105247498 stop count : 0\n",
      "Epoch 28 Loss 0.531457245349884 acc : 0.5107629299163818 stop count : 0\n",
      "Epoch 29 Loss 0.5269694328308105 acc : 0.5069938898086548 stop count : 0\n",
      "Epoch 30 Loss 0.5228426456451416 acc : 0.5035887360572815 stop count : 0\n",
      "Epoch 31 Loss 0.5191328525543213 acc : 0.5005279183387756 stop count : 0\n",
      "Epoch 32 Loss 0.5158297419548035 acc : 0.4976762533187866 stop count : 0\n",
      "Epoch 33 Loss 0.5128119587898254 acc : 0.4949868619441986 stop count : 0\n",
      "Epoch 34 Loss 0.5100212097167969 acc : 0.49246078729629517 stop count : 0\n",
      "Epoch 35 Loss 0.5074301362037659 acc : 0.4901200234889984 stop count : 0\n",
      "Epoch 36 Loss 0.5050325393676758 acc : 0.4880410432815552 stop count : 0\n",
      "Epoch 37 Loss 0.5029046535491943 acc : 0.48614996671676636 stop count : 0\n",
      "Epoch 38 Loss 0.5009974837303162 acc : 0.4844220280647278 stop count : 0\n",
      "Epoch 39 Loss 0.49924173951148987 acc : 0.48290351033210754 stop count : 0\n",
      "Epoch 40 Loss 0.4977031648159027 acc : 0.4815998673439026 stop count : 0\n",
      "Epoch 41 Loss 0.4963877201080322 acc : 0.4804438650608063 stop count : 0\n",
      "Epoch 42 Loss 0.49525517225265503 acc : 0.47938159108161926 stop count : 0\n",
      "Epoch 43 Loss 0.49423807859420776 acc : 0.4784127473831177 stop count : 0\n",
      "Epoch 44 Loss 0.49329686164855957 acc : 0.4775668680667877 stop count : 0\n",
      "Epoch 45 Loss 0.4924827218055725 acc : 0.47677746415138245 stop count : 0\n",
      "Epoch 46 Loss 0.49175599217414856 acc : 0.47602155804634094 stop count : 0\n",
      "Epoch 47 Loss 0.4910856783390045 acc : 0.4753109812736511 stop count : 0\n",
      "Epoch 48 Loss 0.4904642403125763 acc : 0.47465190291404724 stop count : 0\n",
      "Epoch 49 Loss 0.489872545003891 acc : 0.47405511140823364 stop count : 0\n",
      "Epoch 50 Loss 0.4893392324447632 acc : 0.47344568371772766 stop count : 0\n",
      "Epoch 51 Loss 0.48881223797798157 acc : 0.4728429615497589 stop count : 0\n",
      "Epoch 52 Loss 0.4882868528366089 acc : 0.4722955822944641 stop count : 0\n",
      "Epoch 53 Loss 0.4877875745296478 acc : 0.47179266810417175 stop count : 0\n",
      "Epoch 54 Loss 0.4873075485229492 acc : 0.47128868103027344 stop count : 0\n",
      "Epoch 55 Loss 0.48682770133018494 acc : 0.47079241275787354 stop count : 0\n",
      "Epoch 56 Loss 0.4863507151603699 acc : 0.47034141421318054 stop count : 0\n",
      "Epoch 57 Loss 0.4858889877796173 acc : 0.4699186384677887 stop count : 0\n",
      "Epoch 58 Loss 0.48543673753738403 acc : 0.4694893956184387 stop count : 0\n",
      "Epoch 59 Loss 0.4849863052368164 acc : 0.46906134486198425 stop count : 0\n",
      "Epoch 60 Loss 0.4845416247844696 acc : 0.46866294741630554 stop count : 0\n",
      "Epoch 61 Loss 0.48411035537719727 acc : 0.4682905077934265 stop count : 0\n",
      "Epoch 62 Loss 0.4836927652359009 acc : 0.46791669726371765 stop count : 0\n",
      "Epoch 63 Loss 0.48327961564064026 acc : 0.4675650894641876 stop count : 0\n",
      "Epoch 64 Loss 0.482884019613266 acc : 0.4672430455684662 stop count : 0\n",
      "Epoch 65 Loss 0.48250165581703186 acc : 0.46692612767219543 stop count : 0\n",
      "Epoch 66 Loss 0.4821260869503021 acc : 0.46660280227661133 stop count : 0\n",
      "Epoch 67 Loss 0.48176029324531555 acc : 0.4662933349609375 stop count : 0\n",
      "Epoch 68 Loss 0.4814082086086273 acc : 0.4660148024559021 stop count : 0\n",
      "Epoch 69 Loss 0.4810657203197479 acc : 0.46574947237968445 stop count : 0\n",
      "Epoch 70 Loss 0.4807320535182953 acc : 0.4654892086982727 stop count : 0\n",
      "Epoch 71 Loss 0.4804084002971649 acc : 0.4652356207370758 stop count : 0\n",
      "Epoch 72 Loss 0.4800959527492523 acc : 0.46499407291412354 stop count : 0\n",
      "Epoch 73 Loss 0.47978675365448 acc : 0.4647523760795593 stop count : 0\n",
      "Epoch 74 Loss 0.4794861376285553 acc : 0.4645158648490906 stop count : 0\n",
      "Epoch 75 Loss 0.47919735312461853 acc : 0.4642934501171112 stop count : 0\n",
      "Epoch 76 Loss 0.4789144992828369 acc : 0.46408143639564514 stop count : 0\n",
      "Epoch 77 Loss 0.4786395728588104 acc : 0.4638729393482208 stop count : 0\n",
      "Epoch 78 Loss 0.4783723056316376 acc : 0.4636574983596802 stop count : 0\n",
      "Epoch 79 Loss 0.4781090319156647 acc : 0.46344611048698425 stop count : 0\n",
      "Epoch 80 Loss 0.4778490662574768 acc : 0.4632338583469391 stop count : 0\n",
      "Epoch 81 Loss 0.47759518027305603 acc : 0.46303287148475647 stop count : 0\n",
      "Epoch 82 Loss 0.47734394669532776 acc : 0.46282821893692017 stop count : 0\n",
      "Epoch 83 Loss 0.47708508372306824 acc : 0.4626290798187256 stop count : 0\n",
      "Epoch 84 Loss 0.47682949900627136 acc : 0.4624429941177368 stop count : 0\n",
      "Epoch 85 Loss 0.4765915274620056 acc : 0.46226295828819275 stop count : 0\n",
      "Epoch 86 Loss 0.47636061906814575 acc : 0.4620794951915741 stop count : 0\n",
      "Epoch 87 Loss 0.47612330317497253 acc : 0.4618971645832062 stop count : 0\n",
      "Epoch 88 Loss 0.4758877158164978 acc : 0.46173372864723206 stop count : 0\n",
      "Epoch 89 Loss 0.47566401958465576 acc : 0.46157675981521606 stop count : 0\n",
      "Epoch 90 Loss 0.475445032119751 acc : 0.4614107310771942 stop count : 0\n",
      "Epoch 91 Loss 0.47522395849227905 acc : 0.4612521827220917 stop count : 0\n",
      "Epoch 92 Loss 0.47501063346862793 acc : 0.4611043632030487 stop count : 0\n",
      "Epoch 93 Loss 0.47480785846710205 acc : 0.46095195412635803 stop count : 0\n",
      "Epoch 94 Loss 0.47460323572158813 acc : 0.46080294251441956 stop count : 0\n",
      "Epoch 95 Loss 0.47439906001091003 acc : 0.46066319942474365 stop count : 0\n",
      "Epoch 96 Loss 0.4742042124271393 acc : 0.4605230987071991 stop count : 0\n",
      "Epoch 97 Loss 0.4740148186683655 acc : 0.4603838622570038 stop count : 0\n",
      "Epoch 98 Loss 0.4738256633281708 acc : 0.46024903655052185 stop count : 0\n",
      "Epoch 99 Loss 0.4736403524875641 acc : 0.4601188898086548 stop count : 0\n",
      "Epoch 100 Loss 0.47345802187919617 acc : 0.4599919319152832 stop count : 0\n",
      "Epoch 101 Loss 0.4732743501663208 acc : 0.4598686695098877 stop count : 0\n",
      "Epoch 102 Loss 0.4730912446975708 acc : 0.4597489535808563 stop count : 0\n",
      "Epoch 103 Loss 0.47291186451911926 acc : 0.459636390209198 stop count : 0\n",
      "Epoch 104 Loss 0.4727403521537781 acc : 0.45953288674354553 stop count : 0\n",
      "Epoch 105 Loss 0.4725709557533264 acc : 0.4594357907772064 stop count : 0\n",
      "Epoch 106 Loss 0.47239968180656433 acc : 0.45934414863586426 stop count : 0\n",
      "Epoch 107 Loss 0.47222861647605896 acc : 0.4592541754245758 stop count : 0\n",
      "Epoch 108 Loss 0.47205856442451477 acc : 0.45916908979415894 stop count : 0\n",
      "Epoch 109 Loss 0.4718891978263855 acc : 0.4590868651866913 stop count : 0\n",
      "Epoch 110 Loss 0.47172150015830994 acc : 0.4590052664279938 stop count : 0\n",
      "Epoch 111 Loss 0.47155508399009705 acc : 0.4589264690876007 stop count : 0\n",
      "Epoch 112 Loss 0.4713921844959259 acc : 0.4588518738746643 stop count : 0\n",
      "Epoch 113 Loss 0.47123199701309204 acc : 0.45877325534820557 stop count : 0\n",
      "Epoch 114 Loss 0.4710715115070343 acc : 0.4586966931819916 stop count : 0\n",
      "Epoch 115 Loss 0.47091221809387207 acc : 0.45862290263175964 stop count : 0\n",
      "Epoch 116 Loss 0.47075289487838745 acc : 0.4585520923137665 stop count : 0\n",
      "Epoch 117 Loss 0.4705944359302521 acc : 0.458486944437027 stop count : 0\n",
      "Epoch 118 Loss 0.47043853998184204 acc : 0.4584275186061859 stop count : 0\n",
      "Epoch 119 Loss 0.4702870845794678 acc : 0.4583730399608612 stop count : 0\n",
      "Epoch 120 Loss 0.4701404273509979 acc : 0.4583226144313812 stop count : 0\n",
      "Epoch 121 Loss 0.4699975550174713 acc : 0.4582771062850952 stop count : 0\n",
      "Epoch 122 Loss 0.46985918283462524 acc : 0.45823541283607483 stop count : 0\n",
      "Epoch 123 Loss 0.4697261452674866 acc : 0.4582001864910126 stop count : 0\n",
      "Epoch 124 Loss 0.46959856152534485 acc : 0.45816731452941895 stop count : 0\n",
      "Epoch 125 Loss 0.46947574615478516 acc : 0.45813876390457153 stop count : 0\n",
      "Epoch 126 Loss 0.46935632824897766 acc : 0.45811113715171814 stop count : 0\n",
      "Epoch 127 Loss 0.46924078464508057 acc : 0.45808446407318115 stop count : 0\n",
      "Epoch 128 Loss 0.46912774443626404 acc : 0.45805251598358154 stop count : 0\n",
      "Epoch 129 Loss 0.46901416778564453 acc : 0.45801877975463867 stop count : 0\n",
      "Epoch 130 Loss 0.468904048204422 acc : 0.4579872488975525 stop count : 0\n",
      "Epoch 131 Loss 0.4688031077384949 acc : 0.45796865224838257 stop count : 0\n",
      "Epoch 132 Loss 0.4687156081199646 acc : 0.4579516351222992 stop count : 0\n",
      "Epoch 133 Loss 0.4686362147331238 acc : 0.4579174518585205 stop count : 0\n",
      "Epoch 134 Loss 0.46853867173194885 acc : 0.45785847306251526 stop count : 0\n",
      "Epoch 135 Loss 0.46841880679130554 acc : 0.4578181803226471 stop count : 0\n",
      "Epoch 136 Loss 0.46831822395324707 acc : 0.45778313279151917 stop count : 0\n",
      "Epoch 137 Loss 0.46823883056640625 acc : 0.4577297568321228 stop count : 0\n",
      "Epoch 138 Loss 0.4681393504142761 acc : 0.4576714038848877 stop count : 0\n",
      "Epoch 139 Loss 0.4680268466472626 acc : 0.4576250910758972 stop count : 0\n",
      "Epoch 140 Loss 0.467944860458374 acc : 0.4575926661491394 stop count : 0\n",
      "Epoch 141 Loss 0.4678710997104645 acc : 0.45753729343414307 stop count : 0\n",
      "Epoch 142 Loss 0.46777984499931335 acc : 0.4574848711490631 stop count : 0\n",
      "Epoch 143 Loss 0.4676947593688965 acc : 0.4574580192565918 stop count : 0\n",
      "Epoch 144 Loss 0.46762242913246155 acc : 0.4574156701564789 stop count : 0\n",
      "Epoch 145 Loss 0.46754592657089233 acc : 0.45737841725349426 stop count : 0\n",
      "Epoch 146 Loss 0.46746817231178284 acc : 0.45734986662864685 stop count : 0\n",
      "Epoch 147 Loss 0.46739834547042847 acc : 0.4573155343532562 stop count : 0\n",
      "Epoch 148 Loss 0.4673311114311218 acc : 0.45729362964630127 stop count : 0\n",
      "Epoch 149 Loss 0.4672623872756958 acc : 0.45727378129959106 stop count : 0\n",
      "Epoch 150 Loss 0.4671964645385742 acc : 0.4572489857673645 stop count : 0\n",
      "Epoch 151 Loss 0.46713119745254517 acc : 0.45723187923431396 stop count : 0\n",
      "Epoch 152 Loss 0.4670676290988922 acc : 0.4572066068649292 stop count : 0\n",
      "Epoch 153 Loss 0.467011034488678 acc : 0.4571908116340637 stop count : 0\n",
      "Epoch 154 Loss 0.46695566177368164 acc : 0.4571617841720581 stop count : 0\n",
      "Epoch 155 Loss 0.4668964445590973 acc : 0.4571481943130493 stop count : 0\n",
      "Epoch 156 Loss 0.4668426811695099 acc : 0.4571225941181183 stop count : 0\n",
      "Epoch 157 Loss 0.4667946994304657 acc : 0.4570971131324768 stop count : 0\n",
      "Epoch 158 Loss 0.4667382538318634 acc : 0.45706748962402344 stop count : 0\n",
      "Epoch 159 Loss 0.46668174862861633 acc : 0.45705151557922363 stop count : 0\n",
      "Epoch 160 Loss 0.4666424095630646 acc : 0.4570547640323639 stop count : 1\n",
      "Epoch 161 Loss 0.4666141867637634 acc : 0.4570349156856537 stop count : 0\n",
      "Epoch 162 Loss 0.4665775001049042 acc : 0.456999272108078 stop count : 0\n",
      "Epoch 163 Loss 0.466511070728302 acc : 0.4569534361362457 stop count : 0\n",
      "Epoch 164 Loss 0.46643146872520447 acc : 0.4569331705570221 stop count : 0\n",
      "Epoch 165 Loss 0.46638742089271545 acc : 0.45693764090538025 stop count : 1\n",
      "Epoch 166 Loss 0.46636539697647095 acc : 0.4569103717803955 stop count : 0\n",
      "Epoch 167 Loss 0.46631231904029846 acc : 0.45686718821525574 stop count : 0\n",
      "Epoch 168 Loss 0.46624496579170227 acc : 0.4568520784378052 stop count : 0\n",
      "Epoch 169 Loss 0.4662110507488251 acc : 0.45683753490448 stop count : 0\n",
      "Epoch 170 Loss 0.46617940068244934 acc : 0.45680463314056396 stop count : 0\n",
      "Epoch 171 Loss 0.466120183467865 acc : 0.4567774832248688 stop count : 0\n",
      "Epoch 172 Loss 0.46607330441474915 acc : 0.45677024126052856 stop count : 0\n",
      "Epoch 173 Loss 0.4660448133945465 acc : 0.4567486643791199 stop count : 0\n",
      "Epoch 174 Loss 0.46599698066711426 acc : 0.45672541856765747 stop count : 0\n",
      "Epoch 175 Loss 0.465944766998291 acc : 0.4567190706729889 stop count : 0\n",
      "Epoch 176 Loss 0.4659103453159332 acc : 0.4567136764526367 stop count : 0\n",
      "Epoch 177 Loss 0.4658689498901367 acc : 0.45669639110565186 stop count : 0\n",
      "Epoch 178 Loss 0.4658162593841553 acc : 0.4566916823387146 stop count : 0\n",
      "Epoch 179 Loss 0.4657745361328125 acc : 0.4566880166530609 stop count : 0\n",
      "Epoch 180 Loss 0.4657347500324249 acc : 0.4566749334335327 stop count : 0\n",
      "Epoch 181 Loss 0.46568262577056885 acc : 0.4566623270511627 stop count : 0\n",
      "Epoch 182 Loss 0.4656325578689575 acc : 0.45666244626045227 stop count : 1\n",
      "Epoch 183 Loss 0.4655895531177521 acc : 0.4566496014595032 stop count : 0\n",
      "Epoch 184 Loss 0.46554261445999146 acc : 0.45664530992507935 stop count : 0\n",
      "Epoch 185 Loss 0.46549755334854126 acc : 0.45664116740226746 stop count : 0\n",
      "Epoch 186 Loss 0.46546265482902527 acc : 0.45665451884269714 stop count : 1\n",
      "Epoch 187 Loss 0.4654287099838257 acc : 0.45664504170417786 stop count : 2\n",
      "Epoch 188 Loss 0.4653812050819397 acc : 0.45664772391319275 stop count : 3\n",
      "Epoch 189 Loss 0.46532174944877625 acc : 0.4566487967967987 stop count : 4\n",
      "Epoch 190 Loss 0.46526899933815 acc : 0.4566621482372284 stop count : 5\n",
      "Epoch 191 Loss 0.46523362398147583 acc : 0.4566802680492401 stop count : 6\n",
      "Epoch 192 Loss 0.4652065932750702 acc : 0.4566872715950012 stop count : 7\n",
      "Epoch 193 Loss 0.46517354249954224 acc : 0.456693559885025 stop count : 8\n",
      "Epoch 194 Loss 0.46513164043426514 acc : 0.456691175699234 stop count : 9\n",
      "Epoch 195 Loss 0.4650866389274597 acc : 0.4566805064678192 stop count : 10\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "result_list = []\n",
    "for seed in [115,151,123,8,25]:\n",
    "    # 데이터 로드 및 전처리\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(n_components=8)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=seed)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Pennylane 장치 설정\n",
    "    dev = qml.device(\"default.qubit\", wires=8)\n",
    "\n",
    "\n",
    "    def ZZFeatureMapLayer(features, wires):\n",
    "        \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
    "        index = 0\n",
    "        for i in wires:\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(features[:,index], wires=i)\n",
    "            index += 1\n",
    "        for i in range(1):\n",
    "            for j in range(i + 1, len(wires)):\n",
    "                qml.CNOT(wires=[i, j])\n",
    "                qml.RZ((features[:,index]), wires=j)\n",
    "                qml.CNOT(wires=[i, j])\n",
    "                index+=1\n",
    "\n",
    "    # 양자 레이어 정의\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def QuantumLayer(features,params):\n",
    "        ZZFeatureMapLayer(features, wires=range(8))\n",
    "        qml.BasicEntanglerLayers(params, wires=range(8),rotation=qml.RX)\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "    # 하이브리드 모델 정의\n",
    "    class HybridModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(HybridModel, self).__init__()\n",
    "            self.cls_layer_1 = nn.Linear(8,8*2)\n",
    "            self.cls_layer_2 = nn.Linear(8*2,8*2-1)\n",
    "            \n",
    "            self.quantum_layer = QuantumLayer\n",
    "            Q_params = nn.Parameter(torch.rand([2,8],requires_grad=True))\n",
    "            self.Q_params = torch.nn.init.xavier_uniform(Q_params)\n",
    "        def forward(self, x):\n",
    "            x = self.cls_layer_1(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.cls_layer_2(x)\n",
    "            x = nn.Sigmoid()(x)\n",
    "            x = x*np.pi\n",
    "            #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
    "            quantum_output = self.quantum_layer(x,self.Q_params)\n",
    "            quantum_output = quantum_output.type(torch.float32)\n",
    "            quantum_output = (quantum_output+1)/2\n",
    "            return quantum_output\n",
    "        \n",
    "    class Model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Model, self).__init__()\n",
    "            self.cls_layer_1 = nn.Linear(8,8*2)\n",
    "            self.cls_layer_2 = nn.Linear(8*2,8*2-1)\n",
    "            self.output_layer = nn.Linear(8*2-1,1)\n",
    "        def forward(self, x):\n",
    "            x = self.cls_layer_1(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            x = self.cls_layer_2(x)\n",
    "            x = nn.ReLU()(x)\n",
    "            output = self.output_layer(x)\n",
    "            output = nn.Sigmoid()(output)\n",
    "            return output\n",
    "    # 모델, 손실 함수, 최적화 설정\n",
    "    model = HybridModel()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    # 모델 학습 및 평가\n",
    "    train_process = Early_stop_train(model, optimizer, criterion)\n",
    "    train_process.train_model()\n",
    "    accuracy = train_process.evaluate_model()\n",
    "    result_list.append(accuracy)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967251461988304"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(result_list_classical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967251461988304"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터셋 불러오기\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA 적용하여 특성을 4개로 축소\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Pennylane 설정\n",
    "dev = qml.device('default.qubit', wires=4)\n",
    "\n",
    "# 양자 회로 정의\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(features, weights):\n",
    "    qml.AngleEmbedding(features, wires=range(4))\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(4))\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# 가중치 초기화\n",
    "num_layers = 3\n",
    "weights = 0.1 * np.random.randn(num_layers, 4)\n",
    "\n",
    "# 비용 함수\n",
    "def cost(weights, features, labels):\n",
    "    predictions = [quantum_circuit(features[i], weights) for i in range(len(features))]\n",
    "    return ((predictions - labels) ** 2).mean()\n",
    "\n",
    "# 옵티마이저\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.1)\n",
    "\n",
    "# 학습\n",
    "num_steps = 100\n",
    "for step in range(num_steps):\n",
    "    weights, prev_cost = opt.step_and_cost(lambda w: cost(w, X_train, y_train), weights)\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step: {step}, Cost: {prev_cost}\")\n",
    "\n",
    "# 평가\n",
    "predictions = [quantum_circuit(X_test[i], weights) for i in range(len(X_test))]\n",
    "predictions = np.array(predictions)\n",
    "predictions = np.where(predictions < 0.5, 0, 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "\n",
    "dev = qml.device('default.qubit', wires=2)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def circuit4(phi, theta):\n",
    "    qml.RX(phi[0], wires=0)\n",
    "    qml.RZ(phi[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RX(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def cost(phi, theta):\n",
    "    return torch.abs(circuit4(phi, theta) - 0.5)**2\n",
    "\n",
    "phi = torch.tensor([0.011, 0.012], requires_grad=True)\n",
    "theta = torch.tensor(0.05, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([phi, theta], lr = 0.1)\n",
    "\n",
    "steps = 200\n",
    "\n",
    "def closure():\n",
    "    opt.zero_grad()\n",
    "    loss = cost(phi, theta)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for i in range(steps):\n",
    "    opt.step(closure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
