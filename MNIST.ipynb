{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vJl4rYpKBAEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "PCA_dim = 8\n",
        "CLS_num = 2\n",
        "\n",
        "\n",
        "\n",
        "with open('./data.pkl','rb') as file:\n",
        "    data = pickle.load(file)\n",
        "X = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "def Fit_to_quantum(X,PCA_dim):\n",
        "    pca = PCA(n_components=PCA_dim)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "class Feature_data_loader(Dataset):\n",
        "    def __init__(self,x_train,y_train):\n",
        "        self.feature1 = x_train\n",
        "        temp = copy.deepcopy(x_train)\n",
        "        shuffle = torch.randperm(len(temp))\n",
        "        self.feature2 = temp[shuffle]\n",
        "        self.y1 = y_train\n",
        "        temp_y = copy.deepcopy(y_train)\n",
        "        self.y2 = temp_y[shuffle]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.feature1)\n",
        "    def __getitem__(self,idx):\n",
        "        input1 = self.feature1[idx]\n",
        "        input2 = self.feature2[idx]\n",
        "        if self.y1[idx] == self.y2[idx]:\n",
        "            label = torch.tensor(1.).float()\n",
        "        else:\n",
        "            label = torch.tensor(0.).float()\n",
        "        return [input1,input2],label\n",
        "\n",
        "\n",
        "# DataLoader 생성\n",
        "\n",
        "\n",
        "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
        "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
        "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N4Y4dnYJBAEt"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred, true):\n",
        "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
        "    pred = pred.detach().cpu()\n",
        "    true = true.cpu()\n",
        "    try:\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "    except:\n",
        "        pred_labels = torch.round(pred)\n",
        "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
        "    correct = (pred_labels == true).sum()\n",
        "    # 정확도를 계산\n",
        "    acc = correct / true.size(0)\n",
        "    return acc.item()\n",
        "\n",
        "class Early_stop_train():\n",
        "    def __init__(self,model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "\n",
        "        self.loss_list = [1e100]\n",
        "        self.stop_count = 0\n",
        "\n",
        "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            if self.stop_count>=res:\n",
        "                break\n",
        "            loss_val,_ = self.test(test_loader)\n",
        "            self.loss_list.append(loss_val)\n",
        "\n",
        "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
        "                self.stop_count+=1\n",
        "            else:\n",
        "                self.stop_count = 0\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "            for X_train,y_train in train_loader:\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(X_train)\n",
        "\n",
        "                loss = self.criterion(output.squeeze(), y_train)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_list.append(loss.item())\n",
        "                acc = accuracy(output,y_train)\n",
        "                acc_list.append(acc)\n",
        "\n",
        "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} stop count : {self.stop_count}\")\n",
        "\n",
        "\n",
        "    def test(self,test_loader):\n",
        "        if test_loader is None:\n",
        "            return 0,0\n",
        "        else:\n",
        "            #self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in test_loader:\n",
        "                    data, target = data, target\n",
        "                    output = self.model(data)\n",
        "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
        "\n",
        "                    correct += accuracy(output,target)*len(output)\n",
        "\n",
        "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "            return test_loss,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3d3GfMBAEv",
        "outputId": "12102841-2511-4734-c1a8-1100ee35e2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 8.9738, Accuracy: 151.99999964237213/300 (51%)\n",
            "Epoch 1 Loss 1.699935 acc : 0.471212 stop count : 0\n",
            "Test set: Average loss: 6.5566, Accuracy: 151.99999964237213/300 (51%)\n",
            "Epoch 2 Loss 1.266083 acc : 0.471307 stop count : 0\n",
            "Test set: Average loss: 4.8437, Accuracy: 153.0/300 (51%)\n",
            "Epoch 3 Loss 0.975874 acc : 0.484470 stop count : 0\n",
            "Test set: Average loss: 3.8183, Accuracy: 162.99999976158142/300 (54%)\n",
            "Epoch 4 Loss 0.817456 acc : 0.528693 stop count : 0\n",
            "Test set: Average loss: 3.3469, Accuracy: 191.00000023841858/300 (64%)\n",
            "Epoch 5 Loss 0.744641 acc : 0.567330 stop count : 0\n",
            "Test set: Average loss: 3.1501, Accuracy: 196.0000011920929/300 (65%)\n",
            "Epoch 6 Loss 0.701345 acc : 0.614489 stop count : 0\n",
            "Test set: Average loss: 3.0492, Accuracy: 200.0000011920929/300 (67%)\n",
            "Epoch 7 Loss 0.672642 acc : 0.636648 stop count : 0\n",
            "Test set: Average loss: 2.9675, Accuracy: 204.0000011920929/300 (68%)\n",
            "Epoch 8 Loss 0.648576 acc : 0.647538 stop count : 0\n",
            "Test set: Average loss: 2.8877, Accuracy: 209.00000023841858/300 (70%)\n",
            "Epoch 9 Loss 0.624899 acc : 0.669981 stop count : 0\n",
            "Test set: Average loss: 2.8078, Accuracy: 212.00000023841858/300 (71%)\n",
            "Epoch 10 Loss 0.606129 acc : 0.684375 stop count : 0\n",
            "Test set: Average loss: 2.7278, Accuracy: 216.00000095367432/300 (72%)\n",
            "Epoch 11 Loss 0.585699 acc : 0.694129 stop count : 0\n",
            "Test set: Average loss: 2.6463, Accuracy: 219.00000095367432/300 (73%)\n",
            "Epoch 12 Loss 0.567205 acc : 0.711742 stop count : 0\n",
            "Test set: Average loss: 2.5799, Accuracy: 226.0/300 (75%)\n",
            "Epoch 13 Loss 0.553624 acc : 0.725568 stop count : 0\n",
            "Test set: Average loss: 2.5243, Accuracy: 230.00000071525574/300 (77%)\n",
            "Epoch 14 Loss 0.539331 acc : 0.736269 stop count : 0\n",
            "Test set: Average loss: 2.4788, Accuracy: 232.99999976158142/300 (78%)\n",
            "Epoch 15 Loss 0.531577 acc : 0.741098 stop count : 0\n",
            "Test set: Average loss: 2.4321, Accuracy: 234.00000071525574/300 (78%)\n",
            "Epoch 16 Loss 0.520413 acc : 0.755871 stop count : 0\n",
            "Test set: Average loss: 2.3901, Accuracy: 234.00000071525574/300 (78%)\n",
            "Epoch 17 Loss 0.512385 acc : 0.759848 stop count : 0\n",
            "Test set: Average loss: 2.3744, Accuracy: 239.99999976158142/300 (80%)\n",
            "Epoch 18 Loss 0.503446 acc : 0.767235 stop count : 0\n",
            "Test set: Average loss: 2.3547, Accuracy: 239.99999976158142/300 (80%)\n",
            "Epoch 19 Loss 0.497194 acc : 0.766477 stop count : 0\n",
            "Test set: Average loss: 2.3409, Accuracy: 241.9999988079071/300 (81%)\n",
            "Epoch 20 Loss 0.490932 acc : 0.775379 stop count : 0\n",
            "Test set: Average loss: 2.3056, Accuracy: 241.9999988079071/300 (81%)\n",
            "Epoch 21 Loss 0.484145 acc : 0.779830 stop count : 0\n",
            "Test set: Average loss: 2.2960, Accuracy: 238.9999988079071/300 (80%)\n",
            "Epoch 22 Loss 0.476464 acc : 0.781534 stop count : 0\n",
            "Test set: Average loss: 2.2861, Accuracy: 241.9999988079071/300 (81%)\n",
            "Epoch 23 Loss 0.470556 acc : 0.780114 stop count : 0\n",
            "Test set: Average loss: 2.2846, Accuracy: 239.9999988079071/300 (80%)\n",
            "Epoch 24 Loss 0.464774 acc : 0.788826 stop count : 0\n",
            "Test set: Average loss: 2.2511, Accuracy: 245.00000047683716/300 (82%)\n",
            "Epoch 25 Loss 0.459625 acc : 0.789489 stop count : 0\n",
            "Test set: Average loss: 2.2348, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 26 Loss 0.454949 acc : 0.788731 stop count : 0\n",
            "Test set: Average loss: 2.2455, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 27 Loss 0.449841 acc : 0.790436 stop count : 1\n",
            "Test set: Average loss: 2.2246, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 28 Loss 0.444645 acc : 0.793087 stop count : 0\n",
            "Test set: Average loss: 2.2181, Accuracy: 243.00000047683716/300 (81%)\n",
            "Epoch 29 Loss 0.441317 acc : 0.797254 stop count : 0\n",
            "Test set: Average loss: 2.2116, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 30 Loss 0.435859 acc : 0.801610 stop count : 0\n",
            "Test set: Average loss: 2.2000, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 31 Loss 0.433186 acc : 0.799811 stop count : 0\n",
            "Test set: Average loss: 2.1931, Accuracy: 244.00000047683716/300 (81%)\n",
            "Epoch 32 Loss 0.429600 acc : 0.798580 stop count : 0\n",
            "Test set: Average loss: 2.1839, Accuracy: 244.99999952316284/300 (82%)\n",
            "Epoch 33 Loss 0.423394 acc : 0.804545 stop count : 0\n",
            "Test set: Average loss: 2.1799, Accuracy: 243.00000047683716/300 (81%)\n",
            "Epoch 34 Loss 0.419603 acc : 0.807576 stop count : 0\n",
            "Test set: Average loss: 2.1701, Accuracy: 245.00000047683716/300 (82%)\n",
            "Epoch 35 Loss 0.415775 acc : 0.814015 stop count : 0\n",
            "Test set: Average loss: 2.1628, Accuracy: 240.99999976158142/300 (80%)\n",
            "Epoch 36 Loss 0.411704 acc : 0.818371 stop count : 0\n",
            "Test set: Average loss: 2.1596, Accuracy: 245.00000047683716/300 (82%)\n",
            "Epoch 37 Loss 0.407093 acc : 0.817330 stop count : 0\n",
            "Test set: Average loss: 2.1298, Accuracy: 248.00000047683716/300 (83%)\n",
            "Epoch 38 Loss 0.402177 acc : 0.826042 stop count : 0\n",
            "Test set: Average loss: 2.1358, Accuracy: 246.00000047683716/300 (82%)\n",
            "Epoch 39 Loss 0.399964 acc : 0.822538 stop count : 1\n",
            "Test set: Average loss: 2.1398, Accuracy: 249.00000047683716/300 (83%)\n",
            "Epoch 40 Loss 0.394826 acc : 0.828504 stop count : 2\n",
            "Test set: Average loss: 2.1266, Accuracy: 251.00000047683716/300 (84%)\n",
            "Epoch 41 Loss 0.391974 acc : 0.832860 stop count : 0\n",
            "Test set: Average loss: 2.1208, Accuracy: 251.00000047683716/300 (84%)\n",
            "Epoch 42 Loss 0.388218 acc : 0.831439 stop count : 0\n",
            "Test set: Average loss: 2.1122, Accuracy: 251.00000047683716/300 (84%)\n",
            "Epoch 43 Loss 0.385214 acc : 0.834659 stop count : 0\n",
            "Test set: Average loss: 2.0947, Accuracy: 250.00000047683716/300 (83%)\n",
            "Epoch 44 Loss 0.381913 acc : 0.834186 stop count : 0\n",
            "Test set: Average loss: 2.0973, Accuracy: 252.00000047683716/300 (84%)\n",
            "Epoch 45 Loss 0.378886 acc : 0.834280 stop count : 1\n",
            "Test set: Average loss: 2.0844, Accuracy: 253.00000047683716/300 (84%)\n",
            "Epoch 46 Loss 0.376702 acc : 0.838731 stop count : 0\n",
            "Test set: Average loss: 2.0868, Accuracy: 253.00000047683716/300 (84%)\n",
            "Epoch 47 Loss 0.372332 acc : 0.844508 stop count : 1\n",
            "Test set: Average loss: 2.0751, Accuracy: 254.00000047683716/300 (85%)\n",
            "Epoch 48 Loss 0.369969 acc : 0.842614 stop count : 0\n",
            "Test set: Average loss: 2.0743, Accuracy: 251.00000047683716/300 (84%)\n",
            "Epoch 49 Loss 0.367505 acc : 0.838636 stop count : 0\n",
            "Test set: Average loss: 2.0747, Accuracy: 252.00000047683716/300 (84%)\n",
            "Epoch 50 Loss 0.364578 acc : 0.830208 stop count : 1\n",
            "\n",
            " Test start \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 4.0940, Accuracy: 153.0/300 (51%)\n",
            "Epoch 1 Loss 0.747277 acc : 0.501705 stop count : 0\n",
            "Test set: Average loss: 3.1509, Accuracy: 153.99999904632568/300 (51%)\n",
            "Epoch 2 Loss 0.576456 acc : 0.649811 stop count : 0\n",
            "Test set: Average loss: 2.4885, Accuracy: 262.99999952316284/300 (88%)\n",
            "Epoch 3 Loss 0.462462 acc : 0.900379 stop count : 0\n",
            "Test set: Average loss: 2.0692, Accuracy: 276.0000002384186/300 (92%)\n",
            "Epoch 4 Loss 0.394342 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.8030, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 5 Loss 0.351450 acc : 0.925663 stop count : 0\n",
            "Test set: Average loss: 1.6437, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 6 Loss 0.326719 acc : 0.924527 stop count : 0\n",
            "Test set: Average loss: 1.5513, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 7 Loss 0.311557 acc : 0.922917 stop count : 0\n",
            "Test set: Average loss: 1.4923, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 8 Loss 0.302089 acc : 0.924148 stop count : 0\n",
            "Test set: Average loss: 1.4475, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 9 Loss 0.294680 acc : 0.925758 stop count : 0\n",
            "Test set: Average loss: 1.4142, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 10 Loss 0.289143 acc : 0.927083 stop count : 0\n",
            "Test set: Average loss: 1.3881, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 11 Loss 0.285087 acc : 0.926989 stop count : 0\n",
            "Test set: Average loss: 1.3683, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 12 Loss 0.281154 acc : 0.927462 stop count : 0\n",
            "Test set: Average loss: 1.3550, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 13 Loss 0.278878 acc : 0.928977 stop count : 0\n",
            "Test set: Average loss: 1.3428, Accuracy: 279.0000002384186/300 (93%)\n",
            "Epoch 14 Loss 0.277581 acc : 0.925758 stop count : 0\n",
            "Test set: Average loss: 1.3335, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 15 Loss 0.275953 acc : 0.927083 stop count : 0\n",
            "Test set: Average loss: 1.3252, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 16 Loss 0.274158 acc : 0.927367 stop count : 0\n",
            "Test set: Average loss: 1.3165, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 17 Loss 0.327142 acc : 0.898438 stop count : 0"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 147\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Test start \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    146\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(model, optimizer, criterion)\n\u001b[1;32m--> 147\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m _,acc \u001b[38;5;241m=\u001b[39m train_process\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[0;32m    150\u001b[0m result_list_classical\u001b[38;5;241m.\u001b[39mappend(acc)\n",
            "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res)\u001b[0m\n\u001b[0;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X_train)\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39msqueeze(), y_train)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     49\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "result_list_classical = []\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "\"\"\"\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=PCA_dim)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.PCA_dim, random_state=seed)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
        "\n",
        "# Pennylane 장치 설정\n",
        "dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
        "\n",
        "\n",
        "def ZZFeatureMapLayer(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ansatz(params):\n",
        "    for j in range(len(params)):\n",
        "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
        "        for i in range(len(params[0])):\n",
        "            qml.RY(params[j, i, 0], wires=i)\n",
        "            qml.RZ(params[j, i, 1], wires=i)\n",
        "\n",
        "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
        "        if j == len(params)-1:\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(len(params[0])-1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "\n",
        "# 양자 레이어 정의\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def QuantumLayer(features,params):\n",
        "    ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
        "    ansatz(params)\n",
        "    return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
        "\n",
        "\n",
        "## 양자 커널\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal(features1,features2):\n",
        "    ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "\n",
        "class Feature_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Feature_model,self).__init__()\n",
        "        self.cls = nn.Sequential(OrderedDict([('cls1', nn.Linear(PCA_dim,PCA_dim*4)),('relu1', nn.ReLU()),('cls2', nn.Linear(PCA_dim*4,PCA_dim*4)),('relu2', nn.ReLU()),('cls3', nn.Linear(PCA_dim*4,PCA_dim*2-1)),('sigmoid', nn.Sigmoid())]))\n",
        "        self.Kernal = Kernal\n",
        "    def forward(self,inputs):\n",
        "        epsilon = 1e-6\n",
        "        input1 = inputs[0]\n",
        "        input2 = inputs[1]\n",
        "        input1 = self.cls(input1)*np.pi\n",
        "        input2 = self.cls(input2)*np.pi\n",
        "        output = self.Kernal(input1,input2)\n",
        "        output = output.type(torch.float32)\n",
        "        return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 하이브리드 모델 정의\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cls = feature_model.cls\n",
        "\n",
        "        self.quantum_layer = QuantumLayer\n",
        "        self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
        "    def forward(self, x):\n",
        "        x = self.cls(x)*np.pi\n",
        "        #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
        "        quantum_output = self.quantum_layer(x,self.Q_params)\n",
        "        quantum_output = quantum_output.type(torch.float32)\n",
        "        return torch.log(quantum_output)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cls_layer_1 = nn.Linear(PCA_dim,PCA_dim*PCA_dim)\n",
        "        self.cls_layer_2 = nn.Linear(PCA_dim*PCA_dim,PCA_dim*PCA_dim-1)\n",
        "        self.output_layer = nn.Linear(PCA_dim*PCA_dim-1,PCA_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.cls_layer_1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.cls_layer_2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "# 모델, 손실 함수, 최적화 설정\n",
        "\n",
        "\n",
        "feature_model = Feature_model(); criterion = nn.BCELoss()\n",
        "#model = Model(); criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(feature_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
        "train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15)\n",
        "model = HybridModel(); criterion = nn.NLLLoss()\n",
        "for param in model.cls.parameters():\n",
        "    param.requires_grad = False\n",
        "#model.load_state_dict(para_dict)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "print('\\n\\n Test start \\n\\n')\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=5)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "result_list_classical.append(acc)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
