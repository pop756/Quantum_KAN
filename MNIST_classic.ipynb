{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vJl4rYpKBAEk",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "PCA_dim = 8\n",
        "CLS_num = 2\n",
        "\n",
        "\n",
        "\n",
        "with open('./data.pkl','rb') as file:\n",
        "    data = pickle.load(file)\n",
        "X = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "def Fit_to_quantum(X,PCA_dim):\n",
        "    pca = PCA(n_components=PCA_dim)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "    \n",
        "\n",
        "\n",
        "# 정규화 (표준 스케일러 사용)\n",
        "#x_train_pca = Fit_to_quantum(x_train,PCA_dim)\n",
        "#x_test_pca = Fit_to_quantum(x_test,PCA_dim)\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "class Feature_data_loader(Dataset):\n",
        "    def __init__(self,x_train,y_train):\n",
        "        self.feature1 = x_train\n",
        "        temp = copy.deepcopy(x_train)\n",
        "        shuffle = torch.randperm(len(temp))\n",
        "        self.feature2 = temp[shuffle]\n",
        "        \n",
        "        self.y1 = y_train\n",
        "        temp_y = copy.deepcopy(y_train)\n",
        "        self.y2 = temp_y[shuffle]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.feature1)\n",
        "    def __getitem__(self,idx):\n",
        "        input1 = self.feature1[idx]\n",
        "        input2 = self.feature2[idx]\n",
        "        if self.y1[idx] == self.y2[idx]:\n",
        "            label = torch.tensor(1.).float()\n",
        "        else:\n",
        "            label = torch.tensor(0.).float()\n",
        "        return [input1,input2],label\n",
        "\n",
        "\n",
        "# DataLoader 생성\n",
        "\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(x_train_pca, y_train.float()), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_pca, y_test.float()), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N4Y4dnYJBAEt",
        "notebookRunGroups": {
          "groupValue": "2"
        }
      },
      "outputs": [],
      "source": [
        "def accuracy(pred, true):\n",
        "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
        "    pred = pred.detach().cpu()\n",
        "    true = true.cpu()\n",
        "    pred_labels = torch.round(pred)\n",
        "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
        "    correct = (pred_labels == true).sum()\n",
        "    # 정확도를 계산\n",
        "    acc = correct / true.size(0)\n",
        "    return acc.item()\n",
        "\n",
        "class Early_stop_train():\n",
        "    def __init__(self,model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "\n",
        "        self.loss_list = [1e100]\n",
        "        self.stop_count = 0\n",
        "\n",
        "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            if self.stop_count>=res:\n",
        "                break\n",
        "            loss_val,_ = self.test(test_loader)\n",
        "            self.loss_list.append(loss_val)\n",
        "\n",
        "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
        "                self.stop_count+=1\n",
        "            else:\n",
        "                self.stop_count = 0\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "            for X_train,y_train in train_loader:\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(X_train)\n",
        "\n",
        "                loss = self.criterion(output.squeeze(), y_train)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_list.append(loss.item())\n",
        "                acc = accuracy(output,y_train)\n",
        "                acc_list.append(acc)\n",
        "\n",
        "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} stop count : {self.stop_count}\")\n",
        "\n",
        "\n",
        "    def test(self,test_loader):\n",
        "        if test_loader is None:\n",
        "            return 0,0\n",
        "        else:\n",
        "            #self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in test_loader:\n",
        "                    data, target = data, target\n",
        "                    output = self.model(data)\n",
        "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
        "\n",
        "                    correct += accuracy(output,target)*len(output)\n",
        "\n",
        "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "            return test_loss,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3d3GfMBAEv",
        "outputId": "12102841-2511-4734-c1a8-1100ee35e2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 3.3947, Accuracy: 193.99999928474426/300 (65%)\n",
            "Epoch 1 Loss 0.676444 acc : 0.661364 stop count : 0\n",
            "Test set: Average loss: 3.3731, Accuracy: 202.00000095367432/300 (67%)\n",
            "Epoch 2 Loss 0.672576 acc : 0.709848 stop count : 0\n",
            "Test set: Average loss: 3.3519, Accuracy: 216.0/300 (72%)\n",
            "Epoch 3 Loss 0.668341 acc : 0.741951 stop count : 0\n",
            "Test set: Average loss: 3.3315, Accuracy: 220.99999904632568/300 (74%)\n",
            "Epoch 4 Loss 0.664671 acc : 0.764299 stop count : 0\n",
            "Test set: Average loss: 3.3110, Accuracy: 230.00000071525574/300 (77%)\n",
            "Epoch 5 Loss 0.660694 acc : 0.780114 stop count : 0\n",
            "Test set: Average loss: 3.2911, Accuracy: 238.00000071525574/300 (79%)\n",
            "Epoch 6 Loss 0.656719 acc : 0.791193 stop count : 0\n",
            "Test set: Average loss: 3.2708, Accuracy: 237.00000071525574/300 (79%)\n",
            "Epoch 7 Loss 0.652675 acc : 0.794792 stop count : 0\n",
            "Test set: Average loss: 3.2497, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 8 Loss 0.648715 acc : 0.798390 stop count : 0\n",
            "Test set: Average loss: 3.2285, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 9 Loss 0.644613 acc : 0.802652 stop count : 0\n",
            "Test set: Average loss: 3.2072, Accuracy: 242.00000071525574/300 (81%)\n",
            "Epoch 10 Loss 0.640402 acc : 0.804072 stop count : 0\n",
            "Test set: Average loss: 3.1849, Accuracy: 241.00000071525574/300 (80%)\n",
            "Epoch 11 Loss 0.636137 acc : 0.802652 stop count : 0\n",
            "Test set: Average loss: 3.1624, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 12 Loss 0.631495 acc : 0.807386 stop count : 0\n",
            "Test set: Average loss: 3.1399, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 13 Loss 0.627235 acc : 0.805871 stop count : 0\n",
            "Test set: Average loss: 3.1162, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 14 Loss 0.622812 acc : 0.805492 stop count : 0\n",
            "Test set: Average loss: 3.0918, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 15 Loss 0.617788 acc : 0.808617 stop count : 0\n",
            "Test set: Average loss: 3.0679, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 16 Loss 0.613188 acc : 0.807008 stop count : 0\n",
            "Test set: Average loss: 3.0433, Accuracy: 240.00000071525574/300 (80%)\n",
            "Epoch 17 Loss 0.608437 acc : 0.807292 stop count : 0\n",
            "Test set: Average loss: 3.0183, Accuracy: 238.99999904632568/300 (80%)\n",
            "Epoch 18 Loss 0.603519 acc : 0.808239 stop count : 0\n",
            "Test set: Average loss: 2.9935, Accuracy: 238.99999904632568/300 (80%)\n",
            "Epoch 19 Loss 0.598827 acc : 0.809659 stop count : 0\n",
            "Test set: Average loss: 2.9681, Accuracy: 238.99999904632568/300 (80%)\n",
            "Epoch 20 Loss 0.593594 acc : 0.816193 stop count : 0\n",
            "Test set: Average loss: 2.9428, Accuracy: 240.99999904632568/300 (80%)\n",
            "Epoch 21 Loss 0.588908 acc : 0.812973 stop count : 0\n",
            "Test set: Average loss: 2.9174, Accuracy: 241.99999904632568/300 (81%)\n",
            "Epoch 22 Loss 0.584078 acc : 0.811269 stop count : 0\n",
            "Test set: Average loss: 2.8918, Accuracy: 244.00000071525574/300 (81%)\n",
            "Epoch 23 Loss 0.579286 acc : 0.811174 stop count : 0\n",
            "Test set: Average loss: 2.8660, Accuracy: 245.99999976158142/300 (82%)\n",
            "Epoch 24 Loss 0.574197 acc : 0.814015 stop count : 0\n",
            "Test set: Average loss: 2.8401, Accuracy: 245.99999976158142/300 (82%)\n",
            "Epoch 25 Loss 0.568883 acc : 0.814205 stop count : 0\n",
            "Test set: Average loss: 2.8151, Accuracy: 247.99999976158142/300 (83%)\n",
            "Epoch 26 Loss 0.563972 acc : 0.815625 stop count : 0\n",
            "Test set: Average loss: 2.7894, Accuracy: 247.99999976158142/300 (83%)\n",
            "Epoch 27 Loss 0.558629 acc : 0.814583 stop count : 0\n",
            "Test set: Average loss: 2.7632, Accuracy: 249.99999976158142/300 (83%)\n",
            "Epoch 28 Loss 0.553905 acc : 0.816856 stop count : 0\n",
            "Test set: Average loss: 2.7366, Accuracy: 250.9999988079071/300 (84%)\n",
            "Epoch 29 Loss 0.548693 acc : 0.819886 stop count : 0\n",
            "Test set: Average loss: 2.7105, Accuracy: 250.9999988079071/300 (84%)\n",
            "Epoch 30 Loss 0.543373 acc : 0.819886 stop count : 0\n",
            "Test set: Average loss: 2.6840, Accuracy: 250.9999988079071/300 (84%)\n",
            "Epoch 31 Loss 0.538108 acc : 0.819129 stop count : 0\n",
            "Test set: Average loss: 2.6577, Accuracy: 250.9999988079071/300 (84%)\n",
            "Epoch 32 Loss 0.533190 acc : 0.820928 stop count : 0\n",
            "Test set: Average loss: 2.6308, Accuracy: 250.9999988079071/300 (84%)\n",
            "Epoch 33 Loss 0.527844 acc : 0.824053 stop count : 0\n",
            "Test set: Average loss: 2.6043, Accuracy: 249.9999988079071/300 (83%)\n",
            "Epoch 34 Loss 0.522506 acc : 0.827462 stop count : 0\n",
            "Test set: Average loss: 2.5773, Accuracy: 251.9999988079071/300 (84%)\n",
            "Epoch 35 Loss 0.516925 acc : 0.826231 stop count : 0\n",
            "Test set: Average loss: 2.5512, Accuracy: 251.9999988079071/300 (84%)\n",
            "Epoch 36 Loss 0.511995 acc : 0.825947 stop count : 0\n",
            "Test set: Average loss: 2.5243, Accuracy: 254.9999988079071/300 (85%)\n",
            "Epoch 37 Loss 0.506563 acc : 0.830208 stop count : 0\n",
            "Test set: Average loss: 2.4973, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 38 Loss 0.501327 acc : 0.832955 stop count : 0\n",
            "Test set: Average loss: 2.4707, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 39 Loss 0.496087 acc : 0.835890 stop count : 0\n",
            "Test set: Average loss: 2.4439, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 40 Loss 0.491274 acc : 0.836648 stop count : 0\n",
            "Test set: Average loss: 2.4168, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 41 Loss 0.485691 acc : 0.838826 stop count : 0\n",
            "Test set: Average loss: 2.3905, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 42 Loss 0.480437 acc : 0.840057 stop count : 0\n",
            "Test set: Average loss: 2.3641, Accuracy: 255.9999988079071/300 (85%)\n",
            "Epoch 43 Loss 0.475232 acc : 0.842614 stop count : 0\n",
            "Test set: Average loss: 2.3376, Accuracy: 258.00000047683716/300 (86%)\n",
            "Epoch 44 Loss 0.469904 acc : 0.844508 stop count : 0\n",
            "Test set: Average loss: 2.3114, Accuracy: 259.00000047683716/300 (86%)\n",
            "Epoch 45 Loss 0.464876 acc : 0.844697 stop count : 0\n",
            "Test set: Average loss: 2.2860, Accuracy: 259.00000047683716/300 (86%)\n",
            "Epoch 46 Loss 0.459510 acc : 0.845833 stop count : 0\n",
            "Test set: Average loss: 2.2596, Accuracy: 260.00000047683716/300 (87%)\n",
            "Epoch 47 Loss 0.454529 acc : 0.851326 stop count : 0\n",
            "Test set: Average loss: 2.2344, Accuracy: 260.00000047683716/300 (87%)\n",
            "Epoch 48 Loss 0.449672 acc : 0.851515 stop count : 0\n",
            "Test set: Average loss: 2.2091, Accuracy: 261.00000047683716/300 (87%)\n",
            "Epoch 49 Loss 0.444730 acc : 0.855682 stop count : 0\n",
            "Test set: Average loss: 2.1836, Accuracy: 261.99999952316284/300 (87%)\n",
            "Epoch 50 Loss 0.439680 acc : 0.858712 stop count : 0\n",
            "Test set: Average loss: 2.1582, Accuracy: 262.99999952316284/300 (88%)\n",
            "Epoch 51 Loss 0.434677 acc : 0.862973 stop count : 0\n",
            "Test set: Average loss: 2.1327, Accuracy: 261.99999952316284/300 (87%)\n",
            "Epoch 52 Loss 0.429967 acc : 0.862500 stop count : 0\n",
            "Test set: Average loss: 2.1079, Accuracy: 262.99999952316284/300 (88%)\n",
            "Epoch 53 Loss 0.425091 acc : 0.865720 stop count : 0\n",
            "Test set: Average loss: 2.0836, Accuracy: 262.99999952316284/300 (88%)\n",
            "Epoch 54 Loss 0.420186 acc : 0.867235 stop count : 0\n",
            "Test set: Average loss: 2.0594, Accuracy: 265.0000011920929/300 (88%)\n",
            "Epoch 55 Loss 0.415301 acc : 0.867330 stop count : 0\n",
            "Test set: Average loss: 2.0361, Accuracy: 265.0000011920929/300 (88%)\n",
            "Epoch 56 Loss 0.411623 acc : 0.869318 stop count : 0\n",
            "Test set: Average loss: 2.0131, Accuracy: 265.0000011920929/300 (88%)\n",
            "Epoch 57 Loss 0.406658 acc : 0.871307 stop count : 0\n",
            "Test set: Average loss: 1.9896, Accuracy: 266.0000011920929/300 (89%)\n",
            "Epoch 58 Loss 0.401984 acc : 0.875568 stop count : 0\n",
            "Test set: Average loss: 1.9669, Accuracy: 266.0000011920929/300 (89%)\n",
            "Epoch 59 Loss 0.397260 acc : 0.876042 stop count : 0\n",
            "Test set: Average loss: 1.9440, Accuracy: 266.0000011920929/300 (89%)\n",
            "Epoch 60 Loss 0.392974 acc : 0.877367 stop count : 0\n",
            "Test set: Average loss: 1.9220, Accuracy: 266.0000011920929/300 (89%)\n",
            "Epoch 61 Loss 0.388746 acc : 0.877367 stop count : 0\n",
            "Test set: Average loss: 1.9011, Accuracy: 266.0000011920929/300 (89%)\n",
            "Epoch 62 Loss 0.384221 acc : 0.877462 stop count : 0\n",
            "Test set: Average loss: 1.8797, Accuracy: 267.0000002384186/300 (89%)\n",
            "Epoch 63 Loss 0.380623 acc : 0.877083 stop count : 0\n",
            "Test set: Average loss: 1.8586, Accuracy: 267.0000002384186/300 (89%)\n",
            "Epoch 64 Loss 0.376201 acc : 0.878409 stop count : 0\n",
            "Test set: Average loss: 1.8385, Accuracy: 268.0000002384186/300 (89%)\n",
            "Epoch 65 Loss 0.372214 acc : 0.882860 stop count : 0\n",
            "Test set: Average loss: 1.8185, Accuracy: 268.0000002384186/300 (89%)\n",
            "Epoch 66 Loss 0.368532 acc : 0.885417 stop count : 0\n",
            "Test set: Average loss: 1.7981, Accuracy: 268.0000002384186/300 (89%)\n",
            "Epoch 67 Loss 0.364393 acc : 0.885701 stop count : 0\n",
            "Test set: Average loss: 1.7789, Accuracy: 268.0000002384186/300 (89%)\n",
            "Epoch 68 Loss 0.360757 acc : 0.887027 stop count : 0\n",
            "Test set: Average loss: 1.7599, Accuracy: 270.0000002384186/300 (90%)\n",
            "Epoch 69 Loss 0.356741 acc : 0.890341 stop count : 0\n",
            "Test set: Average loss: 1.7412, Accuracy: 271.0000002384186/300 (90%)\n",
            "Epoch 70 Loss 0.353367 acc : 0.891383 stop count : 0\n",
            "Test set: Average loss: 1.7228, Accuracy: 273.0000002384186/300 (91%)\n",
            "Epoch 71 Loss 0.350054 acc : 0.891193 stop count : 0\n",
            "Test set: Average loss: 1.7047, Accuracy: 273.0000002384186/300 (91%)\n",
            "Epoch 72 Loss 0.346076 acc : 0.891477 stop count : 0\n",
            "Test set: Average loss: 1.6878, Accuracy: 273.0000002384186/300 (91%)\n",
            "Epoch 73 Loss 0.343036 acc : 0.891477 stop count : 0\n",
            "Test set: Average loss: 1.6712, Accuracy: 274.0000002384186/300 (91%)\n",
            "Epoch 74 Loss 0.339596 acc : 0.891572 stop count : 0\n",
            "Test set: Average loss: 1.6541, Accuracy: 274.0000002384186/300 (91%)\n",
            "Epoch 75 Loss 0.336072 acc : 0.892898 stop count : 0\n",
            "Test set: Average loss: 1.6379, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 76 Loss 0.333350 acc : 0.894129 stop count : 0\n",
            "Test set: Average loss: 1.6223, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 77 Loss 0.329971 acc : 0.896023 stop count : 0\n",
            "Test set: Average loss: 1.6066, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 78 Loss 0.327487 acc : 0.895360 stop count : 0\n",
            "Test set: Average loss: 1.5922, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 79 Loss 0.324279 acc : 0.895455 stop count : 0\n",
            "Test set: Average loss: 1.5777, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 80 Loss 0.321555 acc : 0.896970 stop count : 0\n",
            "Test set: Average loss: 1.5629, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 81 Loss 0.318214 acc : 0.901610 stop count : 0\n",
            "Test set: Average loss: 1.5496, Accuracy: 274.0000002384186/300 (91%)\n",
            "Epoch 82 Loss 0.315910 acc : 0.901231 stop count : 0\n",
            "Test set: Average loss: 1.5363, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 83 Loss 0.313350 acc : 0.902746 stop count : 0\n",
            "Test set: Average loss: 1.5227, Accuracy: 274.0000002384186/300 (91%)\n",
            "Epoch 84 Loss 0.310608 acc : 0.902746 stop count : 0\n",
            "Test set: Average loss: 1.5099, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 85 Loss 0.308074 acc : 0.902746 stop count : 0\n",
            "Test set: Average loss: 1.4975, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 86 Loss 0.305490 acc : 0.904356 stop count : 0\n",
            "Test set: Average loss: 1.4852, Accuracy: 274.0000002384186/300 (91%)\n",
            "Epoch 87 Loss 0.303933 acc : 0.903693 stop count : 0\n",
            "Test set: Average loss: 1.4731, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 88 Loss 0.300807 acc : 0.904451 stop count : 0\n",
            "Test set: Average loss: 1.4626, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 89 Loss 0.298724 acc : 0.905777 stop count : 0\n",
            "Test set: Average loss: 1.4514, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 90 Loss 0.296286 acc : 0.905777 stop count : 0\n",
            "Test set: Average loss: 1.4408, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 91 Loss 0.294265 acc : 0.905682 stop count : 0\n",
            "Test set: Average loss: 1.4301, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 92 Loss 0.292528 acc : 0.905398 stop count : 0\n",
            "Test set: Average loss: 1.4202, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 93 Loss 0.289909 acc : 0.905871 stop count : 0\n",
            "Test set: Average loss: 1.4102, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 94 Loss 0.288132 acc : 0.907292 stop count : 0\n",
            "Test set: Average loss: 1.4009, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 95 Loss 0.286506 acc : 0.907008 stop count : 0\n",
            "Test set: Average loss: 1.3915, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 96 Loss 0.284415 acc : 0.905871 stop count : 0\n",
            "Test set: Average loss: 1.3822, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 97 Loss 0.282774 acc : 0.905871 stop count : 0\n",
            "Test set: Average loss: 1.3734, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 98 Loss 0.281243 acc : 0.905777 stop count : 0\n",
            "Test set: Average loss: 1.3647, Accuracy: 275.0000002384186/300 (92%)\n",
            "Epoch 99 Loss 0.279262 acc : 0.907197 stop count : 0\n",
            "Test set: Average loss: 1.3570, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 100 Loss 0.277559 acc : 0.907102 stop count : 0\n",
            "Test set: Average loss: 1.3490, Accuracy: 276.0000002384186/300 (92%)\n",
            "Epoch 101 Loss 0.275747 acc : 0.907292 stop count : 0\n",
            "Test set: Average loss: 1.3419, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 102 Loss 0.274213 acc : 0.907576 stop count : 0\n",
            "Test set: Average loss: 1.3343, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 103 Loss 0.272915 acc : 0.907197 stop count : 0\n",
            "Test set: Average loss: 1.3274, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 104 Loss 0.271273 acc : 0.910038 stop count : 0\n",
            "Test set: Average loss: 1.3198, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 105 Loss 0.270883 acc : 0.909754 stop count : 0\n",
            "Test set: Average loss: 1.3130, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 106 Loss 0.268372 acc : 0.910133 stop count : 0\n",
            "Test set: Average loss: 1.3070, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 107 Loss 0.267058 acc : 0.911648 stop count : 0\n",
            "Test set: Average loss: 1.3007, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 108 Loss 0.265813 acc : 0.911458 stop count : 0\n",
            "Test set: Average loss: 1.2944, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 109 Loss 0.264259 acc : 0.911553 stop count : 0\n",
            "Test set: Average loss: 1.2881, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 110 Loss 0.263219 acc : 0.911458 stop count : 0\n",
            "Test set: Average loss: 1.2824, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 111 Loss 0.262365 acc : 0.911364 stop count : 0\n",
            "Test set: Average loss: 1.2766, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 112 Loss 0.260766 acc : 0.911458 stop count : 0\n",
            "Test set: Average loss: 1.2713, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 113 Loss 0.259939 acc : 0.912689 stop count : 0\n",
            "Test set: Average loss: 1.2662, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 114 Loss 0.258723 acc : 0.912595 stop count : 0\n",
            "Test set: Average loss: 1.2610, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 115 Loss 0.258148 acc : 0.912500 stop count : 0\n",
            "Test set: Average loss: 1.2563, Accuracy: 274.99999928474426/300 (92%)\n",
            "Epoch 116 Loss 0.256387 acc : 0.912973 stop count : 0\n",
            "Test set: Average loss: 1.2522, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 117 Loss 0.255542 acc : 0.912689 stop count : 0\n",
            "Test set: Average loss: 1.2473, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 118 Loss 0.254130 acc : 0.913068 stop count : 0\n",
            "Test set: Average loss: 1.2432, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 119 Loss 0.253601 acc : 0.912784 stop count : 0\n",
            "Test set: Average loss: 1.2380, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 120 Loss 0.252406 acc : 0.912784 stop count : 0\n",
            "Test set: Average loss: 1.2343, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 121 Loss 0.252304 acc : 0.912216 stop count : 0\n",
            "Test set: Average loss: 1.2308, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 122 Loss 0.250842 acc : 0.912595 stop count : 0\n",
            "Test set: Average loss: 1.2267, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 123 Loss 0.249584 acc : 0.913068 stop count : 0\n",
            "Test set: Average loss: 1.2229, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 124 Loss 0.248856 acc : 0.915720 stop count : 0\n",
            "Test set: Average loss: 1.2191, Accuracy: 275.99999928474426/300 (92%)\n",
            "Epoch 125 Loss 0.248169 acc : 0.915436 stop count : 0\n",
            "Test set: Average loss: 1.2160, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 126 Loss 0.247063 acc : 0.915909 stop count : 0\n",
            "Test set: Average loss: 1.2127, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 127 Loss 0.246777 acc : 0.915436 stop count : 0\n",
            "Test set: Average loss: 1.2100, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 128 Loss 0.245742 acc : 0.915530 stop count : 0\n",
            "Test set: Average loss: 1.2063, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 129 Loss 0.244881 acc : 0.915625 stop count : 0\n",
            "Test set: Average loss: 1.2032, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 130 Loss 0.244095 acc : 0.915720 stop count : 0\n",
            "Test set: Average loss: 1.2003, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 131 Loss 0.243027 acc : 0.915909 stop count : 0\n",
            "Test set: Average loss: 1.1973, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 132 Loss 0.242008 acc : 0.916098 stop count : 0\n",
            "Test set: Average loss: 1.1947, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 133 Loss 0.241403 acc : 0.915814 stop count : 0\n",
            "Test set: Average loss: 1.1919, Accuracy: 276.99999928474426/300 (92%)\n",
            "Epoch 134 Loss 0.241202 acc : 0.915720 stop count : 0\n",
            "Test set: Average loss: 1.1891, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 135 Loss 0.240630 acc : 0.915720 stop count : 0\n",
            "Test set: Average loss: 1.1870, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 136 Loss 0.239712 acc : 0.915909 stop count : 0\n",
            "Test set: Average loss: 1.1848, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 137 Loss 0.239397 acc : 0.915530 stop count : 0\n",
            "Test set: Average loss: 1.1821, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 138 Loss 0.238714 acc : 0.917045 stop count : 0\n",
            "Test set: Average loss: 1.1795, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 139 Loss 0.238183 acc : 0.915625 stop count : 0\n",
            "Test set: Average loss: 1.1774, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 140 Loss 0.237675 acc : 0.916951 stop count : 0\n",
            "Test set: Average loss: 1.1750, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 141 Loss 0.236675 acc : 0.917140 stop count : 0\n",
            "Test set: Average loss: 1.1733, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 142 Loss 0.236727 acc : 0.916761 stop count : 0\n",
            "Test set: Average loss: 1.1715, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 143 Loss 0.235429 acc : 0.917330 stop count : 0\n",
            "Test set: Average loss: 1.1691, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 144 Loss 0.234589 acc : 0.917519 stop count : 0\n",
            "Test set: Average loss: 1.1676, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 145 Loss 0.234749 acc : 0.916951 stop count : 0\n",
            "Test set: Average loss: 1.1648, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 146 Loss 0.233992 acc : 0.917330 stop count : 0\n",
            "Test set: Average loss: 1.1630, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 147 Loss 0.233893 acc : 0.918466 stop count : 0\n",
            "Test set: Average loss: 1.1616, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 148 Loss 0.232965 acc : 0.919981 stop count : 0\n",
            "Test set: Average loss: 1.1599, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 149 Loss 0.232207 acc : 0.918466 stop count : 0\n",
            "Test set: Average loss: 1.1585, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 150 Loss 0.232466 acc : 0.918371 stop count : 0\n",
            "Test set: Average loss: 1.1567, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 151 Loss 0.231370 acc : 0.918845 stop count : 0\n",
            "Test set: Average loss: 1.1545, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 152 Loss 0.230701 acc : 0.919981 stop count : 0\n",
            "Test set: Average loss: 1.1528, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 153 Loss 0.230067 acc : 0.920170 stop count : 0\n",
            "Test set: Average loss: 1.1515, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 154 Loss 0.229893 acc : 0.920170 stop count : 0\n",
            "Test set: Average loss: 1.1499, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 155 Loss 0.230065 acc : 0.921023 stop count : 0\n",
            "Test set: Average loss: 1.1482, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 156 Loss 0.228706 acc : 0.920170 stop count : 0\n",
            "Test set: Average loss: 1.1472, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 157 Loss 0.228487 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1458, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 158 Loss 0.228354 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1446, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 159 Loss 0.227653 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1436, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 160 Loss 0.227832 acc : 0.921307 stop count : 0\n",
            "Test set: Average loss: 1.1422, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 161 Loss 0.227053 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1410, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 162 Loss 0.227028 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1401, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 163 Loss 0.226159 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1385, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 164 Loss 0.225786 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1373, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 165 Loss 0.225389 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1367, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 166 Loss 0.224806 acc : 0.921686 stop count : 0\n",
            "Test set: Average loss: 1.1350, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 167 Loss 0.224483 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1343, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 168 Loss 0.224145 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1336, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 169 Loss 0.223480 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1324, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 170 Loss 0.223549 acc : 0.921307 stop count : 0\n",
            "Test set: Average loss: 1.1315, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 171 Loss 0.223682 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1303, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 172 Loss 0.222952 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1298, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 173 Loss 0.223474 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1295, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 174 Loss 0.222447 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1284, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 175 Loss 0.222095 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1276, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 176 Loss 0.221642 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1270, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 177 Loss 0.221436 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1267, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 178 Loss 0.220676 acc : 0.921686 stop count : 0\n",
            "Test set: Average loss: 1.1260, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 179 Loss 0.220911 acc : 0.921496 stop count : 0\n",
            "Test set: Average loss: 1.1247, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 180 Loss 0.220662 acc : 0.921117 stop count : 0\n",
            "Test set: Average loss: 1.1237, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 181 Loss 0.220046 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1239, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 182 Loss 0.219890 acc : 0.923011 stop count : 1\n",
            "Test set: Average loss: 1.1231, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 183 Loss 0.219966 acc : 0.922822 stop count : 0\n",
            "Test set: Average loss: 1.1227, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 184 Loss 0.219396 acc : 0.922917 stop count : 0\n",
            "Test set: Average loss: 1.1220, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 185 Loss 0.219267 acc : 0.921402 stop count : 0\n",
            "Test set: Average loss: 1.1209, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 186 Loss 0.219221 acc : 0.921117 stop count : 0\n",
            "Test set: Average loss: 1.1209, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 187 Loss 0.218256 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1205, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 188 Loss 0.218551 acc : 0.921686 stop count : 0\n",
            "Test set: Average loss: 1.1200, Accuracy: 278.99999928474426/300 (93%)\n",
            "Epoch 189 Loss 0.218084 acc : 0.921307 stop count : 0\n",
            "Test set: Average loss: 1.1204, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 190 Loss 0.218347 acc : 0.921117 stop count : 1\n",
            "Test set: Average loss: 1.1191, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 191 Loss 0.217648 acc : 0.921307 stop count : 0\n",
            "Test set: Average loss: 1.1190, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 192 Loss 0.217921 acc : 0.921023 stop count : 0\n",
            "Test set: Average loss: 1.1188, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 193 Loss 0.216801 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 1.1181, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 194 Loss 0.217058 acc : 0.924432 stop count : 0\n",
            "Test set: Average loss: 1.1185, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 195 Loss 0.216264 acc : 0.924621 stop count : 1\n",
            "Test set: Average loss: 1.1180, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 196 Loss 0.216088 acc : 0.922917 stop count : 0\n",
            "Test set: Average loss: 1.1174, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 197 Loss 0.216360 acc : 0.924432 stop count : 0\n",
            "Test set: Average loss: 1.1170, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 198 Loss 0.216791 acc : 0.924053 stop count : 0\n",
            "Test set: Average loss: 1.1166, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 199 Loss 0.216128 acc : 0.922822 stop count : 0\n",
            "Test set: Average loss: 1.1164, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 200 Loss 0.215730 acc : 0.926989 stop count : 0\n",
            "Test set: Average loss: 1.1165, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 201 Loss 0.216064 acc : 0.923958 stop count : 1\n",
            "Test set: Average loss: 1.1156, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 202 Loss 0.214891 acc : 0.924527 stop count : 0\n",
            "Test set: Average loss: 1.1156, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 203 Loss 0.215507 acc : 0.925189 stop count : 0\n",
            "Test set: Average loss: 1.1150, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 204 Loss 0.215646 acc : 0.925473 stop count : 0\n",
            "Test set: Average loss: 1.1151, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 205 Loss 0.214547 acc : 0.925758 stop count : 1\n",
            "Test set: Average loss: 1.1151, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 206 Loss 0.214428 acc : 0.925568 stop count : 2\n",
            "Test set: Average loss: 1.1147, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 207 Loss 0.213984 acc : 0.925758 stop count : 0\n",
            "Test set: Average loss: 1.1146, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 208 Loss 0.214441 acc : 0.926894 stop count : 0\n",
            "Test set: Average loss: 1.1143, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 209 Loss 0.213998 acc : 0.926989 stop count : 0\n",
            "Test set: Average loss: 1.1142, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 210 Loss 0.213507 acc : 0.925947 stop count : 0\n",
            "Test set: Average loss: 1.1135, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 211 Loss 0.213390 acc : 0.924242 stop count : 0\n",
            "Test set: Average loss: 1.1130, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 212 Loss 0.213543 acc : 0.925568 stop count : 0\n",
            "Test set: Average loss: 1.1132, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 213 Loss 0.213560 acc : 0.925663 stop count : 1\n",
            "Test set: Average loss: 1.1135, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 214 Loss 0.213554 acc : 0.924148 stop count : 2\n",
            "Test set: Average loss: 1.1135, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 215 Loss 0.212364 acc : 0.924432 stop count : 3\n",
            "Test set: Average loss: 1.1125, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 216 Loss 0.213325 acc : 0.925568 stop count : 0\n",
            "Test set: Average loss: 1.1125, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 217 Loss 0.212135 acc : 0.925947 stop count : 1\n",
            "Test set: Average loss: 1.1130, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 218 Loss 0.211966 acc : 0.925758 stop count : 2\n",
            "Test set: Average loss: 1.1122, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 219 Loss 0.211849 acc : 0.925947 stop count : 0\n",
            "Test set: Average loss: 1.1117, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 220 Loss 0.212068 acc : 0.925758 stop count : 0\n",
            "Test set: Average loss: 1.1114, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 221 Loss 0.211603 acc : 0.925758 stop count : 0\n",
            "Test set: Average loss: 1.1117, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 222 Loss 0.211814 acc : 0.925568 stop count : 1\n",
            "Test set: Average loss: 1.1120, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 223 Loss 0.211020 acc : 0.927273 stop count : 2\n",
            "Test set: Average loss: 1.1122, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 224 Loss 0.211198 acc : 0.925568 stop count : 3\n",
            "Test set: Average loss: 1.1113, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 225 Loss 0.210400 acc : 0.926136 stop count : 0\n",
            "Test set: Average loss: 1.1112, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 226 Loss 0.211678 acc : 0.925568 stop count : 0\n",
            "Test set: Average loss: 1.1108, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 227 Loss 0.210722 acc : 0.925663 stop count : 0\n",
            "Test set: Average loss: 1.1115, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 228 Loss 0.210597 acc : 0.925758 stop count : 1\n",
            "Test set: Average loss: 1.1116, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 229 Loss 0.211505 acc : 0.925473 stop count : 2\n",
            "Test set: Average loss: 1.1113, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 230 Loss 0.210297 acc : 0.927462 stop count : 3\n",
            "Test set: Average loss: 1.1119, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 231 Loss 0.210471 acc : 0.926989 stop count : 4\n",
            "Test set: Average loss: 1.1112, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 232 Loss 0.209670 acc : 0.928788 stop count : 5\n",
            "Test set: Average loss: 1.1114, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 233 Loss 0.210170 acc : 0.928598 stop count : 6\n",
            "Test set: Average loss: 1.1108, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 234 Loss 0.209690 acc : 0.928409 stop count : 0\n",
            "Test set: Average loss: 1.1100, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 235 Loss 0.209662 acc : 0.928504 stop count : 0\n",
            "Test set: Average loss: 1.1106, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 236 Loss 0.209846 acc : 0.928409 stop count : 1\n",
            "Test set: Average loss: 1.1105, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 237 Loss 0.210200 acc : 0.928125 stop count : 2\n",
            "Test set: Average loss: 1.1102, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 238 Loss 0.209011 acc : 0.928693 stop count : 3\n",
            "Test set: Average loss: 1.1111, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 239 Loss 0.208903 acc : 0.928693 stop count : 4\n",
            "Test set: Average loss: 1.1106, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 240 Loss 0.208768 acc : 0.928693 stop count : 5\n",
            "Test set: Average loss: 1.1106, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 241 Loss 0.208700 acc : 0.928693 stop count : 6\n",
            "Test set: Average loss: 1.1105, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 242 Loss 0.208573 acc : 0.928788 stop count : 7\n",
            "Test set: Average loss: 1.1103, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 243 Loss 0.208404 acc : 0.928693 stop count : 8\n",
            "Test set: Average loss: 1.1107, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 244 Loss 0.208705 acc : 0.928504 stop count : 9\n",
            "Test set: Average loss: 1.1111, Accuracy: 280.99999928474426/300 (94%)\n",
            "Epoch 245 Loss 0.208336 acc : 0.928314 stop count : 10\n",
            "Test set: Average loss: 1.1112, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 246 Loss 0.208706 acc : 0.928598 stop count : 11\n",
            "Test set: Average loss: 1.1107, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 247 Loss 0.208172 acc : 0.928598 stop count : 12\n",
            "Test set: Average loss: 1.1115, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 248 Loss 0.207703 acc : 0.928598 stop count : 13\n",
            "Test set: Average loss: 1.1113, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 249 Loss 0.207990 acc : 0.928504 stop count : 14\n",
            "Test set: Average loss: 1.1105, Accuracy: 279.99999928474426/300 (93%)\n",
            "Epoch 250 Loss 0.207524 acc : 0.928598 stop count : 15\n",
            "Test set: Average loss: 1.1108, Accuracy: 279.99999928474426/300 (93%)\n",
            "Test Accuracy: 280.00\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "from kan import KAN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cls_layer_1 = nn.Linear(PCA_dim,PCA_dim*PCA_dim)\n",
        "        self.cls_layer_2 = nn.Linear(PCA_dim*PCA_dim,PCA_dim*2-1)\n",
        "        self.output_layer = nn.Linear(2*PCA_dim-1,1)\n",
        "    def forward(self, x):\n",
        "        x = self.cls_layer_1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.cls_layer_2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        output = self.output_layer(x)\n",
        "        output = nn.Sigmoid()(output)\n",
        "        output = torch.squeeze(output)\n",
        "        return output\n",
        "# 모델, 손실 함수, 최적화 설정\n",
        "\n",
        "\n",
        "\n",
        "model = Model(); criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=5000,res=15)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from kan import KAN, create_dataset\n",
        "def reg(acts_scale,KAN_layer, factor=1,lamb_l1=1.,lamb_entropy=2.,lamb_coef=0.,lamb_coefdiff=0.):\n",
        "\n",
        "    def nonlinear(x, th=1e-16):\n",
        "        return (x < th) * x * factor + (x > th) * (x + (factor - 1) * th)\n",
        "\n",
        "    reg_ = 0.\n",
        "    for i in range(len(acts_scale)):\n",
        "        vec = acts_scale[i].reshape(-1, )\n",
        "\n",
        "        p = vec / torch.sum(vec)\n",
        "        l1 = torch.sum(nonlinear(vec))\n",
        "        entropy = - torch.sum(p * torch.log2(p + 1e-4))\n",
        "        reg_ += lamb_l1 * l1 + lamb_entropy * entropy  # both l1 and entropy\n",
        "\n",
        "    # regularize coefficient to encourage spline to be zero\n",
        "    for i in range(len(KAN_layer.act_fun)):\n",
        "        coeff_l1 = torch.sum(torch.mean(torch.abs(KAN_layer.act_fun[i].coef), dim=1))\n",
        "        coeff_diff_l1 = torch.sum(torch.mean(torch.abs(torch.diff(KAN_layer.act_fun[i].coef)), dim=1))\n",
        "        reg_ += lamb_coef * coeff_l1 + lamb_coefdiff * coeff_diff_l1\n",
        "\n",
        "    return reg_\n",
        "def accuracy(pred, true):\n",
        "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
        "    pred = pred.detach().cpu()\n",
        "    true = true.cpu()\n",
        "    try:\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "    except:\n",
        "        pred_labels = torch.round(pred)\n",
        "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
        "    correct = (pred_labels == true).sum()\n",
        "    # 정확도를 계산\n",
        "    acc = correct / true.size(0)\n",
        "    return acc.item() \n",
        "\n",
        "class Early_stop_train():\n",
        "    def __init__(self,model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        \n",
        "\n",
        "        \n",
        "        self.loss_list = [1e100]\n",
        "        self.acc_list = []\n",
        "        self.stop_count = 0\n",
        "        \n",
        "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10,lamb=0.):\n",
        "        #self.model.train()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            if self.stop_count>=res:\n",
        "                break\n",
        "            loss_val,_ = self.test(test_loader)\n",
        "            self.loss_list.append(loss_val)\n",
        "            \n",
        "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
        "                self.stop_count+=1\n",
        "            else:\n",
        "                self.optimal = self.model.state_dict()\n",
        "                self.stop_count = 0\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "            for X_train,y_train in train_loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(X_train)\n",
        "                reg_ = lamb*reg(self.model.KAN.acts_scale,self.model.KAN)\n",
        "                try:\n",
        "                    loss = self.criterion(output.squeeze(), y_train)+reg_\n",
        "                except:\n",
        "                    print(output)\n",
        "                    raise\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_list.append(loss.item())\n",
        "                acc = accuracy(output,y_train)\n",
        "                acc_list.append(acc)\n",
        "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} reg : {reg_:4f} stop count : {self.stop_count}\")\n",
        "        self.model.load_state_dict(self.optimal)\n",
        "    def test(self,test_loader):\n",
        "        if test_loader is None:\n",
        "            return 0,0\n",
        "        else:\n",
        "            #self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in test_loader:\n",
        "                    output = self.model(data)\n",
        "\n",
        "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
        "                    \n",
        "                    correct += accuracy(output,target)*len(output)\n",
        "\n",
        "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "            return test_loss,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 4.7787, Accuracy: 137.0/300 (46%)\n",
            "Epoch 1 Loss 0.709164 acc : 0.562689 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 2.8432, Accuracy: 196.00000023841858/300 (65%)\n",
            "Epoch 2 Loss 0.512632 acc : 0.796212 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 2.0469, Accuracy: 258.00000047683716/300 (86%)\n",
            "Epoch 3 Loss 0.371469 acc : 0.856913 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.5352, Accuracy: 266.0000002384186/300 (89%)\n",
            "Epoch 4 Loss 0.289598 acc : 0.899905 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.2422, Accuracy: 273.0000011920929/300 (91%)\n",
            "Epoch 5 Loss 0.239178 acc : 0.911364 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.1882, Accuracy: 274.99999952316284/300 (92%)\n",
            "Epoch 6 Loss 0.215999 acc : 0.924337 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.1382, Accuracy: 278.99999952316284/300 (93%)\n",
            "Epoch 7 Loss 0.198740 acc : 0.922633 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.1318, Accuracy: 278.99999952316284/300 (93%)\n",
            "Epoch 8 Loss 0.188532 acc : 0.931250 reg : 0.000000 stop count : 0\n",
            "Test set: Average loss: 1.1366, Accuracy: 280.99999952316284/300 (94%)\n",
            "Epoch 9 Loss 0.208819 acc : 0.932292 reg : 0.000000 stop count : 1"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 20\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m     19\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(model, optimizer, criterion)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m _,acc \u001b[38;5;241m=\u001b[39m train_process\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[31], line 74\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res, lamb)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     76\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from kan import KAN\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.KAN = KAN([PCA_dim,PCA_dim*2-1,1])\n",
        "    def forward(self, x):\n",
        "        output = self.KAN(x)\n",
        "        output = nn.Sigmoid()(output)\n",
        "        output = torch.squeeze(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Model(); criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=15)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
