{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pennylane\n",
    "!pip install pykan\n",
    "!git clone https://github.com/pop756/Quantum_machine.git\n",
    "%cd Quantum_machine\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "PCA_dim = 8\n",
    "CLS_num = 2\n",
    "\n",
    "\n",
    "\n",
    "with open('./data.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "X = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "def Fit_to_quantum(X,PCA_dim):\n",
    "    pca = PCA(n_components=PCA_dim)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# PyTorch Tensor로 변환\n",
    "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "class Feature_data_loader(Dataset):\n",
    "    def __init__(self,x_train,y_train):\n",
    "        self.feature1 = x_train\n",
    "        temp = copy.deepcopy(x_train)\n",
    "        shuffle = torch.randperm(len(temp))\n",
    "        self.feature2 = temp[shuffle]\n",
    "        self.y1 = y_train\n",
    "        temp_y = copy.deepcopy(y_train)\n",
    "        self.y2 = temp_y[shuffle]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature1)\n",
    "    def __getitem__(self,idx):\n",
    "        input1 = self.feature1[idx]\n",
    "        input2 = self.feature2[idx]\n",
    "        if self.y1[idx] == self.y2[idx]:\n",
    "            label = torch.tensor(1.).float()\n",
    "        else:\n",
    "            label = torch.tensor(0.).float()\n",
    "        return [input1,input2],label\n",
    "\n",
    "\n",
    "# DataLoader 생성\n",
    "\n",
    "\n",
    "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
    "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN, create_dataset\n",
    "def reg(acts_scale,KAN_layer, factor=1,lamb_l1=1.,lamb_entropy=2.,lamb_coef=0.,lamb_coefdiff=0.):\n",
    "\n",
    "    def nonlinear(x, th=1e-16):\n",
    "        return (x < th) * x * factor + (x > th) * (x + (factor - 1) * th)\n",
    "\n",
    "    reg_ = 0.\n",
    "    for i in range(len(acts_scale)):\n",
    "        vec = acts_scale[i].reshape(-1, )\n",
    "\n",
    "        p = vec / torch.sum(vec)\n",
    "        l1 = torch.sum(nonlinear(vec))\n",
    "        entropy = - torch.sum(p * torch.log2(p + 1e-4))\n",
    "        reg_ += lamb_l1 * l1 + lamb_entropy * entropy  # both l1 and entropy\n",
    "\n",
    "    # regularize coefficient to encourage spline to be zero\n",
    "    for i in range(len(KAN_layer.act_fun)):\n",
    "        coeff_l1 = torch.sum(torch.mean(torch.abs(KAN_layer.act_fun[i].coef), dim=1))\n",
    "        coeff_diff_l1 = torch.sum(torch.mean(torch.abs(torch.diff(KAN_layer.act_fun[i].coef)), dim=1))\n",
    "        reg_ += lamb_coef * coeff_l1 + lamb_coefdiff * coeff_diff_l1\n",
    "\n",
    "    return reg_\n",
    "def accuracy(pred, true):\n",
    "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
    "    pred = pred.detach().cpu()\n",
    "    true = true.cpu()\n",
    "    try:\n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "    except:\n",
    "        pred_labels = torch.round(pred)\n",
    "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
    "    correct = (pred_labels == true).sum()\n",
    "    # 정확도를 계산\n",
    "    acc = correct / true.size(0)\n",
    "    return acc.item() \n",
    "\n",
    "class Early_stop_train():\n",
    "    def __init__(self,model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "\n",
    "        \n",
    "        self.loss_list = [1e100]\n",
    "        self.acc_list = []\n",
    "        self.stop_count = 0\n",
    "        \n",
    "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10,lamb=0.,lamb_entropy=2.):\n",
    "        #self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if self.stop_count>=res:\n",
    "                break\n",
    "            loss_val,_ = self.test(test_loader)\n",
    "            self.loss_list.append(loss_val)\n",
    "            \n",
    "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
    "                self.stop_count+=1\n",
    "            else:\n",
    "                self.optimal = self.model.state_dict()\n",
    "                self.stop_count = 0\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            for X_train,y_train in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(X_train)\n",
    "                reg_ = lamb*reg(self.model.KAN.acts_scale,self.model.KAN,lamb_entropy=lamb_entropy)\n",
    "                try:\n",
    "                    loss = self.criterion(output.squeeze(), y_train)+reg_\n",
    "                except:\n",
    "                    print(output)\n",
    "                    raise\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss_list.append(loss.item())\n",
    "                acc = accuracy(output,y_train)\n",
    "                acc_list.append(acc)\n",
    "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} reg : {reg_:4f} stop count : {self.stop_count} lamb : {lamb} lamb_enp : {lamb_entropy}\")\n",
    "        self.model.load_state_dict(self.optimal)\n",
    "    def test(self,test_loader):\n",
    "        if test_loader is None:\n",
    "            return 0,0\n",
    "        else:\n",
    "            #self.model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    output = self.model(data)\n",
    "\n",
    "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
    "                    \n",
    "                    correct += accuracy(output,target)*len(output)\n",
    "\n",
    "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "            return test_loss,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 5.3539, Accuracy: 153.00000059604645/300 (51%)\n",
      "Epoch 1 Loss 0.800495 acc : 0.539962 reg : 0.000000 stop count : 0 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 3.1488, Accuracy: 204.00000047683716/300 (68%)\n",
      "Epoch 2 Loss 0.525330 acc : 0.747822 reg : 0.000000 stop count : 0 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 2.2007, Accuracy: 235.99999928474426/300 (79%)\n",
      "Epoch 3 Loss 0.382062 acc : 0.846970 reg : 0.000000 stop count : 0 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.9693, Accuracy: 248.99999976158142/300 (83%)\n",
      "Epoch 4 Loss 0.314495 acc : 0.868277 reg : 0.000000 stop count : 0 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.7901, Accuracy: 255.99999976158142/300 (85%)\n",
      "Epoch 5 Loss 0.253342 acc : 0.905682 reg : 0.000000 stop count : 0 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.9500, Accuracy: 253.00000047683716/300 (84%)\n",
      "Epoch 6 Loss 0.210566 acc : 0.923201 reg : 0.000000 stop count : 1 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.8943, Accuracy: 251.99999976158142/300 (84%)\n",
      "Epoch 7 Loss 0.182696 acc : 0.941477 reg : 0.000000 stop count : 2 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.8632, Accuracy: 257.99999952316284/300 (86%)\n",
      "Epoch 8 Loss 0.160226 acc : 0.951610 reg : 0.000000 stop count : 3 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.9155, Accuracy: 253.9999988079071/300 (85%)\n",
      "Epoch 9 Loss 0.133674 acc : 0.954167 reg : 0.000000 stop count : 4 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 2.1028, Accuracy: 247.99999904632568/300 (83%)\n",
      "Epoch 10 Loss 0.115624 acc : 0.959848 reg : 0.000000 stop count : 5 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.9348, Accuracy: 252.99999976158142/300 (84%)\n",
      "Epoch 11 Loss 0.095579 acc : 0.975568 reg : 0.000000 stop count : 6 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 2.1539, Accuracy: 248.99999976158142/300 (83%)\n",
      "Epoch 12 Loss 0.081255 acc : 0.975379 reg : 0.000000 stop count : 7 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 1.9672, Accuracy: 258.00000047683716/300 (86%)\n",
      "Epoch 13 Loss 0.073120 acc : 0.977273 reg : 0.000000 stop count : 8 lamb : 0.0 lamb_enp : 0.0\n",
      "Test set: Average loss: 2.2891, Accuracy: 248.00000071525574/300 (83%)\n",
      "Epoch 14 Loss 0.059560 acc : 0.982422 reg : 0.000000 stop count : 9 lamb : 0.0 lamb_enp : 0.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 166\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# 모델 학습 및 평가\u001b[39;00m\n\u001b[0;32m    165\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(feature_model, optimizer, criterion)\n\u001b[1;32m--> 166\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_feature_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlamb_entropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlamb_entropy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m#feature_model.KAN.plot(beta=3,scale=2)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m feature_model\u001b[38;5;241m.\u001b[39mKAN \u001b[38;5;241m=\u001b[39m feature_model\u001b[38;5;241m.\u001b[39mKAN\u001b[38;5;241m.\u001b[39mprune()\n",
      "Cell \u001b[1;32mIn[28], line 67\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res, lamb, lamb_entropy)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_train,y_train \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 67\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     reg_ \u001b[38;5;241m=\u001b[39m lamb\u001b[38;5;241m*\u001b[39mreg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mKAN\u001b[38;5;241m.\u001b[39macts_scale,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mKAN,lamb_entropy\u001b[38;5;241m=\u001b[39mlamb_entropy)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 128\u001b[0m, in \u001b[0;36mFeature_model.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    126\u001b[0m input2 \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#input2_copy = input2.clone().detach().requires_grad_(True)\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m input1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSigmoid()(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m#input1 = torch.concat([input1,input1_copy],dim=1)\u001b[39;00m\n\u001b[0;32m    130\u001b[0m input2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSigmoid()(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKAN(input2))\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\KAN.py:314\u001b[0m, in \u001b[0;36mKAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    311\u001b[0m x_numerical, preacts, postacts_numerical, postspline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fun[l](x)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_enabled \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     x_symbolic, postacts_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbolic_fun\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     x_symbolic \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\Symbolic_KANLayer.py:109\u001b[0m, in \u001b[0;36mSymbolic_KANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m postacts_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim):\n\u001b[1;32m--> 109\u001b[0m     xij \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine[j,i,\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine[j,i,\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    110\u001b[0m     postacts_\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[j][i]\u001b[38;5;241m*\u001b[39mxij)\n\u001b[0;32m    111\u001b[0m postacts\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mstack(postacts_))\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\kan\\Symbolic_KANLayer.py:67\u001b[0m, in \u001b[0;36mSymbolic_KANLayer.__init__.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(out_dim, in_dim, device\u001b[38;5;241m=\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# torch\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuns \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim)]\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# name\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuns_name \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_dim)] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import math\n",
    "lamb_list = [0.01*i for i in range(100)]\n",
    "lamb_entropy_list = [0.2*i for i in range(10)]\n",
    "\n",
    "\n",
    "result_dict  = []\n",
    "for lamb in lamb_list:\n",
    "    for lamb_entropy in lamb_entropy_list:\n",
    "\n",
    "        # 데이터 로드 및 전처리\n",
    "        \"\"\"\n",
    "        data = load_breast_cancer()\n",
    "        X = data.data\n",
    "        y = data.target\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        pca = PCA(n_components=PCA_dim)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.PCA_dim, random_state=seed)\n",
    "\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
    "\n",
    "        # Pennylane 장치 설정\n",
    "        dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
    "\n",
    "\n",
    "        def ZZFeatureMapLayer(features, wires):\n",
    "            \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
    "            index = 0\n",
    "            for i in wires:\n",
    "                qml.Hadamard(wires=i)\n",
    "                qml.RZ(features[:,index], wires=i)\n",
    "                index += 1\n",
    "\n",
    "            for j in range(0, len(wires)-1):\n",
    "                qml.CNOT(wires=[j, j+1])\n",
    "                qml.RZ((features[:,index]), wires=j+1)\n",
    "                qml.CNOT(wires=[j, j+1])\n",
    "                index+=1\n",
    "\n",
    "        def ansatz(params):\n",
    "            for j in range(len(params)):\n",
    "                # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
    "                for i in range(len(params[0])):\n",
    "                    qml.RY(params[j, i, 0], wires=i)\n",
    "                    qml.RZ(params[j, i, 1], wires=i)\n",
    "                    \n",
    "                # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
    "                if j == len(params)-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    for i in range(len(params[0])-1):\n",
    "                        qml.CNOT(wires=[i, i+1])\n",
    "\n",
    "\n",
    "        # 양자 레이어 정의\n",
    "        @qml.qnode(dev, interface='torch')\n",
    "        def QuantumLayer(features,params):\n",
    "            ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
    "            ansatz(params)\n",
    "            return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
    "\n",
    "\n",
    "        ## 양자 커널\n",
    "        @qml.qnode(dev, interface='torch')\n",
    "        def Kernal(features1,features2):\n",
    "            ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
    "            qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
    "            return qml.probs(wires=range(PCA_dim))\n",
    "\n",
    "\n",
    "        class Feature_model(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Feature_model,self).__init__()\n",
    "                KAN_model = KAN([PCA_dim,PCA_dim*4+1,PCA_dim*2-1],grid=5,)\n",
    "                \"\"\"\n",
    "                for i in range(PCA_dim):\n",
    "                    temp_list = [i for i in range(PCA_dim*2-1)]\n",
    "                    temp_list.remove(i)\n",
    "                    for j in temp_list:\n",
    "                        KAN_model.remove_edge(0,i,j)\n",
    "                for i in range(PCA_dim-1):\n",
    "                    if i == 0:\n",
    "                        temp_list = [i for i in range(PCA_dim*2-1)]\n",
    "                        temp_list.remove(i+PCA_dim)\n",
    "                        for j in temp_list:\n",
    "                            KAN_model.remove_edge(0,i,j)\n",
    "                    elif i == PCA_dim-2:\n",
    "                        temp_list = [i for i in range(PCA_dim*2-1)]\n",
    "                        temp_list.remove(PCA_dim*2-2)\n",
    "                        for j in temp_list:\n",
    "                            KAN_model.remove_edge(0,i,j)\n",
    "                    else:\n",
    "                        temp_list = [i for i in range(PCA_dim*2-1)]\n",
    "                        temp_list.remove(i+PCA_dim)\n",
    "                        temp_list.remove(i+PCA_dim-1)\n",
    "                        for j in temp_list:\n",
    "                            KAN_model.remove_edge(0,i,j)\n",
    "                            \n",
    "                for i in range(2*PCA_dim-1):\n",
    "                    temp_list = [i for i in range(PCA_dim*2-1)]\n",
    "                    temp_list.remove(i)\n",
    "                    for j in temp_list:\n",
    "                        KAN_model.remove_edge(1,i,j)\n",
    "                \"\"\"\n",
    "                self.KAN = KAN_model\n",
    "                self.Kernal = Kernal\n",
    "            def forward(self,inputs):\n",
    "                epsilon = 1e-6\n",
    "                input1 = inputs[0]\n",
    "                #input1_copy = input1.clone().detach().requires_grad_(True)\n",
    "                input2 = inputs[1]\n",
    "                #input2_copy = input2.clone().detach().requires_grad_(True)\n",
    "                input1 = nn.Sigmoid()(self.KAN(input1))*np.pi\n",
    "                #input1 = torch.concat([input1,input1_copy],dim=1)\n",
    "                input2 = nn.Sigmoid()(self.KAN(input2))*np.pi\n",
    "                #input2 = torch.concat([input2,input2_copy],dim=1)\n",
    "                output = self.Kernal(input1,input2)\n",
    "                output = output.type(torch.float32)\n",
    "                \n",
    "                return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "        # 하이브리드 모델 정의\n",
    "        class HybridModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(HybridModel, self).__init__()\n",
    "                self.KAN = feature_model.KAN\n",
    "                \n",
    "                self.quantum_layer = QuantumLayer\n",
    "                self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
    "            def forward(self, x):\n",
    "                x = nn.Sigmoid()(self.KAN(x))*np.pi\n",
    "                #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
    "                quantum_output = self.quantum_layer(x,self.Q_params)\n",
    "                quantum_output = quantum_output.type(torch.float32)\n",
    "                return torch.log(quantum_output)\n",
    "            \n",
    "\n",
    "        feature_model = Feature_model(); criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(feature_model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "        # 모델 학습 및 평가\n",
    "        train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
    "        train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15,lamb=lamb,lamb_entropy=lamb_entropy)\n",
    "        #feature_model.KAN.plot(beta=3,scale=2)\n",
    "        feature_model.KAN = feature_model.KAN.prune()\n",
    "        _,acc = train_process.test(test_feature_loader)\n",
    "        print(f\"\\n Pretrain acc : {acc}\")\n",
    "\n",
    "        model = HybridModel(); criterion = nn.NLLLoss()\n",
    "\n",
    "        for param in model.KAN.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        optimizer = optim.Adam(model.Q_params.parameters(), lr=0.01)\n",
    "        print(\"\\n\\nTest start\\n\\n\")\n",
    "        train_process = Early_stop_train(model, optimizer, criterion)\n",
    "        train_process.train_model(train_loader,test_loader,epochs=15,lamb=0)\n",
    "\n",
    "        _,acc = train_process.test(test_loader)\n",
    "        print(f\"Test Accuracy: {acc:.2f}\")\n",
    "        result_dict.append({\"lamb\":lamb,\"lamb_entropy\":f\"{lamb_entropy:.1f}\",\"acc\" : acc})\n",
    "        result = pd.DataFrame(result_dict)\n",
    "        result.to_csv('./Results.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
