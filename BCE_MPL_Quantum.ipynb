{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP7p3x26kiZ2",
        "outputId": "ffe9c30b-83e8-4e63-9d7b-8837392664ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.14.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.6.9)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
            "Requirement already satisfied: pennylane-lightning>=0.36 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.11.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
            "Requirement already satisfied: pykan in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Cloning into 'Quantum_machine'...\n",
            "remote: Enumerating objects: 481, done.\u001b[K\n",
            "remote: Counting objects: 100% (481/481), done.\u001b[K\n",
            "remote: Compressing objects: 100% (469/469), done.\u001b[K\n",
            "remote: Total 481 (delta 26), reused 457 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (481/481), 8.95 MiB | 26.88 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/Quantum_machine/Quantum_machine\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane\n",
        "!pip install pykan\n",
        "!git clone https://github.com/pop756/Quantum_machine.git\n",
        "%cd Quantum_machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vJl4rYpKBAEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "PCA_dim = 8\n",
        "CLS_num = 2\n",
        "\n",
        "\n",
        "\n",
        "with open('./data/data.pkl','rb') as file:\n",
        "    data = pickle.load(file)\n",
        "X = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "def Fit_to_quantum(X,PCA_dim):\n",
        "    pca = PCA(n_components=PCA_dim)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "class Feature_data_loader(Dataset):\n",
        "    def __init__(self,x_train,y_train):\n",
        "        self.feature1 = x_train\n",
        "        temp = copy.deepcopy(x_train)\n",
        "        shuffle = torch.randperm(len(temp))\n",
        "        self.feature2 = temp[shuffle]\n",
        "        self.y1 = y_train\n",
        "        temp_y = copy.deepcopy(y_train)\n",
        "        self.y2 = temp_y[shuffle]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature1)\n",
        "    def __getitem__(self,idx):\n",
        "        input1 = self.feature1[idx]\n",
        "        input2 = self.feature2[idx]\n",
        "        if self.y1[idx] == self.y2[idx]:\n",
        "            label = torch.tensor(1.).float()\n",
        "        else:\n",
        "            label = torch.tensor(0.).float()\n",
        "        return [input1,input2],label\n",
        "\n",
        "\n",
        "# DataLoader 생성\n",
        "\n",
        "\n",
        "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
        "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
        "train_loader = DataLoader(TensorDataset(x_train_pca, y_train.float()), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_pca, y_test.float()), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3d3GfMBAEv",
        "outputId": "dce7ef57-3f8c-4efd-a9f9-b7b100cc40b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 9.1759, Accuracy: 141.99999952316284/300 (47%)\n",
            "Epoch 1 Loss 1.147430 acc : 0.494602 stop count : 0\n",
            "Test set: Average loss: 4.0952, Accuracy: 172.00000047683716/300 (57%)\n",
            "Epoch 2 Loss 0.723343 acc : 0.624148 stop count : 0\n",
            "Test set: Average loss: 3.4196, Accuracy: 190.00000047683716/300 (63%)\n",
            "Epoch 3 Loss 0.601010 acc : 0.688258 stop count : 0\n",
            "Test set: Average loss: 3.2664, Accuracy: 196.99999976158142/300 (66%)\n",
            "Epoch 4 Loss 0.540545 acc : 0.742708 stop count : 0\n",
            "Test set: Average loss: 2.9905, Accuracy: 212.00000047683716/300 (71%)\n",
            "Epoch 5 Loss 0.487960 acc : 0.762879 stop count : 0\n",
            "Test set: Average loss: 2.8034, Accuracy: 222.99999928474426/300 (74%)\n",
            "Epoch 6 Loss 0.449544 acc : 0.784470 stop count : 0\n",
            "Test set: Average loss: 2.7302, Accuracy: 222.99999928474426/300 (74%)\n",
            "Epoch 7 Loss 0.406674 acc : 0.817330 stop count : 0\n",
            "Test set: Average loss: 2.5834, Accuracy: 231.00000095367432/300 (77%)\n",
            "Epoch 8 Loss 0.374612 acc : 0.831155 stop count : 0\n",
            "Test set: Average loss: 2.4782, Accuracy: 231.99999904632568/300 (77%)\n",
            "Epoch 9 Loss 0.350925 acc : 0.837311 stop count : 0\n",
            "Test set: Average loss: 2.3417, Accuracy: 232.0/300 (77%)\n",
            "Epoch 10 Loss 0.323864 acc : 0.858617 stop count : 0\n",
            "Test set: Average loss: 2.3068, Accuracy: 234.99999904632568/300 (78%)\n",
            "Epoch 11 Loss 0.303137 acc : 0.871212 stop count : 0\n",
            "Test set: Average loss: 2.2568, Accuracy: 235.00000071525574/300 (78%)\n",
            "Epoch 12 Loss 0.274901 acc : 0.881723 stop count : 0\n",
            "Test set: Average loss: 2.0924, Accuracy: 235.99999904632568/300 (79%)\n",
            "Epoch 13 Loss 0.254321 acc : 0.894223 stop count : 0\n",
            "Test set: Average loss: 2.2092, Accuracy: 233.0/300 (78%)\n",
            "Epoch 14 Loss 0.245468 acc : 0.892614 stop count : 1\n",
            "Test set: Average loss: 2.1070, Accuracy: 238.99999976158142/300 (80%)\n",
            "Epoch 15 Loss 0.223377 acc : 0.907102 stop count : 2\n",
            "Test set: Average loss: 2.0858, Accuracy: 235.99999904632568/300 (79%)\n",
            "Epoch 16 Loss 0.211156 acc : 0.912973 stop count : 0\n",
            "Test set: Average loss: 2.0104, Accuracy: 246.00000071525574/300 (82%)\n",
            "Epoch 17 Loss 0.194944 acc : 0.927083 stop count : 0\n",
            "Test set: Average loss: 2.0326, Accuracy: 242.99999976158142/300 (81%)\n",
            "Epoch 18 Loss 0.178886 acc : 0.937216 stop count : 1\n",
            "Test set: Average loss: 1.9131, Accuracy: 247.9999988079071/300 (83%)\n",
            "Epoch 19 Loss 0.181119 acc : 0.921591 stop count : 0\n",
            "Test set: Average loss: 2.0962, Accuracy: 243.00000071525574/300 (81%)\n",
            "Epoch 20 Loss 0.161228 acc : 0.939962 stop count : 1\n",
            "Test set: Average loss: 2.0453, Accuracy: 248.99999976158142/300 (83%)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 148\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 모델 학습 및 평가\u001b[39;00m\n\u001b[0;32m    147\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(feature_model, optimizer, criterion)\n\u001b[1;32m--> 148\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_feature_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridModel(); criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39mparameters():\n",
            "File \u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Project\\AI_quantum_project\\Quantum_machine\\functions\\training.py:62\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res, device)\u001b[0m\n\u001b[0;32m     58\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X_train)\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39msqueeze(), y_train)\n\u001b[1;32m---> 62\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     64\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "from functions.training import Early_stop_train\n",
        "\n",
        "\n",
        "result_list_classical = []\n",
        "# Pennylane 장치 설정\n",
        "dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
        "\n",
        "\n",
        "def ZZFeatureMapLayer(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ZZFeatureMapLayer_fixed(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "    index=0\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index])*(features[:,index+1]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ansatz(params):\n",
        "    for j in range(len(params)):\n",
        "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
        "        for i in range(len(params[0])):\n",
        "            qml.RY(params[j, i, 0], wires=i)\n",
        "            qml.RZ(params[j, i, 1], wires=i)\n",
        "\n",
        "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
        "        if j == len(params)-1:\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(len(params[0])-1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "\n",
        "# 양자 레이어 정의\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def QuantumLayer(features,params):\n",
        "    ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
        "    ansatz(params)\n",
        "    return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
        "\n",
        "\n",
        "## 양자 커널\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal(features1,features2):\n",
        "    ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal_fix(features1,features2):\n",
        "    ZZFeatureMapLayer_fixed(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer_fixed)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "\n",
        "class Feature_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Feature_model,self).__init__()\n",
        "        self.cls = nn.Sequential(OrderedDict([('cls1', nn.Linear(PCA_dim,PCA_dim*8)),\n",
        "                                              ('relu1', nn.ReLU()),('cls2', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu2', nn.ReLU()),('cls3', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu3', nn.ReLU()),('cls4', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu4', nn.ReLU()),('cls5', nn.Linear(PCA_dim*8,PCA_dim*2-1))\n",
        "                                              ]))\n",
        "        self.Kernal = Kernal\n",
        "    def forward(self,inputs):\n",
        "        epsilon = 1e-6\n",
        "        input1 = inputs[0]\n",
        "        input2 = inputs[1]\n",
        "        input1 = self.cls(input1)*np.pi\n",
        "        input2 = self.cls(input2)*np.pi\n",
        "        output = self.Kernal(input1,input2)\n",
        "        output = output.type(torch.float32)\n",
        "        return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 하이브리드 모델 정의\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cls = feature_model.cls\n",
        "\n",
        "        self.quantum_layer = QuantumLayer\n",
        "        self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
        "    def forward(self, x):\n",
        "        epsilon = 1e-6\n",
        "        x = self.cls(x)*np.pi\n",
        "        #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
        "        quantum_output = self.quantum_layer(x,self.Q_params)\n",
        "        quantum_output = quantum_output.type(torch.float32)\n",
        "        return quantum_output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cls_layer_1 = nn.Linear(PCA_dim,PCA_dim*PCA_dim)\n",
        "        self.cls_layer_2 = nn.Linear(PCA_dim*PCA_dim,PCA_dim*PCA_dim-1)\n",
        "        self.output_layer = nn.Linear(PCA_dim*PCA_dim-1,PCA_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.cls_layer_1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.cls_layer_2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "# 모델, 손실 함수, 최적화 설정\n",
        "\n",
        "\n",
        "feature_model = Feature_model(); criterion = nn.BCELoss()\n",
        "#model = Model(); criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(feature_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
        "train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15)\n",
        " \n",
        "model = HybridModel(); criterion = nn.BCELoss()\n",
        "for param in model.cls.parameters():\n",
        "    param.requires_grad = False\n",
        "#model.load_state_dict(para_dict)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "print('\\n\\n Test start \\n\\n')\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=5)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "result_list_classical.append(acc)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Test start \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 3.2300, Accuracy: 257.99999952316284/300 (86%)\n",
            "Epoch 1 Loss 0.532657 acc : 0.978693 stop count : 0\n",
            "Test set: Average loss: 2.4358, Accuracy: 265.9999988079071/300 (89%)\n",
            "Epoch 2 Loss 0.379985 acc : 0.981534 stop count : 0\n",
            "Test set: Average loss: 2.0270, Accuracy: 263.9999988079071/300 (88%)\n",
            "Epoch 3 Loss 0.284708 acc : 0.977273 stop count : 0\n",
            "Test set: Average loss: 1.7702, Accuracy: 264.9999988079071/300 (88%)\n",
            "Epoch 4 Loss 0.225733 acc : 0.977178 stop count : 0\n",
            "Test set: Average loss: 1.6297, Accuracy: 263.9999988079071/300 (88%)\n",
            "Epoch 5 Loss 0.188910 acc : 0.977273 stop count : 0\n",
            "Test set: Average loss: 1.5429, Accuracy: 264.9999988079071/300 (88%)\n",
            "Epoch 6 Loss 0.162743 acc : 0.977273 stop count : 0\n",
            "Test set: Average loss: 1.4786, Accuracy: 263.9999988079071/300 (88%)\n",
            "Epoch 7 Loss 0.138512 acc : 0.980469 stop count : 0"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Test start \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(model, optimizer, criterion)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m _,acc \u001b[38;5;241m=\u001b[39m train_process\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[0;32m     11\u001b[0m result_list_classical\u001b[38;5;241m.\u001b[39mappend(acc)\n",
            "File \u001b[1;32mc:\\Users\\pop75\\OneDrive\\Desktop\\Project\\AI_quantum_project\\Quantum_machine\\functions\\training.py:58\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res, device)\u001b[0m\n\u001b[0;32m     56\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 58\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39msqueeze(), y_train)\n\u001b[0;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[4], line 121\u001b[0m, in \u001b[0;36mHybridModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    119\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(x)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#print(qml.draw(self.quantum_layer)(x,self.Q_params))\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m quantum_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantum_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m quantum_output \u001b[38;5;241m=\u001b[39m quantum_output\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantum_output[:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39mepsilon, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mepsilon)\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\qnode.py:1027\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         full_transform_program\u001b[38;5;241m.\u001b[39m_set_all_argnums(\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m, args, kwargs, argnums\n\u001b[0;32m   1024\u001b[0m         )  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m-> 1027\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:616\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[1;32m--> 616\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[0;32m    619\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:249\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[1;34m(tapes, **_)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_only:\n\u001b[0;32m    248\u001b[0m     tapes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(qml\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mconvert_to_numpy_parameters(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tapes)\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_device_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\interfaces\\execution.py:371\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (res, []) \u001b[38;5;28;01mif\u001b[39;00m return_tuple \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# convert to list as new device interface returns a tuple\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecution_tapes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    373\u001b[0m final_res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tapes):\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:474\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[1;34m(self, circuits, execution_config)\u001b[0m\n\u001b[0;32m    468\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    469\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 474\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m    475\u001b[0m         simulate(\n\u001b[0;32m    476\u001b[0m             c,\n\u001b[0;32m    477\u001b[0m             rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    478\u001b[0m             prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prng_key,\n\u001b[0;32m    479\u001b[0m             debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_debugger,\n\u001b[0;32m    480\u001b[0m             interface\u001b[38;5;241m=\u001b[39minterface,\n\u001b[0;32m    481\u001b[0m         )\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    485\u001b[0m     vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\default_qubit.py:475\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    468\u001b[0m interface \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    469\u001b[0m     execution_config\u001b[38;5;241m.\u001b[39minterface\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m execution_config\u001b[38;5;241m.\u001b[39mgradient_method \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    474\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m--> 475\u001b[0m         \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prng_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    485\u001b[0m     vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:269\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(circuit, rng, prng_key, debugger, interface)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimulate\u001b[39m(\n\u001b[0;32m    239\u001b[0m     circuit: qml\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mQuantumScript, rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prng_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interface\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result:\n\u001b[0;32m    241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Simulate a single quantum script.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m    This is an internal function that will be called by the successor to ``default.qubit``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m \n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     state, is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m measure_final_state(circuit, state, is_state_batched, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key)\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\qubit\\simulate.py:161\u001b[0m, in \u001b[0;36mget_final_state\u001b[1;34m(circuit, debugger, interface)\u001b[0m\n\u001b[0;32m    159\u001b[0m is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(prep \u001b[38;5;129;01mand\u001b[39;00m prep\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m circuit\u001b[38;5;241m.\u001b[39moperations[\u001b[38;5;28mbool\u001b[39m(prep) :]:\n\u001b[1;32m--> 161\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:198\u001b[0m, in \u001b[0;36mapply_operation\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;129m@singledispatch\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation\u001b[39m(\n\u001b[0;32m    152\u001b[0m     op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator, state, is_state_batched: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, debugger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    153\u001b[0m ):\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply and operator to a given state.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_operation_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:208\u001b[0m, in \u001b[0;36m_apply_operation_default\u001b[1;34m(op, state, is_state_batched, debugger)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The default behaviour of apply_operation, accessed through the standard dispatch\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03mof apply_operation, as well as conditionally in other dispatches.\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires) \u001b[38;5;241m<\u001b[39m EINSUM_OP_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39mndim(state) \u001b[38;5;241m<\u001b[39m EINSUM_STATE_WIRECOUNT_PERF_THRESHOLD\n\u001b[0;32m    207\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (op\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;129;01mand\u001b[39;00m is_state_batched):\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_operation_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_operation_tensordot(op, state, is_state_batched\u001b[38;5;241m=\u001b[39mis_state_batched)\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\devices\\qubit\\apply_operation.py:72\u001b[0m, in \u001b[0;36mapply_operation_einsum\u001b[1;34m(op, state, is_state_batched)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_operation_einsum\u001b[39m(op: qml\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mOperator, state, is_state_batched: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply ``Operator`` to ``state`` using ``einsum``. This is more efficent at lower qubit\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    numbers.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        array[complex]: output_state\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     mat \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     total_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(state\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m is_state_batched\n\u001b[0;32m     75\u001b[0m     num_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(op\u001b[38;5;241m.\u001b[39mwires)\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\operation.py:775\u001b[0m, in \u001b[0;36mOperator.matrix\u001b[1;34m(self, wire_order)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatrix\u001b[39m(\u001b[38;5;28mself\u001b[39m, wire_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    756\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Representation of the operator as a matrix in the computational basis.\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \n\u001b[0;32m    758\u001b[0m \u001b[38;5;124;03m    If ``wire_order`` is provided, the numerical representation considers the position of the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    773\u001b[0m \u001b[38;5;124;03m        tensor_like: matrix representation\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 775\u001b[0m     canonical_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wire_order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwires \u001b[38;5;241m==\u001b[39m Wires(wire_order):\n\u001b[0;32m    778\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m canonical_matrix\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops_single_qubit.py:296\u001b[0m, in \u001b[0;36mRZ.compute_matrix\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m    293\u001b[0m signs \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], like\u001b[38;5;241m=\u001b[39mtheta)\n\u001b[0;32m    294\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39mj \u001b[38;5;241m*\u001b[39m theta\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m(arg) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mdiag(qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(arg \u001b[38;5;241m*\u001b[39m signs))\n\u001b[0;32m    299\u001b[0m diags \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mexp(qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mouter(arg, signs))\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\pennylane\\math\\__init__.py:129\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# small constant for numerical stability that the user can modify\u001b[39;00m\n\u001b[0;32m    126\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-14\u001b[39m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(numpy_mimic, name)\n\u001b[0;32m    133\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallclose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    180\u001b[0m ]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = HybridModel(); criterion = nn.BCELoss()\n",
        "for param in model.cls.parameters():\n",
        "    param.requires_grad = False\n",
        "#model.load_state_dict(para_dict)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "print('\\n\\n Test start \\n\\n')\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=5)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "result_list_classical.append(acc)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fltgmdndkgx0",
        "outputId": "62dc3fa9-daad-4d8d-9226-2c659471af99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [05:01<00:00,  2.32it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "class Kernal_method():\n",
        "    def __init__(self,Kernal):\n",
        "        self.Ker = Kernal\n",
        "    \n",
        "    def \n",
        "\n",
        "\n",
        "x_train = torch.tensor(x_train).float()\n",
        "num_data = x_train.shape[0]\n",
        "kernel_matrix = torch.zeros((num_data, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_data)\n",
        "    output = feature_model([data,x_train])\n",
        "    kernel_matrix[i] = output.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgvLiqvOkgx3",
        "outputId": "2645aaf8-6657-42bf-f1bc-e32218adf955"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_17080\\2430800910.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(y_train).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(11626.8711, grad_fn=<AddBackward0>) tensor(40000., grad_fn=<MulBackward0>)\n",
            "tensor(-11467.4785, grad_fn=<AddBackward0>) tensor(16900.4082, grad_fn=<MulBackward0>)\n",
            "tensor(-24483.9355, grad_fn=<AddBackward0>) tensor(3879.0730, grad_fn=<MulBackward0>)\n",
            "tensor(-28358.6680, grad_fn=<AddBackward0>) tensor(0.1115, grad_fn=<MulBackward0>)\n",
            "tensor(-25809.5742, grad_fn=<AddBackward0>) tensor(2546.0581, grad_fn=<MulBackward0>)\n",
            "tensor(-21156.7012, grad_fn=<AddBackward0>) tensor(7197.2168, grad_fn=<MulBackward0>)\n",
            "tensor(-18112.3242, grad_fn=<AddBackward0>) tensor(10241.2979, grad_fn=<MulBackward0>)\n",
            "tensor(-17972.2812, grad_fn=<AddBackward0>) tensor(10382.1631, grad_fn=<MulBackward0>)\n",
            "tensor(-20125.0586, grad_fn=<AddBackward0>) tensor(8231.0068, grad_fn=<MulBackward0>)\n",
            "tensor(-23266.3809, grad_fn=<AddBackward0>) tensor(5091.8472, grad_fn=<MulBackward0>)\n",
            "tensor(-26152.6465, grad_fn=<AddBackward0>) tensor(2208.0950, grad_fn=<MulBackward0>)\n",
            "tensor(-27942.9102, grad_fn=<AddBackward0>) tensor(420.5293, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.1973, grad_fn=<AddBackward0>) tensor(28.9568, grad_fn=<MulBackward0>)\n",
            "tensor(-27575.6133, grad_fn=<AddBackward0>) tensor(793.1105, grad_fn=<MulBackward0>)\n",
            "tensor(-26277.5000, grad_fn=<AddBackward0>) tensor(2093.5007, grad_fn=<MulBackward0>)\n",
            "tensor(-25145.7070, grad_fn=<AddBackward0>) tensor(3227.1565, grad_fn=<MulBackward0>)\n",
            "tensor(-24668.9238, grad_fn=<AddBackward0>) tensor(3705.3374, grad_fn=<MulBackward0>)\n",
            "tensor(-24973.3555, grad_fn=<AddBackward0>) tensor(3401.8562, grad_fn=<MulBackward0>)\n",
            "tensor(-25859.3594, grad_fn=<AddBackward0>) tensor(2516.4084, grad_fn=<MulBackward0>)\n",
            "tensor(-26949.6211, grad_fn=<AddBackward0>) tensor(1426.4170, grad_fn=<MulBackward0>)\n",
            "tensor(-27859.1113, grad_fn=<AddBackward0>) tensor(517.0309, grad_fn=<MulBackward0>)\n",
            "tensor(-28330.3613, grad_fn=<AddBackward0>) tensor(45.8541, grad_fn=<MulBackward0>)\n",
            "tensor(-28304.8867, grad_fn=<AddBackward0>) tensor(71.4865, grad_fn=<MulBackward0>)\n",
            "tensor(-27917.5898, grad_fn=<AddBackward0>) tensor(459.1383, grad_fn=<MulBackward0>)\n",
            "tensor(-27419.2109, grad_fn=<AddBackward0>) tensor(958.1445, grad_fn=<MulBackward0>)\n",
            "tensor(-27060.6504, grad_fn=<AddBackward0>) tensor(1317.6547, grad_fn=<MulBackward0>)\n",
            "tensor(-26992.0078, grad_fn=<AddBackward0>) tensor(1387.5674, grad_fn=<MulBackward0>)\n",
            "tensor(-27219.1211, grad_fn=<AddBackward0>) tensor(1162.0302, grad_fn=<MulBackward0>)\n",
            "tensor(-27626.1016, grad_fn=<AddBackward0>) tensor(756.8748, grad_fn=<MulBackward0>)\n",
            "tensor(-28042.2305, grad_fn=<AddBackward0>) tensor(342.7581, grad_fn=<MulBackward0>)\n",
            "tensor(-28318.7266, grad_fn=<AddBackward0>) tensor(68.3811, grad_fn=<MulBackward0>)\n",
            "tensor(-28385.2793, grad_fn=<AddBackward0>) tensor(3.9780, grad_fn=<MulBackward0>)\n",
            "tensor(-28267.6641, grad_fn=<AddBackward0>) tensor(123.6896, grad_fn=<MulBackward0>)\n",
            "tensor(-28063.6465, grad_fn=<AddBackward0>) tensor(329.6855, grad_fn=<MulBackward0>)\n",
            "tensor(-27891.0840, grad_fn=<AddBackward0>) tensor(504.0558, grad_fn=<MulBackward0>)\n",
            "tensor(-27833.9531, grad_fn=<AddBackward0>) tensor(562.8059, grad_fn=<MulBackward0>)\n",
            "tensor(-27911.0020, grad_fn=<AddBackward0>) tensor(487.1861, grad_fn=<MulBackward0>)\n",
            "tensor(-28077.8125, grad_fn=<AddBackward0>) tensor(321.6510, grad_fn=<MulBackward0>)\n",
            "tensor(-28256.4531, grad_fn=<AddBackward0>) tensor(144.1795, grad_fn=<MulBackward0>)\n",
            "tensor(-28375.2070, grad_fn=<AddBackward0>) tensor(26.5464, grad_fn=<MulBackward0>)\n",
            "tensor(-28399.9414, grad_fn=<AddBackward0>) tensor(2.9548, grad_fn=<MulBackward0>)\n",
            "tensor(-28344.0098, grad_fn=<AddBackward0>) tensor(60.1107, grad_fn=<MulBackward0>)\n",
            "tensor(-28255.2070, grad_fn=<AddBackward0>) tensor(150.2680, grad_fn=<MulBackward0>)\n",
            "tensor(-28188.3535, grad_fn=<AddBackward0>) tensor(218.6330, grad_fn=<MulBackward0>)\n",
            "tensor(-28178.4316, grad_fn=<AddBackward0>) tensor(230.2302, grad_fn=<MulBackward0>)\n",
            "tensor(-28226.9883, grad_fn=<AddBackward0>) tensor(183.5119, grad_fn=<MulBackward0>)\n",
            "tensor(-28306.7109, grad_fn=<AddBackward0>) tensor(105.7571, grad_fn=<MulBackward0>)\n",
            "tensor(-28379.1309, grad_fn=<AddBackward0>) tensor(35.3991, grad_fn=<MulBackward0>)\n",
            "tensor(-28415.1777, grad_fn=<AddBackward0>) tensor(1.4594, grad_fn=<MulBackward0>)\n",
            "tensor(-28408.0547, grad_fn=<AddBackward0>) tensor(10.6923, grad_fn=<MulBackward0>)\n",
            "tensor(-28373.2383, grad_fn=<AddBackward0>) tensor(47.5807, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.3125, grad_fn=<AddBackward0>) tensor(85.5056, grad_fn=<MulBackward0>)\n",
            "tensor(-28322.7578, grad_fn=<AddBackward0>) tensor(101.9731, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.1113, grad_fn=<AddBackward0>) tensor(89.4451, grad_fn=<MulBackward0>)\n",
            "tensor(-28371.4551, grad_fn=<AddBackward0>) tensor(56.8546, grad_fn=<MulBackward0>)\n",
            "tensor(-28407.6816, grad_fn=<AddBackward0>) tensor(22.3385, grad_fn=<MulBackward0>)\n",
            "tensor(-28429.4863, grad_fn=<AddBackward0>) tensor(2.2332, grad_fn=<MulBackward0>)\n",
            "tensor(-28430.7090, grad_fn=<AddBackward0>) tensor(2.7308, grad_fn=<MulBackward0>)\n",
            "tensor(-28417.0566, grad_fn=<AddBackward0>) tensor(18.1601, grad_fn=<MulBackward0>)\n",
            "tensor(-28401.1465, grad_fn=<AddBackward0>) tensor(35.9233, grad_fn=<MulBackward0>)\n",
            "tensor(-28394.5879, grad_fn=<AddBackward0>) tensor(44.4304, grad_fn=<MulBackward0>)\n",
            "tensor(-28401.7852, grad_fn=<AddBackward0>) tensor(39.2714, grad_fn=<MulBackward0>)\n",
            "tensor(-28418.6895, grad_fn=<AddBackward0>) tensor(24.4874, grad_fn=<MulBackward0>)\n",
            "tensor(-28436.4141, grad_fn=<AddBackward0>) tensor(8.9416, grad_fn=<MulBackward0>)\n",
            "tensor(-28447.0137, grad_fn=<AddBackward0>) tensor(0.5586, grad_fn=<MulBackward0>)\n",
            "tensor(-28447.8164, grad_fn=<AddBackward0>) tensor(1.9838, grad_fn=<MulBackward0>)\n",
            "tensor(-28442.0977, grad_fn=<AddBackward0>) tensor(9.9154, grad_fn=<MulBackward0>)\n",
            "tensor(-28436.2598, grad_fn=<AddBackward0>) tensor(17.9427, grad_fn=<MulBackward0>)\n",
            "tensor(-28435.6152, grad_fn=<AddBackward0>) tensor(20.7342, grad_fn=<MulBackward0>)\n",
            "tensor(-28441.5020, grad_fn=<AddBackward0>) tensor(16.9580, grad_fn=<MulBackward0>)\n",
            "tensor(-28451.1582, grad_fn=<AddBackward0>) tensor(9.3877, grad_fn=<MulBackward0>)\n",
            "tensor(-28460.0078, grad_fn=<AddBackward0>) tensor(2.6122, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.6992, grad_fn=<AddBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.9004, grad_fn=<AddBackward0>) tensor(1.9066, grad_fn=<MulBackward0>)\n",
            "tensor(-28463.0137, grad_fn=<AddBackward0>) tensor(5.9434, grad_fn=<MulBackward0>)\n",
            "tensor(-28462.2598, grad_fn=<AddBackward0>) tensor(8.8970, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.6035, grad_fn=<AddBackward0>) tensor(8.8095, grad_fn=<MulBackward0>)\n",
            "tensor(-28469.7324, grad_fn=<AddBackward0>) tensor(5.9841, grad_fn=<MulBackward0>)\n",
            "tensor(-28475.6602, grad_fn=<AddBackward0>) tensor(2.4030, grad_fn=<MulBackward0>)\n",
            "tensor(-28480.2227, grad_fn=<AddBackward0>) tensor(0.2173, grad_fn=<MulBackward0>)\n",
            "tensor(-28482.4648, grad_fn=<AddBackward0>) tensor(0.3629, grad_fn=<MulBackward0>)\n",
            "tensor(-28483.0488, grad_fn=<AddBackward0>) tensor(2.1652, grad_fn=<MulBackward0>)\n",
            "tensor(-28483.5703, grad_fn=<AddBackward0>) tensor(4.0290, grad_fn=<MulBackward0>)\n",
            "tensor(-28485.3691, grad_fn=<AddBackward0>) tensor(4.6012, grad_fn=<MulBackward0>)\n",
            "tensor(-28488.7344, grad_fn=<AddBackward0>) tensor(3.5941, grad_fn=<MulBackward0>)\n",
            "tensor(-28492.8887, grad_fn=<AddBackward0>) tensor(1.7907, grad_fn=<MulBackward0>)\n",
            "tensor(-28496.6738, grad_fn=<AddBackward0>) tensor(0.3593, grad_fn=<MulBackward0>)\n",
            "tensor(-28499.3691, grad_fn=<AddBackward0>) tensor(0.0294, grad_fn=<MulBackward0>)\n",
            "tensor(-28501.0938, grad_fn=<AddBackward0>) tensor(0.6919, grad_fn=<MulBackward0>)\n",
            "tensor(-28502.5898, grad_fn=<AddBackward0>) tensor(1.6155, grad_fn=<MulBackward0>)\n",
            "tensor(-28504.6113, grad_fn=<AddBackward0>) tensor(2.0425, grad_fn=<MulBackward0>)\n",
            "tensor(-28507.4492, grad_fn=<AddBackward0>) tensor(1.6940, grad_fn=<MulBackward0>)\n",
            "tensor(-28510.7812, grad_fn=<AddBackward0>) tensor(0.8791, grad_fn=<MulBackward0>)\n",
            "tensor(-28514.0215, grad_fn=<AddBackward0>) tensor(0.1817, grad_fn=<MulBackward0>)\n",
            "tensor(-28516.7461, grad_fn=<AddBackward0>) tensor(0.0151, grad_fn=<MulBackward0>)\n",
            "tensor(-28518.9688, grad_fn=<AddBackward0>) tensor(0.3640, grad_fn=<MulBackward0>)\n",
            "tensor(-28521.0371, grad_fn=<AddBackward0>) tensor(0.8651, grad_fn=<MulBackward0>)\n",
            "tensor(-28523.3613, grad_fn=<AddBackward0>) tensor(1.1127, grad_fn=<MulBackward0>)\n",
            "tensor(-28526.1016, grad_fn=<AddBackward0>) tensor(0.9446, grad_fn=<MulBackward0>)\n",
            "tensor(-28529.1094, grad_fn=<AddBackward0>) tensor(0.5123, grad_fn=<MulBackward0>)\n",
            "tensor(-28532.0762, grad_fn=<AddBackward0>) tensor(0.1235, grad_fn=<MulBackward0>)\n",
            "tensor(-28534.7910, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28537.2578, grad_fn=<AddBackward0>) tensor(0.1432, grad_fn=<MulBackward0>)\n",
            "tensor(-28539.6641, grad_fn=<AddBackward0>) tensor(0.3644, grad_fn=<MulBackward0>)\n",
            "tensor(-28542.2227, grad_fn=<AddBackward0>) tensor(0.4636, grad_fn=<MulBackward0>)\n",
            "tensor(-28544.9941, grad_fn=<AddBackward0>) tensor(0.3697, grad_fn=<MulBackward0>)\n",
            "tensor(-28547.8945, grad_fn=<AddBackward0>) tensor(0.1712, grad_fn=<MulBackward0>)\n",
            "tensor(-28550.7656, grad_fn=<AddBackward0>) tensor(0.0217, grad_fn=<MulBackward0>)\n",
            "tensor(-28553.5020, grad_fn=<AddBackward0>) tensor(0.0169, grad_fn=<MulBackward0>)\n",
            "tensor(-28556.1348, grad_fn=<AddBackward0>) tensor(0.1311, grad_fn=<MulBackward0>)\n",
            "tensor(-28558.7598, grad_fn=<AddBackward0>) tensor(0.2549, grad_fn=<MulBackward0>)\n",
            "tensor(-28561.4844, grad_fn=<AddBackward0>) tensor(0.2891, grad_fn=<MulBackward0>)\n",
            "tensor(-28564.3223, grad_fn=<AddBackward0>) tensor(0.2151, grad_fn=<MulBackward0>)\n",
            "tensor(-28567.2109, grad_fn=<AddBackward0>) tensor(0.0948, grad_fn=<MulBackward0>)\n",
            "tensor(-28570.0781, grad_fn=<AddBackward0>) tensor(0.0122, grad_fn=<MulBackward0>)\n",
            "tensor(-28572.8770, grad_fn=<AddBackward0>) tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "tensor(-28575.6406, grad_fn=<AddBackward0>) tensor(0.0551, grad_fn=<MulBackward0>)\n",
            "tensor(-28578.4258, grad_fn=<AddBackward0>) tensor(0.0986, grad_fn=<MulBackward0>)\n",
            "tensor(-28581.2734, grad_fn=<AddBackward0>) tensor(0.0973, grad_fn=<MulBackward0>)\n",
            "tensor(-28584.1797, grad_fn=<AddBackward0>) tensor(0.0557, grad_fn=<MulBackward0>)\n",
            "tensor(-28587.1074, grad_fn=<AddBackward0>) tensor(0.0120, grad_fn=<MulBackward0>)\n",
            "tensor(-28590.0137, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28592.8965, grad_fn=<AddBackward0>) tensor(0.0278, grad_fn=<MulBackward0>)\n",
            "tensor(-28595.7754, grad_fn=<AddBackward0>) tensor(0.0659, grad_fn=<MulBackward0>)\n",
            "tensor(-28598.6875, grad_fn=<AddBackward0>) tensor(0.0841, grad_fn=<MulBackward0>)\n",
            "tensor(-28601.6387, grad_fn=<AddBackward0>) tensor(0.0700, grad_fn=<MulBackward0>)\n",
            "tensor(-28604.6152, grad_fn=<AddBackward0>) tensor(0.0363, grad_fn=<MulBackward0>)\n",
            "tensor(-28607.6035, grad_fn=<AddBackward0>) tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "tensor(-28610.5801, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28613.5508, grad_fn=<AddBackward0>) tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "tensor(-28616.5410, grad_fn=<AddBackward0>) tensor(0.0199, grad_fn=<MulBackward0>)\n",
            "tensor(-28619.5547, grad_fn=<AddBackward0>) tensor(0.0204, grad_fn=<MulBackward0>)\n",
            "tensor(-28622.5918, grad_fn=<AddBackward0>) tensor(0.0109, grad_fn=<MulBackward0>)\n",
            "tensor(-28625.6426, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28628.7031, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28631.7598, grad_fn=<AddBackward0>) tensor(0.0114, grad_fn=<MulBackward0>)\n",
            "tensor(-28634.8301, grad_fn=<AddBackward0>) tensor(0.0231, grad_fn=<MulBackward0>)\n",
            "tensor(-28637.9160, grad_fn=<AddBackward0>) tensor(0.0276, grad_fn=<MulBackward0>)\n",
            "tensor(-28641.0234, grad_fn=<AddBackward0>) tensor(0.0221, grad_fn=<MulBackward0>)\n",
            "tensor(-28644.1426, grad_fn=<AddBackward0>) tensor(0.0113, grad_fn=<MulBackward0>)\n",
            "tensor(-28647.2734, grad_fn=<AddBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n",
            "tensor(-28650.4121, grad_fn=<AddBackward0>) tensor(1.0510e-06, grad_fn=<MulBackward0>)\n",
            "tensor(-28653.5566, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28656.7168, grad_fn=<AddBackward0>) tensor(0.0035, grad_fn=<MulBackward0>)\n",
            "tensor(-28659.8926, grad_fn=<AddBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n",
            "tensor(-28663.0840, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28666.2852, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28669.4980, grad_fn=<AddBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n",
            "tensor(-28672.7188, grad_fn=<AddBackward0>) tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "tensor(-28675.9531, grad_fn=<AddBackward0>) tensor(0.0103, grad_fn=<MulBackward0>)\n",
            "tensor(-28679.2012, grad_fn=<AddBackward0>) tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "tensor(-28682.4609, grad_fn=<AddBackward0>) tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "tensor(-28685.7344, grad_fn=<AddBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n",
            "tensor(-28689.0195, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28692.3125, grad_fn=<AddBackward0>) tensor(1.1787e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28695.6191, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28698.9375, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28702.2695, grad_fn=<AddBackward0>) tensor(1.3481e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28705.6133, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28708.9668, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28712.3340, grad_fn=<AddBackward0>) tensor(0.0031, grad_fn=<MulBackward0>)\n",
            "tensor(-28715.7109, grad_fn=<AddBackward0>) tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "tensor(-28719.0996, grad_fn=<AddBackward0>) tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "tensor(-28722.5020, grad_fn=<AddBackward0>) tensor(0.0042, grad_fn=<MulBackward0>)\n",
            "tensor(-28725.9160, grad_fn=<AddBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n",
            "tensor(-28729.3418, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28732.7793, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28736.2246, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28739.6855, grad_fn=<AddBackward0>) tensor(7.4079e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28743.1562, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28746.6406, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28750.1328, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28753.6387, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28757.1562, grad_fn=<AddBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n",
            "tensor(-28760.6836, grad_fn=<AddBackward0>) tensor(0.0030, grad_fn=<MulBackward0>)\n",
            "tensor(-28764.2266, grad_fn=<AddBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n",
            "tensor(-28767.7773, grad_fn=<AddBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n",
            "tensor(-28771.3398, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28774.9141, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28778.5020, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28782.0977, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28785.7090, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-28789.3281, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28792.9551, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28796.6016, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28800.2559, grad_fn=<AddBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n",
            "tensor(-28803.9199, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28807.5957, grad_fn=<AddBackward0>) tensor(0.0018, grad_fn=<MulBackward0>)\n",
            "tensor(-28811.2812, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28814.9824, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28818.6914, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28822.4141, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28826.1465, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28829.8906, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28833.6426, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28837.4082, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28841.1875, grad_fn=<AddBackward0>) tensor(0.0016, grad_fn=<MulBackward0>)\n",
            "tensor(-28844.9746, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28848.7734, grad_fn=<AddBackward0>) tensor(0.0016, grad_fn=<MulBackward0>)\n",
            "tensor(-28852.5820, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28856.4023, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28860.2324, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28864.0801, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28867.9355, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28871.7969, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28875.6738, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28879.5605, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28883.4570, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28887.3652, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28891.2852, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28895.2148, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28899.1582, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28903.1094, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28907.0723, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28911.0469, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28915.0332, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28919.0273, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28923.0332, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28927.0488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28931.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28935.1172, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28939.1680, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28943.2285, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28947.2988, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28951.3828, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28955.4727, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28959.5762, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28963.6914, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28967.8145, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28971.9492, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28976.0977, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28980.2539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28984.4199, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28988.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28992.7852, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28996.9863, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29001.1953, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29005.4160, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29009.6465, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29013.8887, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29018.1387, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29022.4023, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29026.6758, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29030.9609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29035.2559, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29039.5586, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29043.8750, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29048.2012, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29052.5371, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29056.8828, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29061.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29065.6113, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29069.9863, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29074.3770, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29078.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29083.1836, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29087.6055, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29092.0371, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29096.4766, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29100.9277, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29105.3887, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29109.8633, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29114.3438, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29118.8379, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29123.3398, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29127.8535, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29132.3789, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29136.9121, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29141.4590, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29146.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29150.5762, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29155.1523, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29159.7402, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29164.3359, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29168.9434, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29173.5605, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29178.1895, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29182.8262, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29187.4746, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29192.1328, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29196.8008, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29201.4785, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29206.1680, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29210.8672, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29215.5781, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29220.2969, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29225.0293, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29229.7676, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29234.5195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29239.2793, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29244.0508, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29248.8320, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29253.6250, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29258.4258, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29263.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29268.0586, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29272.8926, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29277.7344, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29282.5879, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29287.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29292.3262, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29297.2090, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29302.1035, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29307.0059, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29311.9199, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29316.8438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29321.7793, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29326.7227, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29331.6758, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29336.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29341.6172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29346.6035, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29351.5957, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29356.6035, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29361.6191, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29366.6445, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29371.6777, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29376.7266, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29381.7812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29386.8496, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29391.9238, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29397.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29402.1074, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29407.2148, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29412.3320, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29417.4590, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29422.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29427.7422, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29432.8984, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29438.0684, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29443.2441, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29448.4336, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29453.6289, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29458.8379, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29464.0547, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29469.2812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29474.5195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29479.7695, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29485.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29490.2949, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29495.5723, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29500.8613, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29506.1602, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29511.4688, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29516.7871, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29522.1172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29527.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29532.8027, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29538.1602, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29543.5312, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29548.9082, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29554.2988, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29559.6973, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29565.1074, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29570.5254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29575.9551, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29581.3945, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29586.8438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29592.3027, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29597.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29603.2520, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29608.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29614.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29619.7520, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29625.2695, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29630.8008, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29636.3398, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29641.8906, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29647.4492, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29653.0176, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29658.5996, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29664.1895, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29669.7891, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29675.3984, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29681.0195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29686.6504, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29692.2910, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29697.9395, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29703.5996, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29709.2715, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29714.9512, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29720.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29726.3418, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29732.0527, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29737.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29743.5020, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29749.2441, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29754.9941, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29760.7539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29766.5254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29772.3066, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29778.0977, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29783.8965, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29789.7090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29795.5293, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29801.3613, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29807.2031, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29813.0527, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29818.9141, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29824.7852, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29830.6660, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29836.5566, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29842.4590, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29848.3711, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29854.2930, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29860.2227, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29866.1641, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29872.1172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29878.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29884.0488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29890.0332, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29896.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29902.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29908.0352, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29914.0586, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29920.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29926.1328, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29932.1855, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29938.2461, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29944.3203, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29950.4004, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29956.4941, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29962.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29968.7090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29974.8340, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29980.9648, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29987.1094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29993.2617, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29999.4238, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30005.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30011.7773, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30017.9746, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30024.1777, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30030.3906, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30036.6133, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30042.8477, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30049.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30055.3438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30061.6094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30067.8828, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30074.1680, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30080.4609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30086.7656, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30093.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30099.4043, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30105.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30112.0859, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30118.4395, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30124.8047, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30131.1797, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30137.5645, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30143.9609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30150.3672, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30156.7812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30163.2090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30169.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30176.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30182.5469, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30189.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30195.4902, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30201.9746, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30208.4707, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30214.9805, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30221.4980, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30228.0234, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30234.5625, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30241.1094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30247.6699, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30254.2363, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30260.8145, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30267.4043, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30274., grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30280.6094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30287.2285, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30293.8574, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30300.4980, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30307.1445, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30313.8047, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30320.4766, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30327.1562, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30333.8477, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30340.5488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30347.2598, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30353.9805, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30360.7109, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30367.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30374.2031, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30380.9648, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30387.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30394.5215, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30401.3125, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30408.1152, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30414.9316, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30421.7539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30428.5859, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30435.4316, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30442.2852, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30449.1484, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30456.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30462.9102, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "Optimized alphas: tensor([0.5160, 0.5141, 0.5133, 0.5204, 0.5213, 0.5208, 0.5154, 0.5213, 0.5215,\n",
            "        0.5161, 0.5151, 0.5134, 0.5227, 0.5211, 0.5144, 0.5215, 0.5210, 0.5137,\n",
            "        0.5153, 0.5216, 0.5158, 0.5211, 0.5156, 0.5187, 0.5153, 0.5151, 0.5213,\n",
            "        0.5125, 0.5151, 0.5221, 0.5214, 0.5207, 0.5125, 0.5215, 0.5152, 0.5210,\n",
            "        0.5213, 0.5138, 0.5207, 0.5155, 0.5216, 0.5159, 0.5216, 0.5216, 0.5216,\n",
            "        0.5205, 0.5156, 0.5216, 0.5159, 0.5216, 0.5203, 0.5161, 0.5215, 0.5163,\n",
            "        0.5214, 0.5216, 0.5216, 0.5199, 0.5157, 0.5212, 0.5211, 0.5161, 0.5210,\n",
            "        0.5206, 0.5109, 0.5190, 0.5142, 0.5152, 0.5169, 0.5148, 0.5158, 0.5158,\n",
            "        0.5211, 0.5143, 0.5160, 0.5213, 0.5129, 0.5159, 0.5210, 0.5201, 0.5161,\n",
            "        0.5210, 0.5216, 0.5209, 0.5214, 0.5160, 0.5140, 0.5213, 0.5212, 0.5216,\n",
            "        0.5208, 0.5159, 0.5191, 0.5148, 0.5212, 0.5138, 0.5148, 0.5215, 0.5152,\n",
            "        0.5211, 0.5211, 0.5153, 0.5216, 0.5216, 0.5213, 0.5159, 0.5159, 0.5216,\n",
            "        0.5189, 0.5158, 0.5216, 0.5152, 0.5191, 0.5210, 0.5211, 0.5214, 0.5206,\n",
            "        0.5215, 0.5214, 0.5215, 0.5157, 0.5216, 0.5216, 0.5128, 0.5151, 0.5160,\n",
            "        0.5213, 0.5159, 0.5158, 0.5214, 0.5213, 0.5157, 0.5223, 0.5206, 0.5156,\n",
            "        0.5160, 0.5152, 0.5206, 0.5158, 0.5139, 0.5204, 0.5219, 0.5159, 0.5188,\n",
            "        0.5149, 0.5210, 0.5153, 0.5159, 0.5160, 0.5133, 0.5153, 0.5212, 0.5227,\n",
            "        0.5216, 0.5155, 0.5150, 0.5209, 0.5126, 0.5145, 0.5159, 0.5225, 0.5208,\n",
            "        0.5147, 0.5218, 0.5211, 0.5199, 0.5209, 0.5159, 0.5147, 0.5152, 0.5211,\n",
            "        0.5209, 0.5096, 0.5159, 0.5140, 0.5151, 0.5144, 0.5161, 0.5161, 0.5211,\n",
            "        0.5126, 0.5154, 0.5159, 0.5158, 0.5216, 0.5158, 0.5206, 0.5159, 0.5159,\n",
            "        0.5195, 0.5152, 0.5216, 0.5154, 0.5159, 0.5157, 0.5154, 0.5160, 0.5213,\n",
            "        0.5216, 0.5216, 0.5210, 0.5225, 0.5155, 0.5218, 0.5161, 0.5218, 0.5216,\n",
            "        0.5149, 0.5215, 0.5160, 0.5214, 0.5160, 0.5216, 0.5089, 0.5209, 0.5214,\n",
            "        0.5216, 0.5201, 0.5160, 0.5156, 0.5161, 0.5156, 0.5193, 0.5215, 0.5161,\n",
            "        0.5216, 0.5213, 0.5212, 0.5159, 0.5215, 0.5155, 0.5145, 0.5159, 0.5214,\n",
            "        0.5140, 0.5160, 0.5216, 0.5136, 0.5214, 0.5161, 0.5142, 0.5215, 0.5160,\n",
            "        0.5154, 0.5151, 0.5213, 0.5216, 0.5217, 0.5147, 0.5150, 0.5213, 0.5148,\n",
            "        0.5131, 0.5215, 0.5153, 0.5206, 0.5138, 0.5216, 0.5203, 0.5160, 0.5217,\n",
            "        0.5158, 0.5210, 0.5197, 0.5216, 0.5216, 0.5144, 0.5215, 0.5216, 0.5135,\n",
            "        0.5114, 0.5207, 0.5195, 0.5215, 0.5115, 0.5212, 0.5156, 0.5159, 0.5162,\n",
            "        0.5160, 0.5148, 0.5205, 0.5141, 0.5160, 0.5213, 0.5158, 0.5210, 0.5144,\n",
            "        0.5215, 0.5217, 0.5160, 0.5156, 0.5154, 0.5161, 0.5160, 0.5215, 0.5216,\n",
            "        0.5210, 0.5157, 0.5152, 0.5160, 0.5216, 0.5158, 0.5201, 0.5137, 0.5215,\n",
            "        0.5213, 0.5148, 0.5161, 0.5160, 0.5161, 0.5157, 0.5158, 0.5207, 0.5154,\n",
            "        0.5181, 0.5160, 0.5126, 0.5134, 0.5160, 0.5214, 0.5214, 0.5209, 0.5159,\n",
            "        0.5216, 0.5159, 0.5215, 0.5143, 0.5147, 0.5159, 0.5132, 0.5154, 0.5162,\n",
            "        0.5152, 0.5215, 0.5215, 0.5142, 0.5209, 0.5157, 0.5160, 0.5158, 0.5159,\n",
            "        0.5215, 0.5209, 0.5151, 0.5206, 0.5214, 0.5216, 0.5213, 0.5216, 0.5215,\n",
            "        0.5212, 0.5211, 0.5213, 0.5152, 0.5159, 0.5160, 0.5162, 0.5207, 0.5214,\n",
            "        0.5161, 0.5153, 0.5157, 0.5220, 0.5158, 0.5227, 0.5159, 0.5158, 0.5215,\n",
            "        0.5233, 0.5157, 0.5156, 0.5161, 0.5133, 0.5157, 0.5159, 0.5158, 0.5151,\n",
            "        0.5215, 0.5160, 0.5132, 0.5152, 0.5151, 0.5211, 0.5155, 0.5158, 0.5213,\n",
            "        0.5152, 0.5201, 0.5220, 0.5215, 0.5199, 0.5161, 0.5215, 0.5216, 0.5151,\n",
            "        0.5213, 0.5138, 0.5148, 0.5230, 0.5140, 0.5160, 0.5146, 0.5213, 0.5160,\n",
            "        0.5217, 0.5151, 0.5161, 0.5158, 0.5213, 0.5209, 0.5159, 0.5186, 0.5203,\n",
            "        0.5209, 0.5210, 0.5137, 0.5157, 0.5216, 0.5219, 0.5216, 0.5156, 0.5117,\n",
            "        0.5217, 0.5157, 0.5216, 0.5206, 0.5132, 0.5149, 0.5205, 0.5148, 0.5212,\n",
            "        0.5160, 0.5216, 0.5160, 0.5212, 0.5215, 0.5152, 0.5215, 0.5158, 0.5211,\n",
            "        0.5216, 0.5150, 0.5152, 0.5159, 0.5209, 0.5206, 0.5136, 0.5159, 0.5161,\n",
            "        0.5156, 0.5158, 0.5214, 0.5218, 0.5212, 0.5216, 0.5216, 0.5215, 0.5203,\n",
            "        0.5159, 0.5216, 0.5132, 0.5212, 0.5211, 0.5142, 0.5181, 0.5169, 0.5160,\n",
            "        0.5130, 0.5216, 0.5143, 0.5148, 0.5156, 0.5214, 0.5214, 0.5158, 0.5197,\n",
            "        0.5159, 0.5214, 0.5163, 0.5122, 0.5216, 0.5160, 0.5118, 0.5156, 0.5207,\n",
            "        0.5215, 0.5133, 0.5140, 0.5155, 0.5216, 0.5151, 0.5155, 0.5215, 0.5157,\n",
            "        0.5160, 0.5140, 0.5218, 0.5148, 0.5231, 0.5197, 0.5153, 0.5213, 0.5129,\n",
            "        0.5226, 0.5125, 0.5156, 0.5159, 0.5130, 0.5133, 0.5211, 0.5211, 0.5215,\n",
            "        0.5216, 0.5207, 0.5232, 0.5119, 0.5143, 0.5156, 0.5154, 0.5207, 0.5214,\n",
            "        0.5163, 0.5214, 0.5157, 0.5159, 0.5219, 0.5197, 0.5210, 0.5213, 0.5216,\n",
            "        0.5203, 0.5151, 0.5162, 0.5216, 0.5139, 0.5160, 0.5160, 0.5158, 0.5149,\n",
            "        0.5208, 0.5160, 0.5212, 0.5214, 0.5215, 0.5215, 0.5148, 0.5156, 0.5215,\n",
            "        0.5158, 0.5154, 0.5217, 0.5213, 0.5216, 0.5212, 0.5214, 0.5209, 0.5215,\n",
            "        0.5208, 0.5212, 0.5203, 0.5155, 0.5215, 0.5101, 0.5201, 0.5206, 0.5153,\n",
            "        0.5162, 0.5215, 0.5148, 0.5160, 0.5206, 0.5215, 0.5154, 0.5214, 0.5214,\n",
            "        0.5214, 0.5205, 0.5210, 0.5156, 0.5160, 0.5182, 0.5156, 0.5139, 0.5160,\n",
            "        0.5158, 0.5208, 0.5143, 0.5141, 0.5158, 0.5134, 0.5216, 0.5215, 0.5157,\n",
            "        0.5172, 0.5213, 0.5154, 0.5218, 0.5158, 0.5149, 0.5213, 0.5156, 0.5157,\n",
            "        0.5213, 0.5143, 0.5160, 0.5212, 0.5160, 0.5212, 0.5161, 0.5207, 0.5212,\n",
            "        0.5140, 0.5161, 0.5209, 0.5201, 0.5195, 0.5211, 0.5158, 0.5215, 0.5216,\n",
            "        0.5216, 0.5206, 0.5160, 0.5158, 0.5152, 0.5216, 0.5148, 0.5145, 0.5161,\n",
            "        0.5215, 0.5151, 0.5152, 0.5152, 0.5158, 0.5160, 0.5216, 0.5213, 0.5160,\n",
            "        0.5154, 0.5154, 0.5216, 0.5160, 0.5159, 0.5215, 0.5161, 0.5210, 0.5215,\n",
            "        0.5216, 0.5206, 0.5156, 0.5215, 0.5153, 0.5214, 0.5160, 0.5156, 0.5160,\n",
            "        0.5216, 0.5161, 0.5235, 0.5154, 0.5196, 0.5158, 0.5216, 0.5156, 0.5137,\n",
            "        0.5209, 0.5147, 0.5219, 0.5214, 0.5216, 0.5216, 0.5160, 0.5144, 0.5216,\n",
            "        0.5205, 0.5183, 0.5197, 0.5159, 0.5141, 0.5212, 0.5149, 0.5211, 0.5160,\n",
            "        0.5212, 0.5149, 0.5145, 0.5144, 0.5218, 0.5216, 0.5211, 0.5155, 0.5159,\n",
            "        0.5163, 0.5211, 0.5205, 0.5216, 0.5161, 0.5161, 0.5214])\n"
          ]
        }
      ],
      "source": [
        "labels = torch.tensor(y_train).float()\n",
        "labels = 2*labels-1\n",
        "alpha = torch.tensor([0.5]*num_data,requires_grad=True)\n",
        "optimizer = torch.optim.Adam([alpha], lr=0.001)\n",
        "def objective_function(alpha, kernel_matrix, labels):\n",
        "    \"\"\"SVM의 쌍대 목적 함수\"\"\"\n",
        "    L = 0.5 * torch.dot(alpha, torch.mv(kernel_matrix, alpha)) - torch.sum(alpha)\n",
        "    # 제약 조건을 유지하기 위해 레이블과 alpha의 곱의 합은 0이어야 합니다.\n",
        "    constraint = torch.dot(alpha, labels)\n",
        "    loss = -L + 1e4 * constraint ** 2\n",
        "    print(loss,1e4 * constraint ** 2)\n",
        "    return loss  # 제약조건에 큰 페널티를 적용\n",
        "\n",
        "# 훈련 과정\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective_function(alpha, kernel_matrix, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    alpha.data.clamp_(0)  # alpha는 0 이상이어야 함\n",
        "\n",
        "print(\"Optimized alphas:\", alpha.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm6T_J2-kgx4",
        "outputId": "e148967c-146a-4fc4-d020-e30e23df3246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [02:38<00:00,  4.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: tensor([-1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1.,  1.], grad_fn=<SignBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pennylane as qml\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 테스트 데이터와 훈련 데이터 간의 양자 커널 행렬 계산\n",
        "x_test = torch.tensor(x_test).float()\n",
        "num_test = x_test.size(0)\n",
        "test_kernel_matrix = torch.zeros((num_test, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_test)\n",
        "    output = feature_model([data,x_test])\n",
        "    test_kernel_matrix[:,i] = output.detach().cpu()\n",
        "\n",
        "# 훈련된 모델을 사용하여 테스트 데이터의 클래스 예측\n",
        "predictions = torch.sign(torch.mv(test_kernel_matrix, alpha * labels))\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tqKJfT74pDjk"
      },
      "outputs": [],
      "source": [
        "predictions = (predictions+1)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUkNq4ZhpAyC",
        "outputId": "07814bee-5a1d-4762-ec20-1e5058ef5731"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9366666674613953"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy(predictions,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
