{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP7p3x26kiZ2",
        "outputId": "ffe9c30b-83e8-4e63-9d7b-8837392664ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n",
            "Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.14.2)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.10.0)\n",
            "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.6.9)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
            "Requirement already satisfied: pennylane-lightning>=0.36 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.36.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.11.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
            "Requirement already satisfied: pykan in /usr/local/lib/python3.10/dist-packages (0.0.5)\n",
            "Cloning into 'Quantum_machine'...\n",
            "remote: Enumerating objects: 481, done.\u001b[K\n",
            "remote: Counting objects: 100% (481/481), done.\u001b[K\n",
            "remote: Compressing objects: 100% (469/469), done.\u001b[K\n",
            "remote: Total 481 (delta 26), reused 457 (delta 9), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (481/481), 8.95 MiB | 26.88 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/Quantum_machine/Quantum_machine\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane\n",
        "!pip install pykan\n",
        "!git clone https://github.com/pop756/Quantum_machine.git\n",
        "%cd Quantum_machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vJl4rYpKBAEk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import sys\n",
        "import copy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 0.01\n",
        "PCA_dim = 8\n",
        "CLS_num = 2\n",
        "\n",
        "\n",
        "\n",
        "with open('./data.pkl','rb') as file:\n",
        "    data = pickle.load(file)\n",
        "X = data['X']\n",
        "y = data['Y']\n",
        "\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "def Fit_to_quantum(X,PCA_dim):\n",
        "    pca = PCA(n_components=PCA_dim)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    return X_pca\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PyTorch Tensor로 변환\n",
        "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
        "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "class Feature_data_loader(Dataset):\n",
        "    def __init__(self,x_train,y_train):\n",
        "        self.feature1 = x_train\n",
        "        temp = copy.deepcopy(x_train)\n",
        "        shuffle = torch.randperm(len(temp))\n",
        "        self.feature2 = temp[shuffle]\n",
        "        self.y1 = y_train\n",
        "        temp_y = copy.deepcopy(y_train)\n",
        "        self.y2 = temp_y[shuffle]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.feature1)\n",
        "    def __getitem__(self,idx):\n",
        "        input1 = self.feature1[idx]\n",
        "        input2 = self.feature2[idx]\n",
        "        if self.y1[idx] == self.y2[idx]:\n",
        "            label = torch.tensor(1.).float()\n",
        "        else:\n",
        "            label = torch.tensor(0.).float()\n",
        "        return [input1,input2],label\n",
        "\n",
        "\n",
        "# DataLoader 생성\n",
        "\n",
        "\n",
        "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
        "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
        "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N4Y4dnYJBAEt"
      },
      "outputs": [],
      "source": [
        "def accuracy(pred, true):\n",
        "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
        "    pred = pred.detach().cpu()\n",
        "    true = true.cpu()\n",
        "    try:\n",
        "        pred_labels = torch.argmax(pred, dim=1)\n",
        "    except:\n",
        "        pred_labels = torch.round(pred)\n",
        "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
        "    correct = (pred_labels == true).sum()\n",
        "    # 정확도를 계산\n",
        "    acc = correct / true.size(0)\n",
        "    return acc.item()\n",
        "\n",
        "class Early_stop_train():\n",
        "    def __init__(self,model, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "\n",
        "\n",
        "\n",
        "        self.loss_list = [1e100]\n",
        "        self.stop_count = 0\n",
        "\n",
        "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10):\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            if self.stop_count>=res:\n",
        "                break\n",
        "            loss_val,_ = self.test(test_loader)\n",
        "            self.loss_list.append(loss_val)\n",
        "\n",
        "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
        "                self.stop_count+=1\n",
        "            else:\n",
        "                self.stop_count = 0\n",
        "            loss_list = []\n",
        "            acc_list = []\n",
        "            for X_train,y_train in train_loader:\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(X_train)\n",
        "\n",
        "                loss = self.criterion(output.squeeze(), y_train)\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                loss_list.append(loss.item())\n",
        "                acc = accuracy(output,y_train)\n",
        "                acc_list.append(acc)\n",
        "\n",
        "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} stop count : {self.stop_count}\")\n",
        "\n",
        "\n",
        "    def test(self,test_loader):\n",
        "        if test_loader is None:\n",
        "            return 0,0\n",
        "        else:\n",
        "            #self.model.eval()\n",
        "            test_loss = 0\n",
        "            correct = 0\n",
        "            with torch.no_grad():\n",
        "                for data, target in test_loader:\n",
        "                    data, target = data, target\n",
        "                    output = self.model(data)\n",
        "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
        "\n",
        "                    correct += accuracy(output,target)*len(output)\n",
        "\n",
        "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "            return test_loss,correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k3d3GfMBAEv",
        "outputId": "dce7ef57-3f8c-4efd-a9f9-b7b100cc40b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 12.3056, Accuracy: 153.99999904632568/300 (51%)\n",
            "Epoch 1 Loss 1.710618 acc : 0.491477 stop count : 0\n",
            "Test set: Average loss: 4.6124, Accuracy: 157.99999952316284/300 (53%)\n",
            "Epoch 2 Loss 0.849652 acc : 0.566098 stop count : 0\n",
            "Test set: Average loss: 4.0512, Accuracy: 187.99999952316284/300 (63%)\n",
            "Epoch 3 Loss 0.747656 acc : 0.602841 stop count : 0\n",
            "Test set: Average loss: 3.2585, Accuracy: 198.0000011920929/300 (66%)\n",
            "Epoch 4 Loss 0.632870 acc : 0.662879 stop count : 0\n",
            "Test set: Average loss: 2.8490, Accuracy: 223.00000071525574/300 (74%)\n",
            "Epoch 5 Loss 0.562981 acc : 0.708712 stop count : 0\n",
            "Test set: Average loss: 2.4969, Accuracy: 230.00000071525574/300 (77%)\n",
            "Epoch 6 Loss 0.507302 acc : 0.754545 stop count : 0\n",
            "Test set: Average loss: 2.2532, Accuracy: 242.00000071525574/300 (81%)\n",
            "Epoch 7 Loss 0.473032 acc : 0.780682 stop count : 0\n",
            "Test set: Average loss: 2.1640, Accuracy: 249.99999976158142/300 (83%)\n",
            "Epoch 8 Loss 0.446762 acc : 0.797254 stop count : 0\n",
            "Test set: Average loss: 2.1647, Accuracy: 251.00000047683716/300 (84%)\n",
            "Epoch 9 Loss 0.422312 acc : 0.816951 stop count : 1\n",
            "Test set: Average loss: 2.0214, Accuracy: 254.0000011920929/300 (85%)\n",
            "Epoch 10 Loss 0.401178 acc : 0.822633 stop count : 0\n",
            "Test set: Average loss: 2.0249, Accuracy: 253.99999952316284/300 (85%)\n",
            "Epoch 11 Loss 0.392974 acc : 0.823295 stop count : 1\n",
            "Test set: Average loss: 1.9921, Accuracy: 254.99999952316284/300 (85%)\n",
            "Epoch 12 Loss 0.377513 acc : 0.833996 stop count : 0\n",
            "Test set: Average loss: 1.9193, Accuracy: 255.0000011920929/300 (85%)\n",
            "Epoch 13 Loss 0.356458 acc : 0.837311 stop count : 0\n",
            "Test set: Average loss: 1.9689, Accuracy: 253.99999952316284/300 (85%)\n",
            "Epoch 14 Loss 0.358165 acc : 0.841193 stop count : 1\n",
            "Test set: Average loss: 1.9766, Accuracy: 255.0000011920929/300 (85%)\n",
            "Epoch 15 Loss 0.341497 acc : 0.845644 stop count : 2\n",
            "Test set: Average loss: 1.8822, Accuracy: 256.99999952316284/300 (86%)\n",
            "Epoch 16 Loss 0.333183 acc : 0.860133 stop count : 0\n",
            "Test set: Average loss: 1.8459, Accuracy: 260.0000011920929/300 (87%)\n",
            "Epoch 17 Loss 0.338314 acc : 0.860038 stop count : 0\n",
            "Test set: Average loss: 2.0581, Accuracy: 259.0000011920929/300 (86%)\n",
            "Epoch 18 Loss 0.318921 acc : 0.861553 stop count : 1\n",
            "Test set: Average loss: 1.9378, Accuracy: 258.99999952316284/300 (86%)\n",
            "Epoch 19 Loss 0.316578 acc : 0.865341 stop count : 2\n",
            "Test set: Average loss: 1.8613, Accuracy: 260.0000011920929/300 (87%)\n",
            "Epoch 20 Loss 0.307561 acc : 0.876799 stop count : 3\n",
            "Test set: Average loss: 1.8816, Accuracy: 259.0000011920929/300 (86%)\n",
            "Epoch 21 Loss 0.302754 acc : 0.875473 stop count : 4\n",
            "Test set: Average loss: 1.9500, Accuracy: 259.99999952316284/300 (87%)\n",
            "Epoch 22 Loss 0.292175 acc : 0.874148 stop count : 5\n",
            "Test set: Average loss: 1.9011, Accuracy: 259.0000011920929/300 (86%)\n",
            "Epoch 23 Loss 0.283747 acc : 0.883144 stop count : 6\n",
            "Test set: Average loss: 1.8457, Accuracy: 259.0000011920929/300 (86%)\n",
            "Epoch 24 Loss 0.285441 acc : 0.888731 stop count : 0\n",
            "Test set: Average loss: 1.9419, Accuracy: 262.0000011920929/300 (87%)\n",
            "Epoch 25 Loss 0.268821 acc : 0.895739 stop count : 1\n",
            "Test set: Average loss: 1.8994, Accuracy: 260.0000011920929/300 (87%)\n",
            "Epoch 26 Loss 0.270317 acc : 0.890152 stop count : 2\n",
            "Test set: Average loss: 1.8313, Accuracy: 263.0000011920929/300 (88%)\n",
            "Epoch 27 Loss 0.261427 acc : 0.895739 stop count : 0\n",
            "Test set: Average loss: 1.9771, Accuracy: 254.00000047683716/300 (85%)\n",
            "Epoch 28 Loss 0.275813 acc : 0.895644 stop count : 1\n",
            "Test set: Average loss: 1.8198, Accuracy: 265.0000011920929/300 (88%)\n",
            "Epoch 29 Loss 0.272722 acc : 0.885606 stop count : 0\n",
            "Test set: Average loss: 2.0236, Accuracy: 253.99999952316284/300 (85%)\n",
            "Epoch 30 Loss 0.252204 acc : 0.894413 stop count : 1\n",
            "Test set: Average loss: 1.8443, Accuracy: 260.99999952316284/300 (87%)\n",
            "Epoch 31 Loss 0.245984 acc : 0.902841 stop count : 2\n",
            "Test set: Average loss: 1.8900, Accuracy: 260.99999952316284/300 (87%)\n",
            "Epoch 32 Loss 0.239371 acc : 0.899811 stop count : 3\n",
            "Test set: Average loss: 1.8233, Accuracy: 263.99999952316284/300 (88%)\n",
            "Epoch 33 Loss 0.232927 acc : 0.915909 stop count : 4\n",
            "Test set: Average loss: 1.8910, Accuracy: 262.0000011920929/300 (87%)\n",
            "Epoch 34 Loss 0.242573 acc : 0.895739 stop count : 5\n",
            "Test set: Average loss: 1.9074, Accuracy: 263.0000011920929/300 (88%)\n",
            "Epoch 35 Loss 0.237067 acc : 0.903977 stop count : 6\n",
            "Test set: Average loss: 1.9037, Accuracy: 256.99999952316284/300 (86%)\n",
            "Epoch 36 Loss 0.254069 acc : 0.899811 stop count : 7\n",
            "Test set: Average loss: 2.1071, Accuracy: 250.99999976158142/300 (84%)\n",
            "Epoch 37 Loss 0.284790 acc : 0.881345 stop count : 8\n",
            "Test set: Average loss: 1.7557, Accuracy: 260.99999952316284/300 (87%)\n",
            "Epoch 38 Loss 0.247705 acc : 0.897064 stop count : 0\n",
            "Test set: Average loss: 2.1542, Accuracy: 252.99999952316284/300 (84%)\n",
            "Epoch 39 Loss 0.253432 acc : 0.901705 stop count : 1\n",
            "Test set: Average loss: 1.7469, Accuracy: 262.99999952316284/300 (88%)\n",
            "Epoch 40 Loss 0.231631 acc : 0.908617 stop count : 0\n",
            "Test set: Average loss: 1.8252, Accuracy: 262.0000011920929/300 (87%)\n",
            "Epoch 41 Loss 0.213182 acc : 0.915720 stop count : 1\n",
            "Test set: Average loss: 1.9371, Accuracy: 259.99999952316284/300 (87%)\n",
            "Epoch 42 Loss 0.231470 acc : 0.905682 stop count : 2\n",
            "Test set: Average loss: 1.8325, Accuracy: 258.99999952316284/300 (86%)\n",
            "Epoch 43 Loss 0.204758 acc : 0.914110 stop count : 3\n",
            "Test set: Average loss: 1.8294, Accuracy: 264.0000011920929/300 (88%)\n",
            "Epoch 44 Loss 0.194899 acc : 0.918561 stop count : 4\n",
            "Test set: Average loss: 1.8193, Accuracy: 259.99999952316284/300 (87%)\n",
            "Epoch 45 Loss 0.200928 acc : 0.922917 stop count : 5\n",
            "Test set: Average loss: 1.9248, Accuracy: 258.99999952316284/300 (86%)\n",
            "Epoch 46 Loss 0.199140 acc : 0.924337 stop count : 6\n",
            "Test set: Average loss: 1.9197, Accuracy: 252.00000047683716/300 (84%)\n",
            "Epoch 47 Loss 0.201033 acc : 0.919602 stop count : 7\n",
            "Test set: Average loss: 1.8713, Accuracy: 261.99999952316284/300 (87%)\n",
            "Epoch 48 Loss 0.206551 acc : 0.918466 stop count : 8\n",
            "Test set: Average loss: 1.7913, Accuracy: 259.0000011920929/300 (86%)\n",
            "Epoch 49 Loss 0.191074 acc : 0.930019 stop count : 9\n",
            "Test set: Average loss: 2.0477, Accuracy: 257.00000047683716/300 (86%)\n",
            "Epoch 50 Loss 0.192237 acc : 0.924432 stop count : 10\n",
            "\n",
            " Test start \n",
            "\n",
            "\n",
            "\n",
            "Test set: Average loss: 3.1763, Accuracy: 277.99999928474426/300 (93%)\n",
            "Epoch 1 Loss 0.560054 acc : 0.948390 stop count : 0\n",
            "Test set: Average loss: 2.4776, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 2 Loss 0.432853 acc : 0.948485 stop count : 0\n",
            "Test set: Average loss: 1.9426, Accuracy: 280.0000002384186/300 (93%)\n",
            "Epoch 3 Loss 0.345934 acc : 0.958333 stop count : 0"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Test start \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    172\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(model, optimizer, criterion)\n\u001b[1;32m--> 173\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m _,acc \u001b[38;5;241m=\u001b[39m train_process\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[0;32m    176\u001b[0m result_list_classical\u001b[38;5;241m.\u001b[39mappend(acc)\n",
            "Cell \u001b[1;32mIn[7], line 47\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res)\u001b[0m\n\u001b[0;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(X_train)\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output\u001b[38;5;241m.\u001b[39msqueeze(), y_train)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     49\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "result_list_classical = []\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "\"\"\"\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=PCA_dim)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.PCA_dim, random_state=seed)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
        "\n",
        "# Pennylane 장치 설정\n",
        "dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
        "\n",
        "\n",
        "def ZZFeatureMapLayer(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ZZFeatureMapLayer_fixed(features, wires):\n",
        "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
        "    index = 0\n",
        "    for i in wires:\n",
        "        qml.Hadamard(wires=i)\n",
        "        qml.RZ(features[:,index], wires=i)\n",
        "        index += 1\n",
        "    index=0\n",
        "    for j in range(0, len(wires)-1):\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        qml.RZ((features[:,index])*(features[:,index+1]), wires=j+1)\n",
        "        qml.CNOT(wires=[j, j+1])\n",
        "        index+=1\n",
        "\n",
        "def ansatz(params):\n",
        "    for j in range(len(params)):\n",
        "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
        "        for i in range(len(params[0])):\n",
        "            qml.RY(params[j, i, 0], wires=i)\n",
        "            qml.RZ(params[j, i, 1], wires=i)\n",
        "\n",
        "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
        "        if j == len(params)-1:\n",
        "            pass\n",
        "        else:\n",
        "            for i in range(len(params[0])-1):\n",
        "                qml.CNOT(wires=[i, i+1])\n",
        "\n",
        "\n",
        "# 양자 레이어 정의\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def QuantumLayer(features,params):\n",
        "    ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
        "    ansatz(params)\n",
        "    return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
        "\n",
        "\n",
        "## 양자 커널\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal(features1,features2):\n",
        "    ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "@qml.qnode(dev, interface='torch', diff_method=\"backprop\")\n",
        "def Kernal_fix(features1,features2):\n",
        "    ZZFeatureMapLayer_fixed(features1, wires=range(PCA_dim))\n",
        "    qml.adjoint(ZZFeatureMapLayer_fixed)(features2,wires=range(PCA_dim))\n",
        "    return qml.probs(wires=range(PCA_dim))\n",
        "\n",
        "\n",
        "class Feature_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Feature_model,self).__init__()\n",
        "        self.cls = nn.Sequential(OrderedDict([('cls1', nn.Linear(PCA_dim,PCA_dim*8)),\n",
        "                                              ('relu1', nn.ReLU()),('cls2', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu2', nn.ReLU()),('cls3', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu3', nn.ReLU()),('cls4', nn.Linear(PCA_dim*8,PCA_dim*8)),\n",
        "                                              ('relu4', nn.ReLU()),('cls5', nn.Linear(PCA_dim*8,PCA_dim*2-1)),\n",
        "                                              ('sigmoid', nn.ReLU())]))\n",
        "        self.Kernal = Kernal\n",
        "    def forward(self,inputs):\n",
        "        epsilon = 1e-6\n",
        "        input1 = inputs[0]\n",
        "        input2 = inputs[1]\n",
        "        input1 = self.cls(input1)*np.pi\n",
        "        input2 = self.cls(input2)*np.pi\n",
        "        output = self.Kernal(input1,input2)\n",
        "        output = output.type(torch.float32)\n",
        "        return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 하이브리드 모델 정의\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HybridModel, self).__init__()\n",
        "        self.cls = feature_model.cls\n",
        "\n",
        "        self.quantum_layer = QuantumLayer\n",
        "        self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
        "    def forward(self, x):\n",
        "        x = self.cls(x)*np.pi\n",
        "        #print(qml.draw(self.quantum_layer)(x,self.Q_params))\n",
        "        quantum_output = self.quantum_layer(x,self.Q_params)\n",
        "        quantum_output = quantum_output.type(torch.float32)\n",
        "        return torch.log(quantum_output)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.cls_layer_1 = nn.Linear(PCA_dim,PCA_dim*PCA_dim)\n",
        "        self.cls_layer_2 = nn.Linear(PCA_dim*PCA_dim,PCA_dim*PCA_dim-1)\n",
        "        self.output_layer = nn.Linear(PCA_dim*PCA_dim-1,PCA_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.cls_layer_1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.cls_layer_2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "# 모델, 손실 함수, 최적화 설정\n",
        "\n",
        "\n",
        "feature_model = Feature_model(); criterion = nn.BCELoss()\n",
        "#model = Model(); criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(feature_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
        "train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15)\n",
        "\n",
        "model = HybridModel(); criterion = nn.NLLLoss()\n",
        "for param in model.cls.parameters():\n",
        "    param.requires_grad = False\n",
        "#model.load_state_dict(para_dict)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "print('\\n\\n Test start \\n\\n')\n",
        "train_process = Early_stop_train(model, optimizer, criterion)\n",
        "train_process.train_model(train_loader,test_loader,epochs=50,res=5)\n",
        "\n",
        "_,acc = train_process.test(test_loader)\n",
        "result_list_classical.append(acc)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fltgmdndkgx0",
        "outputId": "62dc3fa9-daad-4d8d-9226-2c659471af99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [05:01<00:00,  2.32it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "x_train = torch.tensor(x_train).float()\n",
        "num_data = x_train.shape[0]\n",
        "kernel_matrix = torch.zeros((num_data, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_data)\n",
        "    output = feature_model([data,x_train])\n",
        "    kernel_matrix[i] = output.detach().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgvLiqvOkgx3",
        "outputId": "2645aaf8-6657-42bf-f1bc-e32218adf955"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\pop75\\AppData\\Local\\Temp\\ipykernel_17080\\2430800910.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(y_train).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(11626.8711, grad_fn=<AddBackward0>) tensor(40000., grad_fn=<MulBackward0>)\n",
            "tensor(-11467.4785, grad_fn=<AddBackward0>) tensor(16900.4082, grad_fn=<MulBackward0>)\n",
            "tensor(-24483.9355, grad_fn=<AddBackward0>) tensor(3879.0730, grad_fn=<MulBackward0>)\n",
            "tensor(-28358.6680, grad_fn=<AddBackward0>) tensor(0.1115, grad_fn=<MulBackward0>)\n",
            "tensor(-25809.5742, grad_fn=<AddBackward0>) tensor(2546.0581, grad_fn=<MulBackward0>)\n",
            "tensor(-21156.7012, grad_fn=<AddBackward0>) tensor(7197.2168, grad_fn=<MulBackward0>)\n",
            "tensor(-18112.3242, grad_fn=<AddBackward0>) tensor(10241.2979, grad_fn=<MulBackward0>)\n",
            "tensor(-17972.2812, grad_fn=<AddBackward0>) tensor(10382.1631, grad_fn=<MulBackward0>)\n",
            "tensor(-20125.0586, grad_fn=<AddBackward0>) tensor(8231.0068, grad_fn=<MulBackward0>)\n",
            "tensor(-23266.3809, grad_fn=<AddBackward0>) tensor(5091.8472, grad_fn=<MulBackward0>)\n",
            "tensor(-26152.6465, grad_fn=<AddBackward0>) tensor(2208.0950, grad_fn=<MulBackward0>)\n",
            "tensor(-27942.9102, grad_fn=<AddBackward0>) tensor(420.5293, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.1973, grad_fn=<AddBackward0>) tensor(28.9568, grad_fn=<MulBackward0>)\n",
            "tensor(-27575.6133, grad_fn=<AddBackward0>) tensor(793.1105, grad_fn=<MulBackward0>)\n",
            "tensor(-26277.5000, grad_fn=<AddBackward0>) tensor(2093.5007, grad_fn=<MulBackward0>)\n",
            "tensor(-25145.7070, grad_fn=<AddBackward0>) tensor(3227.1565, grad_fn=<MulBackward0>)\n",
            "tensor(-24668.9238, grad_fn=<AddBackward0>) tensor(3705.3374, grad_fn=<MulBackward0>)\n",
            "tensor(-24973.3555, grad_fn=<AddBackward0>) tensor(3401.8562, grad_fn=<MulBackward0>)\n",
            "tensor(-25859.3594, grad_fn=<AddBackward0>) tensor(2516.4084, grad_fn=<MulBackward0>)\n",
            "tensor(-26949.6211, grad_fn=<AddBackward0>) tensor(1426.4170, grad_fn=<MulBackward0>)\n",
            "tensor(-27859.1113, grad_fn=<AddBackward0>) tensor(517.0309, grad_fn=<MulBackward0>)\n",
            "tensor(-28330.3613, grad_fn=<AddBackward0>) tensor(45.8541, grad_fn=<MulBackward0>)\n",
            "tensor(-28304.8867, grad_fn=<AddBackward0>) tensor(71.4865, grad_fn=<MulBackward0>)\n",
            "tensor(-27917.5898, grad_fn=<AddBackward0>) tensor(459.1383, grad_fn=<MulBackward0>)\n",
            "tensor(-27419.2109, grad_fn=<AddBackward0>) tensor(958.1445, grad_fn=<MulBackward0>)\n",
            "tensor(-27060.6504, grad_fn=<AddBackward0>) tensor(1317.6547, grad_fn=<MulBackward0>)\n",
            "tensor(-26992.0078, grad_fn=<AddBackward0>) tensor(1387.5674, grad_fn=<MulBackward0>)\n",
            "tensor(-27219.1211, grad_fn=<AddBackward0>) tensor(1162.0302, grad_fn=<MulBackward0>)\n",
            "tensor(-27626.1016, grad_fn=<AddBackward0>) tensor(756.8748, grad_fn=<MulBackward0>)\n",
            "tensor(-28042.2305, grad_fn=<AddBackward0>) tensor(342.7581, grad_fn=<MulBackward0>)\n",
            "tensor(-28318.7266, grad_fn=<AddBackward0>) tensor(68.3811, grad_fn=<MulBackward0>)\n",
            "tensor(-28385.2793, grad_fn=<AddBackward0>) tensor(3.9780, grad_fn=<MulBackward0>)\n",
            "tensor(-28267.6641, grad_fn=<AddBackward0>) tensor(123.6896, grad_fn=<MulBackward0>)\n",
            "tensor(-28063.6465, grad_fn=<AddBackward0>) tensor(329.6855, grad_fn=<MulBackward0>)\n",
            "tensor(-27891.0840, grad_fn=<AddBackward0>) tensor(504.0558, grad_fn=<MulBackward0>)\n",
            "tensor(-27833.9531, grad_fn=<AddBackward0>) tensor(562.8059, grad_fn=<MulBackward0>)\n",
            "tensor(-27911.0020, grad_fn=<AddBackward0>) tensor(487.1861, grad_fn=<MulBackward0>)\n",
            "tensor(-28077.8125, grad_fn=<AddBackward0>) tensor(321.6510, grad_fn=<MulBackward0>)\n",
            "tensor(-28256.4531, grad_fn=<AddBackward0>) tensor(144.1795, grad_fn=<MulBackward0>)\n",
            "tensor(-28375.2070, grad_fn=<AddBackward0>) tensor(26.5464, grad_fn=<MulBackward0>)\n",
            "tensor(-28399.9414, grad_fn=<AddBackward0>) tensor(2.9548, grad_fn=<MulBackward0>)\n",
            "tensor(-28344.0098, grad_fn=<AddBackward0>) tensor(60.1107, grad_fn=<MulBackward0>)\n",
            "tensor(-28255.2070, grad_fn=<AddBackward0>) tensor(150.2680, grad_fn=<MulBackward0>)\n",
            "tensor(-28188.3535, grad_fn=<AddBackward0>) tensor(218.6330, grad_fn=<MulBackward0>)\n",
            "tensor(-28178.4316, grad_fn=<AddBackward0>) tensor(230.2302, grad_fn=<MulBackward0>)\n",
            "tensor(-28226.9883, grad_fn=<AddBackward0>) tensor(183.5119, grad_fn=<MulBackward0>)\n",
            "tensor(-28306.7109, grad_fn=<AddBackward0>) tensor(105.7571, grad_fn=<MulBackward0>)\n",
            "tensor(-28379.1309, grad_fn=<AddBackward0>) tensor(35.3991, grad_fn=<MulBackward0>)\n",
            "tensor(-28415.1777, grad_fn=<AddBackward0>) tensor(1.4594, grad_fn=<MulBackward0>)\n",
            "tensor(-28408.0547, grad_fn=<AddBackward0>) tensor(10.6923, grad_fn=<MulBackward0>)\n",
            "tensor(-28373.2383, grad_fn=<AddBackward0>) tensor(47.5807, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.3125, grad_fn=<AddBackward0>) tensor(85.5056, grad_fn=<MulBackward0>)\n",
            "tensor(-28322.7578, grad_fn=<AddBackward0>) tensor(101.9731, grad_fn=<MulBackward0>)\n",
            "tensor(-28337.1113, grad_fn=<AddBackward0>) tensor(89.4451, grad_fn=<MulBackward0>)\n",
            "tensor(-28371.4551, grad_fn=<AddBackward0>) tensor(56.8546, grad_fn=<MulBackward0>)\n",
            "tensor(-28407.6816, grad_fn=<AddBackward0>) tensor(22.3385, grad_fn=<MulBackward0>)\n",
            "tensor(-28429.4863, grad_fn=<AddBackward0>) tensor(2.2332, grad_fn=<MulBackward0>)\n",
            "tensor(-28430.7090, grad_fn=<AddBackward0>) tensor(2.7308, grad_fn=<MulBackward0>)\n",
            "tensor(-28417.0566, grad_fn=<AddBackward0>) tensor(18.1601, grad_fn=<MulBackward0>)\n",
            "tensor(-28401.1465, grad_fn=<AddBackward0>) tensor(35.9233, grad_fn=<MulBackward0>)\n",
            "tensor(-28394.5879, grad_fn=<AddBackward0>) tensor(44.4304, grad_fn=<MulBackward0>)\n",
            "tensor(-28401.7852, grad_fn=<AddBackward0>) tensor(39.2714, grad_fn=<MulBackward0>)\n",
            "tensor(-28418.6895, grad_fn=<AddBackward0>) tensor(24.4874, grad_fn=<MulBackward0>)\n",
            "tensor(-28436.4141, grad_fn=<AddBackward0>) tensor(8.9416, grad_fn=<MulBackward0>)\n",
            "tensor(-28447.0137, grad_fn=<AddBackward0>) tensor(0.5586, grad_fn=<MulBackward0>)\n",
            "tensor(-28447.8164, grad_fn=<AddBackward0>) tensor(1.9838, grad_fn=<MulBackward0>)\n",
            "tensor(-28442.0977, grad_fn=<AddBackward0>) tensor(9.9154, grad_fn=<MulBackward0>)\n",
            "tensor(-28436.2598, grad_fn=<AddBackward0>) tensor(17.9427, grad_fn=<MulBackward0>)\n",
            "tensor(-28435.6152, grad_fn=<AddBackward0>) tensor(20.7342, grad_fn=<MulBackward0>)\n",
            "tensor(-28441.5020, grad_fn=<AddBackward0>) tensor(16.9580, grad_fn=<MulBackward0>)\n",
            "tensor(-28451.1582, grad_fn=<AddBackward0>) tensor(9.3877, grad_fn=<MulBackward0>)\n",
            "tensor(-28460.0078, grad_fn=<AddBackward0>) tensor(2.6122, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.6992, grad_fn=<AddBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.9004, grad_fn=<AddBackward0>) tensor(1.9066, grad_fn=<MulBackward0>)\n",
            "tensor(-28463.0137, grad_fn=<AddBackward0>) tensor(5.9434, grad_fn=<MulBackward0>)\n",
            "tensor(-28462.2598, grad_fn=<AddBackward0>) tensor(8.8970, grad_fn=<MulBackward0>)\n",
            "tensor(-28464.6035, grad_fn=<AddBackward0>) tensor(8.8095, grad_fn=<MulBackward0>)\n",
            "tensor(-28469.7324, grad_fn=<AddBackward0>) tensor(5.9841, grad_fn=<MulBackward0>)\n",
            "tensor(-28475.6602, grad_fn=<AddBackward0>) tensor(2.4030, grad_fn=<MulBackward0>)\n",
            "tensor(-28480.2227, grad_fn=<AddBackward0>) tensor(0.2173, grad_fn=<MulBackward0>)\n",
            "tensor(-28482.4648, grad_fn=<AddBackward0>) tensor(0.3629, grad_fn=<MulBackward0>)\n",
            "tensor(-28483.0488, grad_fn=<AddBackward0>) tensor(2.1652, grad_fn=<MulBackward0>)\n",
            "tensor(-28483.5703, grad_fn=<AddBackward0>) tensor(4.0290, grad_fn=<MulBackward0>)\n",
            "tensor(-28485.3691, grad_fn=<AddBackward0>) tensor(4.6012, grad_fn=<MulBackward0>)\n",
            "tensor(-28488.7344, grad_fn=<AddBackward0>) tensor(3.5941, grad_fn=<MulBackward0>)\n",
            "tensor(-28492.8887, grad_fn=<AddBackward0>) tensor(1.7907, grad_fn=<MulBackward0>)\n",
            "tensor(-28496.6738, grad_fn=<AddBackward0>) tensor(0.3593, grad_fn=<MulBackward0>)\n",
            "tensor(-28499.3691, grad_fn=<AddBackward0>) tensor(0.0294, grad_fn=<MulBackward0>)\n",
            "tensor(-28501.0938, grad_fn=<AddBackward0>) tensor(0.6919, grad_fn=<MulBackward0>)\n",
            "tensor(-28502.5898, grad_fn=<AddBackward0>) tensor(1.6155, grad_fn=<MulBackward0>)\n",
            "tensor(-28504.6113, grad_fn=<AddBackward0>) tensor(2.0425, grad_fn=<MulBackward0>)\n",
            "tensor(-28507.4492, grad_fn=<AddBackward0>) tensor(1.6940, grad_fn=<MulBackward0>)\n",
            "tensor(-28510.7812, grad_fn=<AddBackward0>) tensor(0.8791, grad_fn=<MulBackward0>)\n",
            "tensor(-28514.0215, grad_fn=<AddBackward0>) tensor(0.1817, grad_fn=<MulBackward0>)\n",
            "tensor(-28516.7461, grad_fn=<AddBackward0>) tensor(0.0151, grad_fn=<MulBackward0>)\n",
            "tensor(-28518.9688, grad_fn=<AddBackward0>) tensor(0.3640, grad_fn=<MulBackward0>)\n",
            "tensor(-28521.0371, grad_fn=<AddBackward0>) tensor(0.8651, grad_fn=<MulBackward0>)\n",
            "tensor(-28523.3613, grad_fn=<AddBackward0>) tensor(1.1127, grad_fn=<MulBackward0>)\n",
            "tensor(-28526.1016, grad_fn=<AddBackward0>) tensor(0.9446, grad_fn=<MulBackward0>)\n",
            "tensor(-28529.1094, grad_fn=<AddBackward0>) tensor(0.5123, grad_fn=<MulBackward0>)\n",
            "tensor(-28532.0762, grad_fn=<AddBackward0>) tensor(0.1235, grad_fn=<MulBackward0>)\n",
            "tensor(-28534.7910, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28537.2578, grad_fn=<AddBackward0>) tensor(0.1432, grad_fn=<MulBackward0>)\n",
            "tensor(-28539.6641, grad_fn=<AddBackward0>) tensor(0.3644, grad_fn=<MulBackward0>)\n",
            "tensor(-28542.2227, grad_fn=<AddBackward0>) tensor(0.4636, grad_fn=<MulBackward0>)\n",
            "tensor(-28544.9941, grad_fn=<AddBackward0>) tensor(0.3697, grad_fn=<MulBackward0>)\n",
            "tensor(-28547.8945, grad_fn=<AddBackward0>) tensor(0.1712, grad_fn=<MulBackward0>)\n",
            "tensor(-28550.7656, grad_fn=<AddBackward0>) tensor(0.0217, grad_fn=<MulBackward0>)\n",
            "tensor(-28553.5020, grad_fn=<AddBackward0>) tensor(0.0169, grad_fn=<MulBackward0>)\n",
            "tensor(-28556.1348, grad_fn=<AddBackward0>) tensor(0.1311, grad_fn=<MulBackward0>)\n",
            "tensor(-28558.7598, grad_fn=<AddBackward0>) tensor(0.2549, grad_fn=<MulBackward0>)\n",
            "tensor(-28561.4844, grad_fn=<AddBackward0>) tensor(0.2891, grad_fn=<MulBackward0>)\n",
            "tensor(-28564.3223, grad_fn=<AddBackward0>) tensor(0.2151, grad_fn=<MulBackward0>)\n",
            "tensor(-28567.2109, grad_fn=<AddBackward0>) tensor(0.0948, grad_fn=<MulBackward0>)\n",
            "tensor(-28570.0781, grad_fn=<AddBackward0>) tensor(0.0122, grad_fn=<MulBackward0>)\n",
            "tensor(-28572.8770, grad_fn=<AddBackward0>) tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "tensor(-28575.6406, grad_fn=<AddBackward0>) tensor(0.0551, grad_fn=<MulBackward0>)\n",
            "tensor(-28578.4258, grad_fn=<AddBackward0>) tensor(0.0986, grad_fn=<MulBackward0>)\n",
            "tensor(-28581.2734, grad_fn=<AddBackward0>) tensor(0.0973, grad_fn=<MulBackward0>)\n",
            "tensor(-28584.1797, grad_fn=<AddBackward0>) tensor(0.0557, grad_fn=<MulBackward0>)\n",
            "tensor(-28587.1074, grad_fn=<AddBackward0>) tensor(0.0120, grad_fn=<MulBackward0>)\n",
            "tensor(-28590.0137, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28592.8965, grad_fn=<AddBackward0>) tensor(0.0278, grad_fn=<MulBackward0>)\n",
            "tensor(-28595.7754, grad_fn=<AddBackward0>) tensor(0.0659, grad_fn=<MulBackward0>)\n",
            "tensor(-28598.6875, grad_fn=<AddBackward0>) tensor(0.0841, grad_fn=<MulBackward0>)\n",
            "tensor(-28601.6387, grad_fn=<AddBackward0>) tensor(0.0700, grad_fn=<MulBackward0>)\n",
            "tensor(-28604.6152, grad_fn=<AddBackward0>) tensor(0.0363, grad_fn=<MulBackward0>)\n",
            "tensor(-28607.6035, grad_fn=<AddBackward0>) tensor(0.0081, grad_fn=<MulBackward0>)\n",
            "tensor(-28610.5801, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28613.5508, grad_fn=<AddBackward0>) tensor(0.0091, grad_fn=<MulBackward0>)\n",
            "tensor(-28616.5410, grad_fn=<AddBackward0>) tensor(0.0199, grad_fn=<MulBackward0>)\n",
            "tensor(-28619.5547, grad_fn=<AddBackward0>) tensor(0.0204, grad_fn=<MulBackward0>)\n",
            "tensor(-28622.5918, grad_fn=<AddBackward0>) tensor(0.0109, grad_fn=<MulBackward0>)\n",
            "tensor(-28625.6426, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28628.7031, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28631.7598, grad_fn=<AddBackward0>) tensor(0.0114, grad_fn=<MulBackward0>)\n",
            "tensor(-28634.8301, grad_fn=<AddBackward0>) tensor(0.0231, grad_fn=<MulBackward0>)\n",
            "tensor(-28637.9160, grad_fn=<AddBackward0>) tensor(0.0276, grad_fn=<MulBackward0>)\n",
            "tensor(-28641.0234, grad_fn=<AddBackward0>) tensor(0.0221, grad_fn=<MulBackward0>)\n",
            "tensor(-28644.1426, grad_fn=<AddBackward0>) tensor(0.0113, grad_fn=<MulBackward0>)\n",
            "tensor(-28647.2734, grad_fn=<AddBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n",
            "tensor(-28650.4121, grad_fn=<AddBackward0>) tensor(1.0510e-06, grad_fn=<MulBackward0>)\n",
            "tensor(-28653.5566, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28656.7168, grad_fn=<AddBackward0>) tensor(0.0035, grad_fn=<MulBackward0>)\n",
            "tensor(-28659.8926, grad_fn=<AddBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n",
            "tensor(-28663.0840, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28666.2852, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28669.4980, grad_fn=<AddBackward0>) tensor(0.0027, grad_fn=<MulBackward0>)\n",
            "tensor(-28672.7188, grad_fn=<AddBackward0>) tensor(0.0070, grad_fn=<MulBackward0>)\n",
            "tensor(-28675.9531, grad_fn=<AddBackward0>) tensor(0.0103, grad_fn=<MulBackward0>)\n",
            "tensor(-28679.2012, grad_fn=<AddBackward0>) tensor(0.0102, grad_fn=<MulBackward0>)\n",
            "tensor(-28682.4609, grad_fn=<AddBackward0>) tensor(0.0072, grad_fn=<MulBackward0>)\n",
            "tensor(-28685.7344, grad_fn=<AddBackward0>) tensor(0.0034, grad_fn=<MulBackward0>)\n",
            "tensor(-28689.0195, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28692.3125, grad_fn=<AddBackward0>) tensor(1.1787e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28695.6191, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28698.9375, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28702.2695, grad_fn=<AddBackward0>) tensor(1.3481e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28705.6133, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28708.9668, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28712.3340, grad_fn=<AddBackward0>) tensor(0.0031, grad_fn=<MulBackward0>)\n",
            "tensor(-28715.7109, grad_fn=<AddBackward0>) tensor(0.0047, grad_fn=<MulBackward0>)\n",
            "tensor(-28719.0996, grad_fn=<AddBackward0>) tensor(0.0051, grad_fn=<MulBackward0>)\n",
            "tensor(-28722.5020, grad_fn=<AddBackward0>) tensor(0.0042, grad_fn=<MulBackward0>)\n",
            "tensor(-28725.9160, grad_fn=<AddBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n",
            "tensor(-28729.3418, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28732.7793, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28736.2246, grad_fn=<AddBackward0>) tensor(0.0001, grad_fn=<MulBackward0>)\n",
            "tensor(-28739.6855, grad_fn=<AddBackward0>) tensor(7.4079e-05, grad_fn=<MulBackward0>)\n",
            "tensor(-28743.1562, grad_fn=<AddBackward0>) tensor(0.0002, grad_fn=<MulBackward0>)\n",
            "tensor(-28746.6406, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28750.1328, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28753.6387, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28757.1562, grad_fn=<AddBackward0>) tensor(0.0028, grad_fn=<MulBackward0>)\n",
            "tensor(-28760.6836, grad_fn=<AddBackward0>) tensor(0.0030, grad_fn=<MulBackward0>)\n",
            "tensor(-28764.2266, grad_fn=<AddBackward0>) tensor(0.0026, grad_fn=<MulBackward0>)\n",
            "tensor(-28767.7773, grad_fn=<AddBackward0>) tensor(0.0019, grad_fn=<MulBackward0>)\n",
            "tensor(-28771.3398, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28774.9141, grad_fn=<AddBackward0>) tensor(0.0007, grad_fn=<MulBackward0>)\n",
            "tensor(-28778.5020, grad_fn=<AddBackward0>) tensor(0.0005, grad_fn=<MulBackward0>)\n",
            "tensor(-28782.0977, grad_fn=<AddBackward0>) tensor(0.0004, grad_fn=<MulBackward0>)\n",
            "tensor(-28785.7090, grad_fn=<AddBackward0>) tensor(0.0006, grad_fn=<MulBackward0>)\n",
            "tensor(-28789.3281, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28792.9551, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28796.6016, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28800.2559, grad_fn=<AddBackward0>) tensor(0.0020, grad_fn=<MulBackward0>)\n",
            "tensor(-28803.9199, grad_fn=<AddBackward0>) tensor(0.0021, grad_fn=<MulBackward0>)\n",
            "tensor(-28807.5957, grad_fn=<AddBackward0>) tensor(0.0018, grad_fn=<MulBackward0>)\n",
            "tensor(-28811.2812, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28814.9824, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28818.6914, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28822.4141, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28826.1465, grad_fn=<AddBackward0>) tensor(0.0008, grad_fn=<MulBackward0>)\n",
            "tensor(-28829.8906, grad_fn=<AddBackward0>) tensor(0.0009, grad_fn=<MulBackward0>)\n",
            "tensor(-28833.6426, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28837.4082, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28841.1875, grad_fn=<AddBackward0>) tensor(0.0016, grad_fn=<MulBackward0>)\n",
            "tensor(-28844.9746, grad_fn=<AddBackward0>) tensor(0.0017, grad_fn=<MulBackward0>)\n",
            "tensor(-28848.7734, grad_fn=<AddBackward0>) tensor(0.0016, grad_fn=<MulBackward0>)\n",
            "tensor(-28852.5820, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28856.4023, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28860.2324, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28864.0801, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28867.9355, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28871.7969, grad_fn=<AddBackward0>) tensor(0.0010, grad_fn=<MulBackward0>)\n",
            "tensor(-28875.6738, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28879.5605, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28883.4570, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28887.3652, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28891.2852, grad_fn=<AddBackward0>) tensor(0.0015, grad_fn=<MulBackward0>)\n",
            "tensor(-28895.2148, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28899.1582, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28903.1094, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28907.0723, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28911.0469, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28915.0332, grad_fn=<AddBackward0>) tensor(0.0011, grad_fn=<MulBackward0>)\n",
            "tensor(-28919.0273, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28923.0332, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28927.0488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28931.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28935.1172, grad_fn=<AddBackward0>) tensor(0.0014, grad_fn=<MulBackward0>)\n",
            "tensor(-28939.1680, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28943.2285, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28947.2988, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28951.3828, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28955.4727, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28959.5762, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28963.6914, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28967.8145, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28971.9492, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28976.0977, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28980.2539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28984.4199, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28988.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-28992.7852, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-28996.9863, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29001.1953, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29005.4160, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29009.6465, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29013.8887, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29018.1387, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29022.4023, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29026.6758, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29030.9609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29035.2559, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29039.5586, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29043.8750, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29048.2012, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29052.5371, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29056.8828, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29061.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29065.6113, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29069.9863, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29074.3770, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29078.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29083.1836, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29087.6055, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29092.0371, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29096.4766, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29100.9277, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29105.3887, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29109.8633, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29114.3438, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29118.8379, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29123.3398, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29127.8535, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29132.3789, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29136.9121, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29141.4590, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29146.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29150.5762, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29155.1523, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29159.7402, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29164.3359, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29168.9434, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29173.5605, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29178.1895, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29182.8262, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29187.4746, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29192.1328, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29196.8008, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29201.4785, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29206.1680, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29210.8672, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29215.5781, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29220.2969, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29225.0293, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29229.7676, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29234.5195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29239.2793, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29244.0508, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29248.8320, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29253.6250, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29258.4258, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29263.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29268.0586, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29272.8926, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29277.7344, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29282.5879, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29287.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29292.3262, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29297.2090, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29302.1035, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29307.0059, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29311.9199, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29316.8438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29321.7793, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29326.7227, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29331.6758, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29336.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29341.6172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29346.6035, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29351.5957, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29356.6035, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29361.6191, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29366.6445, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29371.6777, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29376.7266, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29381.7812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29386.8496, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29391.9238, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29397.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29402.1074, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29407.2148, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29412.3320, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29417.4590, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29422.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29427.7422, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29432.8984, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29438.0684, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29443.2441, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29448.4336, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29453.6289, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29458.8379, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29464.0547, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29469.2812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29474.5195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29479.7695, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29485.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29490.2949, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29495.5723, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29500.8613, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29506.1602, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29511.4688, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29516.7871, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29522.1172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29527.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29532.8027, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29538.1602, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29543.5312, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29548.9082, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29554.2988, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29559.6973, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29565.1074, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29570.5254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29575.9551, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29581.3945, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29586.8438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29592.3027, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29597.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29603.2520, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29608.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29614.2402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29619.7520, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29625.2695, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29630.8008, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29636.3398, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29641.8906, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29647.4492, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29653.0176, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29658.5996, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29664.1895, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29669.7891, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29675.3984, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29681.0195, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29686.6504, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29692.2910, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29697.9395, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29703.5996, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29709.2715, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29714.9512, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29720.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29726.3418, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29732.0527, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29737.7734, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29743.5020, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29749.2441, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29754.9941, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29760.7539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29766.5254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29772.3066, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29778.0977, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29783.8965, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29789.7090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29795.5293, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29801.3613, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29807.2031, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29813.0527, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29818.9141, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29824.7852, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29830.6660, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29836.5566, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29842.4590, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29848.3711, grad_fn=<AddBackward0>) tensor(0.0012, grad_fn=<MulBackward0>)\n",
            "tensor(-29854.2930, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29860.2227, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29866.1641, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29872.1172, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29878.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29884.0488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29890.0332, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29896.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29902.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29908.0352, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29914.0586, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29920.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29926.1328, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29932.1855, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29938.2461, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29944.3203, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29950.4004, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29956.4941, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29962.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29968.7090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29974.8340, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29980.9648, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29987.1094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29993.2617, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-29999.4238, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30005.5957, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30011.7773, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30017.9746, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30024.1777, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30030.3906, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30036.6133, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30042.8477, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30049.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30055.3438, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30061.6094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30067.8828, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30074.1680, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30080.4609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30086.7656, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30093.0801, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30099.4043, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30105.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30112.0859, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30118.4395, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30124.8047, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30131.1797, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30137.5645, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30143.9609, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30150.3672, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30156.7812, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30163.2090, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30169.6426, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30176.0918, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30182.5469, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30189.0117, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30195.4902, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30201.9746, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30208.4707, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30214.9805, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30221.4980, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30228.0234, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30234.5625, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30241.1094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30247.6699, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30254.2363, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30260.8145, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30267.4043, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30274., grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30280.6094, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30287.2285, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30293.8574, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30300.4980, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30307.1445, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30313.8047, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30320.4766, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30327.1562, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30333.8477, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30340.5488, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30347.2598, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30353.9805, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30360.7109, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30367.4531, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30374.2031, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30380.9648, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30387.7402, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30394.5215, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30401.3125, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30408.1152, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30414.9316, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30421.7539, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30428.5859, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30435.4316, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30442.2852, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30449.1484, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30456.0254, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "tensor(-30462.9102, grad_fn=<AddBackward0>) tensor(0.0013, grad_fn=<MulBackward0>)\n",
            "Optimized alphas: tensor([0.5160, 0.5141, 0.5133, 0.5204, 0.5213, 0.5208, 0.5154, 0.5213, 0.5215,\n",
            "        0.5161, 0.5151, 0.5134, 0.5227, 0.5211, 0.5144, 0.5215, 0.5210, 0.5137,\n",
            "        0.5153, 0.5216, 0.5158, 0.5211, 0.5156, 0.5187, 0.5153, 0.5151, 0.5213,\n",
            "        0.5125, 0.5151, 0.5221, 0.5214, 0.5207, 0.5125, 0.5215, 0.5152, 0.5210,\n",
            "        0.5213, 0.5138, 0.5207, 0.5155, 0.5216, 0.5159, 0.5216, 0.5216, 0.5216,\n",
            "        0.5205, 0.5156, 0.5216, 0.5159, 0.5216, 0.5203, 0.5161, 0.5215, 0.5163,\n",
            "        0.5214, 0.5216, 0.5216, 0.5199, 0.5157, 0.5212, 0.5211, 0.5161, 0.5210,\n",
            "        0.5206, 0.5109, 0.5190, 0.5142, 0.5152, 0.5169, 0.5148, 0.5158, 0.5158,\n",
            "        0.5211, 0.5143, 0.5160, 0.5213, 0.5129, 0.5159, 0.5210, 0.5201, 0.5161,\n",
            "        0.5210, 0.5216, 0.5209, 0.5214, 0.5160, 0.5140, 0.5213, 0.5212, 0.5216,\n",
            "        0.5208, 0.5159, 0.5191, 0.5148, 0.5212, 0.5138, 0.5148, 0.5215, 0.5152,\n",
            "        0.5211, 0.5211, 0.5153, 0.5216, 0.5216, 0.5213, 0.5159, 0.5159, 0.5216,\n",
            "        0.5189, 0.5158, 0.5216, 0.5152, 0.5191, 0.5210, 0.5211, 0.5214, 0.5206,\n",
            "        0.5215, 0.5214, 0.5215, 0.5157, 0.5216, 0.5216, 0.5128, 0.5151, 0.5160,\n",
            "        0.5213, 0.5159, 0.5158, 0.5214, 0.5213, 0.5157, 0.5223, 0.5206, 0.5156,\n",
            "        0.5160, 0.5152, 0.5206, 0.5158, 0.5139, 0.5204, 0.5219, 0.5159, 0.5188,\n",
            "        0.5149, 0.5210, 0.5153, 0.5159, 0.5160, 0.5133, 0.5153, 0.5212, 0.5227,\n",
            "        0.5216, 0.5155, 0.5150, 0.5209, 0.5126, 0.5145, 0.5159, 0.5225, 0.5208,\n",
            "        0.5147, 0.5218, 0.5211, 0.5199, 0.5209, 0.5159, 0.5147, 0.5152, 0.5211,\n",
            "        0.5209, 0.5096, 0.5159, 0.5140, 0.5151, 0.5144, 0.5161, 0.5161, 0.5211,\n",
            "        0.5126, 0.5154, 0.5159, 0.5158, 0.5216, 0.5158, 0.5206, 0.5159, 0.5159,\n",
            "        0.5195, 0.5152, 0.5216, 0.5154, 0.5159, 0.5157, 0.5154, 0.5160, 0.5213,\n",
            "        0.5216, 0.5216, 0.5210, 0.5225, 0.5155, 0.5218, 0.5161, 0.5218, 0.5216,\n",
            "        0.5149, 0.5215, 0.5160, 0.5214, 0.5160, 0.5216, 0.5089, 0.5209, 0.5214,\n",
            "        0.5216, 0.5201, 0.5160, 0.5156, 0.5161, 0.5156, 0.5193, 0.5215, 0.5161,\n",
            "        0.5216, 0.5213, 0.5212, 0.5159, 0.5215, 0.5155, 0.5145, 0.5159, 0.5214,\n",
            "        0.5140, 0.5160, 0.5216, 0.5136, 0.5214, 0.5161, 0.5142, 0.5215, 0.5160,\n",
            "        0.5154, 0.5151, 0.5213, 0.5216, 0.5217, 0.5147, 0.5150, 0.5213, 0.5148,\n",
            "        0.5131, 0.5215, 0.5153, 0.5206, 0.5138, 0.5216, 0.5203, 0.5160, 0.5217,\n",
            "        0.5158, 0.5210, 0.5197, 0.5216, 0.5216, 0.5144, 0.5215, 0.5216, 0.5135,\n",
            "        0.5114, 0.5207, 0.5195, 0.5215, 0.5115, 0.5212, 0.5156, 0.5159, 0.5162,\n",
            "        0.5160, 0.5148, 0.5205, 0.5141, 0.5160, 0.5213, 0.5158, 0.5210, 0.5144,\n",
            "        0.5215, 0.5217, 0.5160, 0.5156, 0.5154, 0.5161, 0.5160, 0.5215, 0.5216,\n",
            "        0.5210, 0.5157, 0.5152, 0.5160, 0.5216, 0.5158, 0.5201, 0.5137, 0.5215,\n",
            "        0.5213, 0.5148, 0.5161, 0.5160, 0.5161, 0.5157, 0.5158, 0.5207, 0.5154,\n",
            "        0.5181, 0.5160, 0.5126, 0.5134, 0.5160, 0.5214, 0.5214, 0.5209, 0.5159,\n",
            "        0.5216, 0.5159, 0.5215, 0.5143, 0.5147, 0.5159, 0.5132, 0.5154, 0.5162,\n",
            "        0.5152, 0.5215, 0.5215, 0.5142, 0.5209, 0.5157, 0.5160, 0.5158, 0.5159,\n",
            "        0.5215, 0.5209, 0.5151, 0.5206, 0.5214, 0.5216, 0.5213, 0.5216, 0.5215,\n",
            "        0.5212, 0.5211, 0.5213, 0.5152, 0.5159, 0.5160, 0.5162, 0.5207, 0.5214,\n",
            "        0.5161, 0.5153, 0.5157, 0.5220, 0.5158, 0.5227, 0.5159, 0.5158, 0.5215,\n",
            "        0.5233, 0.5157, 0.5156, 0.5161, 0.5133, 0.5157, 0.5159, 0.5158, 0.5151,\n",
            "        0.5215, 0.5160, 0.5132, 0.5152, 0.5151, 0.5211, 0.5155, 0.5158, 0.5213,\n",
            "        0.5152, 0.5201, 0.5220, 0.5215, 0.5199, 0.5161, 0.5215, 0.5216, 0.5151,\n",
            "        0.5213, 0.5138, 0.5148, 0.5230, 0.5140, 0.5160, 0.5146, 0.5213, 0.5160,\n",
            "        0.5217, 0.5151, 0.5161, 0.5158, 0.5213, 0.5209, 0.5159, 0.5186, 0.5203,\n",
            "        0.5209, 0.5210, 0.5137, 0.5157, 0.5216, 0.5219, 0.5216, 0.5156, 0.5117,\n",
            "        0.5217, 0.5157, 0.5216, 0.5206, 0.5132, 0.5149, 0.5205, 0.5148, 0.5212,\n",
            "        0.5160, 0.5216, 0.5160, 0.5212, 0.5215, 0.5152, 0.5215, 0.5158, 0.5211,\n",
            "        0.5216, 0.5150, 0.5152, 0.5159, 0.5209, 0.5206, 0.5136, 0.5159, 0.5161,\n",
            "        0.5156, 0.5158, 0.5214, 0.5218, 0.5212, 0.5216, 0.5216, 0.5215, 0.5203,\n",
            "        0.5159, 0.5216, 0.5132, 0.5212, 0.5211, 0.5142, 0.5181, 0.5169, 0.5160,\n",
            "        0.5130, 0.5216, 0.5143, 0.5148, 0.5156, 0.5214, 0.5214, 0.5158, 0.5197,\n",
            "        0.5159, 0.5214, 0.5163, 0.5122, 0.5216, 0.5160, 0.5118, 0.5156, 0.5207,\n",
            "        0.5215, 0.5133, 0.5140, 0.5155, 0.5216, 0.5151, 0.5155, 0.5215, 0.5157,\n",
            "        0.5160, 0.5140, 0.5218, 0.5148, 0.5231, 0.5197, 0.5153, 0.5213, 0.5129,\n",
            "        0.5226, 0.5125, 0.5156, 0.5159, 0.5130, 0.5133, 0.5211, 0.5211, 0.5215,\n",
            "        0.5216, 0.5207, 0.5232, 0.5119, 0.5143, 0.5156, 0.5154, 0.5207, 0.5214,\n",
            "        0.5163, 0.5214, 0.5157, 0.5159, 0.5219, 0.5197, 0.5210, 0.5213, 0.5216,\n",
            "        0.5203, 0.5151, 0.5162, 0.5216, 0.5139, 0.5160, 0.5160, 0.5158, 0.5149,\n",
            "        0.5208, 0.5160, 0.5212, 0.5214, 0.5215, 0.5215, 0.5148, 0.5156, 0.5215,\n",
            "        0.5158, 0.5154, 0.5217, 0.5213, 0.5216, 0.5212, 0.5214, 0.5209, 0.5215,\n",
            "        0.5208, 0.5212, 0.5203, 0.5155, 0.5215, 0.5101, 0.5201, 0.5206, 0.5153,\n",
            "        0.5162, 0.5215, 0.5148, 0.5160, 0.5206, 0.5215, 0.5154, 0.5214, 0.5214,\n",
            "        0.5214, 0.5205, 0.5210, 0.5156, 0.5160, 0.5182, 0.5156, 0.5139, 0.5160,\n",
            "        0.5158, 0.5208, 0.5143, 0.5141, 0.5158, 0.5134, 0.5216, 0.5215, 0.5157,\n",
            "        0.5172, 0.5213, 0.5154, 0.5218, 0.5158, 0.5149, 0.5213, 0.5156, 0.5157,\n",
            "        0.5213, 0.5143, 0.5160, 0.5212, 0.5160, 0.5212, 0.5161, 0.5207, 0.5212,\n",
            "        0.5140, 0.5161, 0.5209, 0.5201, 0.5195, 0.5211, 0.5158, 0.5215, 0.5216,\n",
            "        0.5216, 0.5206, 0.5160, 0.5158, 0.5152, 0.5216, 0.5148, 0.5145, 0.5161,\n",
            "        0.5215, 0.5151, 0.5152, 0.5152, 0.5158, 0.5160, 0.5216, 0.5213, 0.5160,\n",
            "        0.5154, 0.5154, 0.5216, 0.5160, 0.5159, 0.5215, 0.5161, 0.5210, 0.5215,\n",
            "        0.5216, 0.5206, 0.5156, 0.5215, 0.5153, 0.5214, 0.5160, 0.5156, 0.5160,\n",
            "        0.5216, 0.5161, 0.5235, 0.5154, 0.5196, 0.5158, 0.5216, 0.5156, 0.5137,\n",
            "        0.5209, 0.5147, 0.5219, 0.5214, 0.5216, 0.5216, 0.5160, 0.5144, 0.5216,\n",
            "        0.5205, 0.5183, 0.5197, 0.5159, 0.5141, 0.5212, 0.5149, 0.5211, 0.5160,\n",
            "        0.5212, 0.5149, 0.5145, 0.5144, 0.5218, 0.5216, 0.5211, 0.5155, 0.5159,\n",
            "        0.5163, 0.5211, 0.5205, 0.5216, 0.5161, 0.5161, 0.5214])\n"
          ]
        }
      ],
      "source": [
        "labels = torch.tensor(y_train).float()\n",
        "labels = 2*labels-1\n",
        "alpha = torch.tensor([0.5]*num_data,requires_grad=True)\n",
        "optimizer = torch.optim.Adam([alpha], lr=0.001)\n",
        "def objective_function(alpha, kernel_matrix, labels):\n",
        "    \"\"\"SVM의 쌍대 목적 함수\"\"\"\n",
        "    L = 0.5 * torch.dot(alpha, torch.mv(kernel_matrix, alpha)) - torch.sum(alpha)\n",
        "    # 제약 조건을 유지하기 위해 레이블과 alpha의 곱의 합은 0이어야 합니다.\n",
        "    constraint = torch.dot(alpha, labels)\n",
        "    loss = -L + 1e4 * constraint ** 2\n",
        "    print(loss,1e4 * constraint ** 2)\n",
        "    return loss  # 제약조건에 큰 페널티를 적용\n",
        "\n",
        "# 훈련 과정\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective_function(alpha, kernel_matrix, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    alpha.data.clamp_(0)  # alpha는 0 이상이어야 함\n",
        "\n",
        "print(\"Optimized alphas:\", alpha.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm6T_J2-kgx4",
        "outputId": "e148967c-146a-4fc4-d020-e30e23df3246"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 700/700 [02:38<00:00,  4.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: tensor([-1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,\n",
            "        -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
            "         1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1., -1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
            "         1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
            "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
            "        -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
            "         1., -1., -1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1., -1.,\n",
            "         1., -1.,  1., -1.,  1., -1.,  1.,  1., -1., -1., -1., -1., -1., -1.,\n",
            "        -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1., -1.,\n",
            "        -1.,  1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
            "         1., -1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
            "         1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
            "        -1., -1., -1., -1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,\n",
            "        -1., -1.,  1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,\n",
            "         1., -1., -1.,  1., -1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,\n",
            "        -1., -1.,  1.,  1.,  1.,  1.], grad_fn=<SignBackward0>)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pennylane as qml\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 테스트 데이터와 훈련 데이터 간의 양자 커널 행렬 계산\n",
        "x_test = torch.tensor(x_test).float()\n",
        "num_test = x_test.size(0)\n",
        "test_kernel_matrix = torch.zeros((num_test, num_data), dtype=torch.float32)\n",
        "\n",
        "for i in tqdm(range(num_data)):\n",
        "    data = torch.stack([x_train[i]]*num_test)\n",
        "    output = feature_model([data,x_test])\n",
        "    test_kernel_matrix[:,i] = output.detach().cpu()\n",
        "\n",
        "# 훈련된 모델을 사용하여 테스트 데이터의 클래스 예측\n",
        "predictions = torch.sign(torch.mv(test_kernel_matrix, alpha * labels))\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tqKJfT74pDjk"
      },
      "outputs": [],
      "source": [
        "predictions = (predictions+1)/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUkNq4ZhpAyC",
        "outputId": "07814bee-5a1d-4762-ec20-1e5058ef5731"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9366666674613953"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy(predictions,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
