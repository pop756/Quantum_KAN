{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pennylane\n",
    "!pip install pykan\n",
    "!git clone https://github.com/pop756/Quantum_machine.git\n",
    "%cd Quantum_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "PCA_dim = 8\n",
    "CLS_num = 2\n",
    "device = 'cpu'\n",
    "\n",
    "\n",
    "with open('./data.pkl','rb') as file:\n",
    "    data = pickle.load(file)\n",
    "X = data['X']\n",
    "y = data['Y']\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "def Fit_to_quantum(X,PCA_dim):\n",
    "    pca = PCA(n_components=PCA_dim)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# PyTorch Tensor로 변환\n",
    "x_train_pca, y_train = torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_pca, y_test = torch.tensor(x_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "class Feature_data_loader(Dataset):\n",
    "    def __init__(self,x_train,y_train):\n",
    "        self.feature1 = x_train\n",
    "        temp = copy.deepcopy(x_train)\n",
    "        shuffle = torch.randperm(len(temp))\n",
    "        self.feature2 = temp[shuffle]\n",
    "        self.y1 = y_train\n",
    "        temp_y = copy.deepcopy(y_train)\n",
    "        self.y2 = temp_y[shuffle]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature1)\n",
    "    def __getitem__(self,idx):\n",
    "        input1 = self.feature1[idx]\n",
    "        input2 = self.feature2[idx]\n",
    "        if self.y1[idx] == self.y2[idx]:\n",
    "            label = torch.tensor(1.).float()\n",
    "        else:\n",
    "            label = torch.tensor(0.).float()\n",
    "        return [input1,input2],label\n",
    "\n",
    "\n",
    "# DataLoader 생성\n",
    "\n",
    "\n",
    "feature_loader = DataLoader(Feature_data_loader(x_train_pca, y_train.float()),batch_size=batch_size,shuffle=True)\n",
    "test_feature_loader = DataLoader(Feature_data_loader(x_test_pca, y_test.float()),batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(TensorDataset(x_train_pca, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test_pca, y_test), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN, create_dataset\n",
    "def reg(acts_scale,KAN_layer, factor=1,lamb_l1=1.,lamb_entropy=2.,lamb_coef=0.,lamb_coefdiff=0.):\n",
    "\n",
    "    def nonlinear(x, th=1e-16):\n",
    "        return (x < th) * x * factor + (x > th) * (x + (factor - 1) * th)\n",
    "\n",
    "    reg_ = 0.\n",
    "    for i in range(len(acts_scale)):\n",
    "        vec = acts_scale[i].reshape(-1, )\n",
    "\n",
    "        p = vec / torch.sum(vec)\n",
    "        l1 = torch.sum(nonlinear(vec))\n",
    "        entropy = - torch.sum(p * torch.log2(p + 1e-4))\n",
    "        reg_ += lamb_l1 * l1 + lamb_entropy * entropy  # both l1 and entropy\n",
    "\n",
    "    # regularize coefficient to encourage spline to be zero\n",
    "    for i in range(len(KAN_layer.act_fun)):\n",
    "        coeff_l1 = torch.sum(torch.mean(torch.abs(KAN_layer.act_fun[i].coef), dim=1))\n",
    "        coeff_diff_l1 = torch.sum(torch.mean(torch.abs(torch.diff(KAN_layer.act_fun[i].coef)), dim=1))\n",
    "        reg_ += lamb_coef * coeff_l1 + lamb_coefdiff * coeff_diff_l1\n",
    "\n",
    "    return reg_\n",
    "def accuracy(pred, true):\n",
    "    # 예측값이 로짓 혹은 확률값인 경우, 최대 값을 가진 인덱스를 구함 (가장 확률이 높은 클래스)\n",
    "    pred = pred.detach().cpu()\n",
    "    true = true.cpu()\n",
    "    try:\n",
    "        pred_labels = torch.argmax(pred, dim=1)\n",
    "    except:\n",
    "        pred_labels = torch.round(pred)\n",
    "    # 예측 레이블과 실제 레이블이 일치하는 경우를 계산\n",
    "    correct = (pred_labels == true).sum()\n",
    "    # 정확도를 계산\n",
    "    acc = correct / true.size(0)\n",
    "    return acc.item() \n",
    "\n",
    "class Early_stop_train():\n",
    "    def __init__(self,model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "\n",
    "        \n",
    "        self.loss_list = [1e100]\n",
    "        self.acc_list = []\n",
    "        self.stop_count = 0\n",
    "        \n",
    "    def train_model(self,train_loader,test_loader=None ,epochs=200,res = 10,lamb=0.,lamb_entropy=2.,device='cpu'):\n",
    "        #self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if self.stop_count>=res:\n",
    "                break\n",
    "            loss_val,_ = self.test(test_loader,device=device)\n",
    "            self.loss_list.append(loss_val)\n",
    "            \n",
    "            if self.loss_list[-1]>=np.min(self.loss_list[:-1]):\n",
    "                self.stop_count+=1\n",
    "            else:\n",
    "                self.optimal = copy.deepcopy(self.model.state_dict())\n",
    "                self.stop_count = 0\n",
    "            loss_list = []\n",
    "            acc_list = []\n",
    "            for X_train,y_train in train_loader:\n",
    "                if isinstance(X_train,list):\n",
    "                    for i,data in enumerate(X_train):\n",
    "                        X_train[i] = data.to(device)\n",
    "                else:\n",
    "                    X_train = data.to(device)\n",
    "                y_train = y_train.to(device)\n",
    "                    \n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(X_train)\n",
    "                reg_ = lamb*reg(self.model.KAN.acts_scale,self.model.KAN,lamb_entropy=lamb_entropy)\n",
    "                try:\n",
    "                    loss = self.criterion(output.squeeze(), y_train)+reg_\n",
    "                except:\n",
    "                    print(output)\n",
    "                    raise\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                loss_list.append(loss.item())\n",
    "                acc = accuracy(output,y_train)\n",
    "                acc_list.append(acc)\n",
    "                sys.stdout.write(f\"\\rEpoch {epoch+1} Loss {np.mean(loss_list):4f} acc : {np.mean(acc_list):4f} reg : {reg_:4f} stop count : {self.stop_count} lamb : {lamb} lamb_enp : {lamb_entropy}\")\n",
    "        self.model.load_state_dict(self.optimal)\n",
    "    def test(self,test_loader,device='cpu'):\n",
    "        if test_loader is None:\n",
    "            return 0,0\n",
    "        else:\n",
    "            #self.model.eval()\n",
    "            test_loss = 0\n",
    "            correct = 0\n",
    "            with torch.no_grad():\n",
    "                for X_train, target in test_loader:\n",
    "                    if isinstance(X_train,list):\n",
    "                        for i,data in enumerate(X_train):\n",
    "                            X_train[i] = data.to(device)\n",
    "                    else:\n",
    "                        X_train = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    output = self.model(X_train)\n",
    "\n",
    "                    test_loss += self.criterion(output.squeeze(), target).item()\n",
    "                    \n",
    "                    correct += accuracy(output,target)*len(output)\n",
    "\n",
    "            print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)')\n",
    "            return test_loss,correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import math\n",
    "lamb_list = [0.005*(i+1) for i in range(100)]\n",
    "lamb_entropy_list = [0.2*(i+1) for i in range(10)]\n",
    "grid=1\n",
    "\n",
    "result_dict  = []\n",
    "\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "\"\"\"\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=PCA_dim)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.PCA_dim, random_state=seed)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\"\"\"\n",
    "\n",
    "# Pennylane 장치 설정\n",
    "dev = qml.device(\"default.qubit\", wires=PCA_dim)\n",
    "\n",
    "\n",
    "def ZZFeatureMapLayer(features, wires):\n",
    "    \"\"\"사용자 정의 ZZFeatureMap 레이어\"\"\"\n",
    "    index = 0\n",
    "    for i in wires:\n",
    "        qml.Hadamard(wires=i)\n",
    "        qml.RZ(features[:,index], wires=i)\n",
    "        index += 1\n",
    "\n",
    "    for j in range(0, len(wires)-1):\n",
    "        qml.CNOT(wires=[j, j+1])\n",
    "        qml.RZ((features[:,index]), wires=j+1)\n",
    "        qml.CNOT(wires=[j, j+1])\n",
    "        index+=1\n",
    "\n",
    "def ansatz(params):\n",
    "    for j in range(len(params)):\n",
    "        # 각 큐비트에 대해 RX, RY, RZ 회전 적용\n",
    "        for i in range(len(params[0])):\n",
    "            qml.RY(params[j, i, 0], wires=i)\n",
    "            qml.RZ(params[j, i, 1], wires=i)\n",
    "            \n",
    "        # 인접한 큐비트 간 CNOT 게이트로 엔탱글링\n",
    "        if j == len(params)-1:\n",
    "            pass\n",
    "        else:\n",
    "            for i in range(len(params[0])-1):\n",
    "                qml.CNOT(wires=[i, i+1])\n",
    "\n",
    "\n",
    "# 양자 레이어 정의\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def QuantumLayer(features,params):\n",
    "    ZZFeatureMapLayer(features, wires=range(PCA_dim))\n",
    "    ansatz(params)\n",
    "    return qml.probs(wires=range(math.ceil(math.log2(CLS_num))))\n",
    "\n",
    "\n",
    "## 양자 커널\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def Kernal(features1,features2):\n",
    "    ZZFeatureMapLayer(features1, wires=range(PCA_dim))\n",
    "    qml.adjoint(ZZFeatureMapLayer)(features2,wires=range(PCA_dim))\n",
    "    return qml.probs(wires=range(PCA_dim))\n",
    "\n",
    "\n",
    "class Feature_model(nn.Module):\n",
    "    def __init__(self,grid,device='cpu'):\n",
    "        super(Feature_model,self).__init__()\n",
    "        KAN_model = KAN([PCA_dim,2*PCA_dim+1,PCA_dim*2-1],grid=grid,device=device)\n",
    "        self.KAN = KAN_model\n",
    "        self.Kernal = Kernal\n",
    "    def forward(self,inputs):\n",
    "        epsilon = 1e-6\n",
    "        input1 = inputs[0]\n",
    "        #input1_copy = input1.clone().detach().requires_grad_(True)\n",
    "        input2 = inputs[1]\n",
    "        #input2_copy = input2.clone().detach().requires_grad_(True)\n",
    "        input1 = nn.Sigmoid()(self.KAN(input1))*np.pi\n",
    "        #input1 = torch.concat([input1,input1_copy],dim=1)\n",
    "        input2 = nn.Sigmoid()(self.KAN(input2))*np.pi\n",
    "        #input2 = torch.concat([input2,input2_copy],dim=1)\n",
    "        output = self.Kernal(input1,input2)\n",
    "        output = output.type(torch.float32)\n",
    "        \n",
    "        return output[:,0].clamp(min=epsilon, max=1-epsilon)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# 하이브리드 모델 정의\n",
    "\n",
    "    \n",
    "def search(params):\n",
    "    print(f\"Test set \\n lamb : {params['lamb']} \\n lamb_enp :  {params['lamb_entropy']} \\n grid : {params['grid']}\")\n",
    "    grid = params['grid']\n",
    "    feature_model = Feature_model(grid,device); criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "    optimizer = optim.Adam(feature_model.parameters(), lr=0.01)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 모델 학습 및 평가\n",
    "    train_process = Early_stop_train(feature_model, optimizer, criterion)\n",
    "    train_process.train_model(feature_loader,test_feature_loader,epochs=50,res=15,lamb=params['lamb'],lamb_entropy=params['lamb_entropy'])\n",
    "    #feature_model.KAN.plot(beta=3,scale=2)\n",
    "    feature_model.KAN = feature_model.KAN.prune()\n",
    "    res,pretrain_acc = train_process.test(test_feature_loader)\n",
    "    print(f\"\\n Pretrain acc : {pretrain_acc}\")\n",
    "    class HybridModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(HybridModel, self).__init__()\n",
    "            self.KAN = feature_model.KAN\n",
    "            \n",
    "            self.quantum_layer = QuantumLayer\n",
    "            self.Q_params = nn.Parameter((torch.rand([PCA_dim,PCA_dim,2])*2-1)*np.pi,requires_grad=True)\n",
    "        def forward(self, x):\n",
    "            x = nn.Sigmoid()(self.KAN(x))*np.pi\n",
    "            quantum_output = self.quantum_layer(x,self.Q_params)\n",
    "            quantum_output = quantum_output.type(torch.float32)\n",
    "            return torch.log(quantum_output)\n",
    "    model = HybridModel(); criterion = nn.NLLLoss()\n",
    "\n",
    "    for param in model.KAN.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    optimizer = optim.Adam([model.Q_params], lr=0.01)\n",
    "    print(\"\\n\\nTest start\\n\\n\")\n",
    "    train_process = Early_stop_train(model, optimizer, criterion)\n",
    "    train_process.train_model(train_loader,test_loader,epochs=25,lamb=0)\n",
    "\n",
    "    _,acc = train_process.test(test_loader)\n",
    "    print(f\"Test Accuracy: {acc:.2f}\")\n",
    "    result_dict.append({\"lamb\":params['lamb'],\"lamb_entropy\":f\"{params['lamb_entropy']:.1f}\", \"grid\":grid,\"acc\" : acc,\"pretrain_acc\" : pretrain_acc})\n",
    "    result = pd.DataFrame(result_dict)\n",
    "    result.to_csv('./Results.csv',index=False)\n",
    "    return -res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set \n",
      " lamb : 0.037374597741005836 \n",
      " lamb_enp :  0.2926390022865375 \n",
      " grid : 1\n",
      "\n",
      "Test set: Average loss: 5.3972, Accuracy: 171.0/300 (57%)\n",
      "Epoch 1 Loss 1.794629 acc : 0.538163 reg : 0.797574 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 4.0435, Accuracy: 154.99999976158142/300 (52%)\n",
      "Epoch 2 Loss 1.409117 acc : 0.534470 reg : 0.540725 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.5478, Accuracy: 177.00000095367432/300 (59%)\n",
      "Epoch 3 Loss 1.219012 acc : 0.560795 reg : 0.452706 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.4662, Accuracy: 178.00000095367432/300 (59%)\n",
      "Epoch 4 Loss 1.107672 acc : 0.590057 reg : 0.381063 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.2857, Accuracy: 189.00000095367432/300 (63%)\n",
      "Epoch 5 Loss 1.033467 acc : 0.627367 reg : 0.363048 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.2109, Accuracy: 190.00000095367432/300 (63%)\n",
      "Epoch 6 Loss 0.979687 acc : 0.659659 reg : 0.339688 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.1996, Accuracy: 191.99999904632568/300 (64%)\n",
      "Epoch 7 Loss 0.933495 acc : 0.674621 reg : 0.335547 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.0852, Accuracy: 201.00000095367432/300 (67%)\n",
      "Epoch 8 Loss 0.899036 acc : 0.699811 reg : 0.324199 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.0692, Accuracy: 203.99999904632568/300 (68%)\n",
      "Epoch 9 Loss 0.863831 acc : 0.718277 reg : 0.325569 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 3.0536, Accuracy: 209.99999904632568/300 (70%)\n",
      "Epoch 10 Loss 0.851450 acc : 0.729072 reg : 0.296233 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.8452, Accuracy: 222.0/300 (74%)\n",
      "Epoch 11 Loss 0.833165 acc : 0.742708 reg : 0.289912 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.7686, Accuracy: 221.00000071525574/300 (74%)\n",
      "Epoch 12 Loss 0.809289 acc : 0.760417 reg : 0.308053 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.6798, Accuracy: 226.99999976158142/300 (76%)\n",
      "Epoch 13 Loss 0.805822 acc : 0.763636 reg : 0.289506 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.6907, Accuracy: 228.99999976158142/300 (76%)\n",
      "Epoch 14 Loss 0.784917 acc : 0.776894 reg : 0.286697 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.5470, Accuracy: 234.00000071525574/300 (78%)\n",
      "Epoch 15 Loss 0.771128 acc : 0.787027 reg : 0.275549 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.5146, Accuracy: 231.99999976158142/300 (77%)\n",
      "Epoch 16 Loss 0.762855 acc : 0.785701 reg : 0.274466 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.5691, Accuracy: 236.99999976158142/300 (79%)\n",
      "Epoch 17 Loss 0.759121 acc : 0.787311 reg : 0.283053 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.5061, Accuracy: 232.9999988079071/300 (78%)\n",
      "Epoch 18 Loss 0.749298 acc : 0.792992 reg : 0.279808 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.4612, Accuracy: 238.9999988079071/300 (80%)\n",
      "Epoch 19 Loss 0.734189 acc : 0.794318 reg : 0.278248 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3485, Accuracy: 241.9999988079071/300 (81%)\n",
      "Epoch 20 Loss 0.729013 acc : 0.796970 reg : 0.272900 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.5328, Accuracy: 235.99999976158142/300 (79%)\n",
      "Epoch 21 Loss 0.720922 acc : 0.796117 reg : 0.253077 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3536, Accuracy: 240.00000047683716/300 (80%)\n",
      "Epoch 22 Loss 0.713395 acc : 0.803030 reg : 0.257783 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3370, Accuracy: 244.9999988079071/300 (82%)\n",
      "Epoch 23 Loss 0.707028 acc : 0.805492 reg : 0.246744 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3241, Accuracy: 245.00000047683716/300 (82%)\n",
      "Epoch 24 Loss 0.695283 acc : 0.798011 reg : 0.242480 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2985, Accuracy: 244.9999988079071/300 (82%)\n",
      "Epoch 25 Loss 0.687927 acc : 0.802652 reg : 0.228443 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3657, Accuracy: 243.00000047683716/300 (81%)\n",
      "Epoch 26 Loss 0.686169 acc : 0.802746 reg : 0.240692 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2768, Accuracy: 245.9999988079071/300 (82%)\n",
      "Epoch 27 Loss 0.676967 acc : 0.809754 reg : 0.243836 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3649, Accuracy: 240.99999976158142/300 (80%)\n",
      "Epoch 28 Loss 0.677298 acc : 0.801136 reg : 0.256806 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3177, Accuracy: 244.9999988079071/300 (82%)\n",
      "Epoch 29 Loss 0.677947 acc : 0.804451 reg : 0.243096 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2437, Accuracy: 246.99999976158142/300 (82%)\n",
      "Epoch 30 Loss 0.670018 acc : 0.808617 reg : 0.222335 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.4485, Accuracy: 238.9999988079071/300 (80%)\n",
      "Epoch 31 Loss 0.658356 acc : 0.808239 reg : 0.225651 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2159, Accuracy: 246.9999988079071/300 (82%)\n",
      "Epoch 32 Loss 0.651534 acc : 0.818655 reg : 0.231140 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.4717, Accuracy: 241.00000047683716/300 (80%)\n",
      "Epoch 33 Loss 0.663999 acc : 0.798864 reg : 0.232819 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3613, Accuracy: 244.9999988079071/300 (82%)\n",
      "Epoch 34 Loss 0.653050 acc : 0.811364 reg : 0.224106 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1992, Accuracy: 249.9999988079071/300 (83%)\n",
      "Epoch 35 Loss 0.663096 acc : 0.814015 reg : 0.233867 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2479, Accuracy: 244.00000047683716/300 (81%)\n",
      "Epoch 36 Loss 0.658970 acc : 0.803314 reg : 0.236254 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2718, Accuracy: 246.00000047683716/300 (82%)\n",
      "Epoch 37 Loss 0.637799 acc : 0.819981 reg : 0.218579 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2238, Accuracy: 250.00000047683716/300 (83%)\n",
      "Epoch 38 Loss 0.629168 acc : 0.823958 reg : 0.221809 stop count : 3 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1513, Accuracy: 252.00000047683716/300 (84%)\n",
      "Epoch 39 Loss 0.633658 acc : 0.813068 reg : 0.227010 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2356, Accuracy: 251.00000047683716/300 (84%)\n",
      "Epoch 40 Loss 0.635446 acc : 0.815436 reg : 0.232048 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2452, Accuracy: 251.9999988079071/300 (84%)\n",
      "Epoch 41 Loss 0.636692 acc : 0.815720 reg : 0.222355 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2657, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 42 Loss 0.656829 acc : 0.795833 reg : 0.221660 stop count : 3 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3974, Accuracy: 244.9999988079071/300 (82%)\n",
      "Epoch 43 Loss 0.650853 acc : 0.803030 reg : 0.223531 stop count : 4 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2859, Accuracy: 246.9999988079071/300 (82%)\n",
      "Epoch 44 Loss 0.635053 acc : 0.811080 reg : 0.210456 stop count : 5 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.3532, Accuracy: 243.9999988079071/300 (81%)\n",
      "Epoch 45 Loss 0.636397 acc : 0.802841 reg : 0.213673 stop count : 6 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.2196, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 46 Loss 0.612808 acc : 0.819602 reg : 0.218075 stop count : 7 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1715, Accuracy: 249.00000047683716/300 (83%)\n",
      "Epoch 47 Loss 0.610948 acc : 0.823769 reg : 0.204252 stop count : 8 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1444, Accuracy: 256.00000047683716/300 (85%)\n",
      "Epoch 48 Loss 0.622550 acc : 0.821117 reg : 0.224166 stop count : 0 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1471, Accuracy: 253.00000047683716/300 (84%)\n",
      "Epoch 49 Loss 0.606272 acc : 0.818939 reg : 0.215061 stop count : 1 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1997, Accuracy: 251.99999952316284/300 (84%)\n",
      "Epoch 50 Loss 0.599385 acc : 0.821496 reg : 0.205700 stop count : 2 lamb : 0.037374597741005836 lamb_enp : 0.2926390022865375\n",
      "Test set: Average loss: 2.1437, Accuracy: 256.00000047683716/300 (85%)\n",
      "\n",
      " Pretrain acc : 256.00000047683716\n",
      "\n",
      "\n",
      "Test start\n",
      "\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.6681, Accuracy: 147.0/300 (49%)\n",
      "Epoch 1 Loss 0.705216 acc : 0.502557 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 3.4120, Accuracy: 147.0/300 (49%)\n",
      "Epoch 2 Loss 0.662506 acc : 0.773485 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 3.1836, Accuracy: 266.99999952316284/300 (89%)\n",
      "Epoch 3 Loss 0.619401 acc : 0.872917 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.9628, Accuracy: 267.99999952316284/300 (89%)\n",
      "Epoch 4 Loss 0.572981 acc : 0.891667 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.6965, Accuracy: 277.0000002384186/300 (92%)\n",
      "Epoch 5 Loss 0.514679 acc : 0.913258 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.3738, Accuracy: 277.0000002384186/300 (92%)\n",
      "Epoch 6 Loss 0.450312 acc : 0.915720 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.0495, Accuracy: 278.0000002384186/300 (93%)\n",
      "Epoch 7 Loss 0.389407 acc : 0.920076 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.8032, Accuracy: 278.0000002384186/300 (93%)\n",
      "Epoch 8 Loss 0.349282 acc : 0.911269 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.6680, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 9 Loss 0.327405 acc : 0.908712 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.6116, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 10 Loss 0.317769 acc : 0.907292 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5825, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 11 Loss 0.311763 acc : 0.904261 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5627, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 12 Loss 0.308024 acc : 0.904545 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5467, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 13 Loss 0.304767 acc : 0.904261 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5342, Accuracy: 273.0000011920929/300 (91%)\n",
      "Epoch 14 Loss 0.301767 acc : 0.902841 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5193, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 15 Loss 0.299026 acc : 0.904356 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5041, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 16 Loss 0.296501 acc : 0.904167 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4910, Accuracy: 273.0000011920929/300 (91%)\n",
      "Epoch 17 Loss 0.294021 acc : 0.904167 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4782, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 18 Loss 0.291609 acc : 0.902936 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4684, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 19 Loss 0.289757 acc : 0.905587 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4574, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 20 Loss 0.288372 acc : 0.905682 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4458, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 21 Loss 0.284976 acc : 0.908807 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4266, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 22 Loss 0.281647 acc : 0.905777 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4057, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 23 Loss 0.275825 acc : 0.912973 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3785, Accuracy: 275.0000011920929/300 (92%)\n",
      "Epoch 24 Loss 0.271154 acc : 0.914205 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3546, Accuracy: 275.0000011920929/300 (92%)\n",
      "Epoch 25 Loss 0.265276 acc : 0.911648 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3546, Accuracy: 275.0000011920929/300 (92%)\n",
      "Test Accuracy: 275.00\n",
      "Test set \n",
      " lamb : 0.09272156226271577 \n",
      " lamb_enp :  0.9409596236335218 \n",
      " grid : 2\n",
      "\n",
      "Test set: Average loss: 5.3972, Accuracy: 171.0/300 (57%)\n",
      "Epoch 1 Loss 3.752105 acc : 0.518466 reg : 2.404360 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 4.5797, Accuracy: 169.99999952316284/300 (57%)\n",
      "Epoch 2 Loss 2.901692 acc : 0.512973 reg : 1.868351 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.7934, Accuracy: 168.99999952316284/300 (56%)\n",
      "Epoch 3 Loss 2.514004 acc : 0.530871 reg : 1.610277 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.7087, Accuracy: 169.99999952316284/300 (57%)\n",
      "Epoch 4 Loss 2.260577 acc : 0.536837 reg : 1.357229 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.6438, Accuracy: 173.99999928474426/300 (58%)\n",
      "Epoch 5 Loss 2.086097 acc : 0.558902 reg : 1.261130 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.6079, Accuracy: 182.99999928474426/300 (61%)\n",
      "Epoch 6 Loss 1.946841 acc : 0.561458 reg : 1.142528 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5663, Accuracy: 184.99999928474426/300 (62%)\n",
      "Epoch 7 Loss 1.843229 acc : 0.575852 reg : 1.071609 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5078, Accuracy: 188.00000095367432/300 (63%)\n",
      "Epoch 8 Loss 1.762035 acc : 0.597064 reg : 1.027701 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4991, Accuracy: 186.00000023841858/300 (62%)\n",
      "Epoch 9 Loss 1.703056 acc : 0.607481 reg : 0.988904 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4839, Accuracy: 182.00000023841858/300 (61%)\n",
      "Epoch 10 Loss 1.657229 acc : 0.607292 reg : 0.909076 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4889, Accuracy: 186.00000023841858/300 (62%)\n",
      "Epoch 11 Loss 1.623631 acc : 0.610795 reg : 0.880359 stop count : 1 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4908, Accuracy: 183.99999928474426/300 (61%)\n",
      "Epoch 12 Loss 1.590008 acc : 0.595360 reg : 0.903253 stop count : 2 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4961, Accuracy: 189.00000095367432/300 (63%)\n",
      "Epoch 13 Loss 1.564109 acc : 0.609848 reg : 0.825088 stop count : 3 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4953, Accuracy: 190.00000095367432/300 (63%)\n",
      "Epoch 14 Loss 1.538071 acc : 0.621402 reg : 0.818156 stop count : 4 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4742, Accuracy: 187.0/300 (62%)\n",
      "Epoch 15 Loss 1.513842 acc : 0.615625 reg : 0.775503 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4990, Accuracy: 186.0000011920929/300 (62%)\n",
      "Epoch 16 Loss 1.484827 acc : 0.615720 reg : 0.749353 stop count : 1 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4813, Accuracy: 190.0/300 (63%)\n",
      "Epoch 17 Loss 1.463095 acc : 0.634375 reg : 0.766824 stop count : 2 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5304, Accuracy: 191.99999928474426/300 (64%)\n",
      "Epoch 18 Loss 1.435958 acc : 0.627083 reg : 0.734206 stop count : 3 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5367, Accuracy: 189.99999928474426/300 (63%)\n",
      "Epoch 19 Loss 1.424489 acc : 0.626894 reg : 0.740707 stop count : 4 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5123, Accuracy: 195.00000095367432/300 (65%)\n",
      "Epoch 20 Loss 1.410184 acc : 0.628883 reg : 0.713988 stop count : 5 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5948, Accuracy: 189.00000023841858/300 (63%)\n",
      "Epoch 21 Loss 1.392165 acc : 0.628693 reg : 0.695909 stop count : 6 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5827, Accuracy: 190.99999928474426/300 (64%)\n",
      "Epoch 22 Loss 1.375537 acc : 0.651326 reg : 0.677081 stop count : 7 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5623, Accuracy: 188.99999928474426/300 (63%)\n",
      "Epoch 23 Loss 1.365136 acc : 0.641004 reg : 0.654318 stop count : 8 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5920, Accuracy: 189.99999928474426/300 (63%)\n",
      "Epoch 24 Loss 1.363097 acc : 0.648485 reg : 0.659943 stop count : 9 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5518, Accuracy: 186.99999928474426/300 (62%)\n",
      "Epoch 25 Loss 1.343667 acc : 0.645928 reg : 0.604644 stop count : 10 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5474, Accuracy: 190.99999928474426/300 (64%)\n",
      "Epoch 26 Loss 1.326219 acc : 0.645644 reg : 0.622212 stop count : 11 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5269, Accuracy: 189.99999928474426/300 (63%)\n",
      "Epoch 27 Loss 1.319353 acc : 0.664110 reg : 0.633229 stop count : 12 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.5145, Accuracy: 193.00000095367432/300 (64%)\n",
      "Epoch 28 Loss 1.322528 acc : 0.656818 reg : 0.637962 stop count : 13 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4275, Accuracy: 199.99999904632568/300 (67%)\n",
      "Epoch 29 Loss 1.282639 acc : 0.666193 reg : 0.631731 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4174, Accuracy: 196.0/300 (65%)\n",
      "Epoch 30 Loss 1.261124 acc : 0.668845 reg : 0.599817 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4082, Accuracy: 199.00000095367432/300 (66%)\n",
      "Epoch 31 Loss 1.236521 acc : 0.678883 reg : 0.588173 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4326, Accuracy: 198.00000095367432/300 (66%)\n",
      "Epoch 32 Loss 1.226647 acc : 0.691383 reg : 0.587817 stop count : 1 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3065, Accuracy: 206.00000095367432/300 (69%)\n",
      "Epoch 33 Loss 1.212193 acc : 0.689867 reg : 0.582827 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3988, Accuracy: 204.00000095367432/300 (68%)\n",
      "Epoch 34 Loss 1.196949 acc : 0.700189 reg : 0.555851 stop count : 1 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3986, Accuracy: 204.0/300 (68%)\n",
      "Epoch 35 Loss 1.192399 acc : 0.700568 reg : 0.554508 stop count : 2 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4136, Accuracy: 202.0/300 (67%)\n",
      "Epoch 36 Loss 1.183877 acc : 0.696307 reg : 0.559084 stop count : 3 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3484, Accuracy: 203.0/300 (68%)\n",
      "Epoch 37 Loss 1.176242 acc : 0.702841 reg : 0.530076 stop count : 4 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3564, Accuracy: 205.0/300 (68%)\n",
      "Epoch 38 Loss 1.175098 acc : 0.700189 reg : 0.553528 stop count : 5 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4062, Accuracy: 205.99999904632568/300 (69%)\n",
      "Epoch 39 Loss 1.174702 acc : 0.695549 reg : 0.545948 stop count : 6 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3222, Accuracy: 205.0/300 (68%)\n",
      "Epoch 40 Loss 1.167444 acc : 0.700947 reg : 0.557437 stop count : 7 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3956, Accuracy: 206.99999904632568/300 (69%)\n",
      "Epoch 41 Loss 1.171105 acc : 0.716951 reg : 0.550785 stop count : 8 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3116, Accuracy: 205.99999904632568/300 (69%)\n",
      "Epoch 42 Loss 1.168376 acc : 0.682955 reg : 0.543390 stop count : 9 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4536, Accuracy: 205.99999904632568/300 (69%)\n",
      "Epoch 43 Loss 1.167621 acc : 0.704640 reg : 0.542930 stop count : 10 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.2685, Accuracy: 208.99999904632568/300 (70%)\n",
      "Epoch 44 Loss 1.154971 acc : 0.714489 reg : 0.499612 stop count : 0 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3766, Accuracy: 210.00000071525574/300 (70%)\n",
      "Epoch 45 Loss 1.147430 acc : 0.714110 reg : 0.521105 stop count : 1 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3196, Accuracy: 206.99999904632568/300 (69%)\n",
      "Epoch 46 Loss 1.155388 acc : 0.709848 reg : 0.539206 stop count : 2 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3034, Accuracy: 210.00000071525574/300 (70%)\n",
      "Epoch 47 Loss 1.142734 acc : 0.711932 reg : 0.506222 stop count : 3 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.4336, Accuracy: 208.0/300 (69%)\n",
      "Epoch 48 Loss 1.146914 acc : 0.711837 reg : 0.541430 stop count : 4 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.2944, Accuracy: 209.00000071525574/300 (70%)\n",
      "Epoch 49 Loss 1.140757 acc : 0.723485 reg : 0.532687 stop count : 5 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.3197, Accuracy: 208.99999904632568/300 (70%)\n",
      "Epoch 50 Loss 1.144631 acc : 0.700947 reg : 0.523259 stop count : 6 lamb : 0.09272156226271577 lamb_enp : 0.9409596236335218\n",
      "Test set: Average loss: 3.2685, Accuracy: 208.99999904632568/300 (70%)\n",
      "\n",
      " Pretrain acc : 208.99999904632568\n",
      "\n",
      "\n",
      "Test start\n",
      "\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.4619, Accuracy: 150.99999964237213/300 (50%)\n",
      "Epoch 1 Loss 0.635182 acc : 0.733144 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.8712, Accuracy: 251.00000071525574/300 (84%)\n",
      "Epoch 2 Loss 0.537879 acc : 0.838636 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.5335, Accuracy: 253.00000071525574/300 (84%)\n",
      "Epoch 3 Loss 0.483041 acc : 0.842708 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.2997, Accuracy: 252.00000071525574/300 (84%)\n",
      "Epoch 4 Loss 0.443432 acc : 0.841856 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.1489, Accuracy: 253.00000071525574/300 (84%)\n",
      "Epoch 5 Loss 0.421806 acc : 0.839962 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.0554, Accuracy: 252.99999976158142/300 (84%)\n",
      "Epoch 6 Loss 0.408419 acc : 0.840246 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.0200, Accuracy: 253.00000071525574/300 (84%)\n",
      "Epoch 7 Loss 0.402678 acc : 0.839015 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9893, Accuracy: 252.99999976158142/300 (84%)\n",
      "Epoch 8 Loss 0.400914 acc : 0.839583 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9857, Accuracy: 252.99999976158142/300 (84%)\n",
      "Epoch 9 Loss 0.397071 acc : 0.840341 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9824, Accuracy: 252.99999976158142/300 (84%)\n",
      "Epoch 10 Loss 0.395535 acc : 0.839962 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9714, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 11 Loss 0.394999 acc : 0.838731 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9658, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 12 Loss 0.393062 acc : 0.839015 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9727, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 13 Loss 0.392847 acc : 0.840246 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9651, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 14 Loss 0.392886 acc : 0.837311 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9568, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 15 Loss 0.392816 acc : 0.839773 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9567, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 16 Loss 0.393821 acc : 0.839678 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9659, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 17 Loss 0.393024 acc : 0.838542 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9521, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 18 Loss 0.392051 acc : 0.840057 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9595, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 19 Loss 0.392395 acc : 0.839773 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9631, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 20 Loss 0.392432 acc : 0.838352 reg : 0.000000 stop count : 2 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9560, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 21 Loss 0.392497 acc : 0.839962 reg : 0.000000 stop count : 3 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9515, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 22 Loss 0.391456 acc : 0.839015 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9634, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 23 Loss 0.392217 acc : 0.843371 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9562, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 24 Loss 0.392465 acc : 0.838636 reg : 0.000000 stop count : 2 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9561, Accuracy: 253.99999976158142/300 (85%)\n",
      "Epoch 25 Loss 0.392805 acc : 0.839678 reg : 0.000000 stop count : 3 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9515, Accuracy: 253.99999976158142/300 (85%)\n",
      "Test Accuracy: 254.00\n",
      "Test set \n",
      " lamb : 0.051278451674345873 \n",
      " lamb_enp :  0.08184797072635908 \n",
      " grid : 2\n",
      "\n",
      "Test set: Average loss: 5.3972, Accuracy: 171.0/300 (57%)\n",
      "Epoch 1 Loss 1.934748 acc : 0.519697 reg : 0.853721 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 4.1584, Accuracy: 159.99999952316284/300 (53%)\n",
      "Epoch 2 Loss 1.443500 acc : 0.520265 reg : 0.549081 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.6373, Accuracy: 172.99999952316284/300 (58%)\n",
      "Epoch 3 Loss 1.229261 acc : 0.556439 reg : 0.433212 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.5642, Accuracy: 175.99999928474426/300 (59%)\n",
      "Epoch 4 Loss 1.104962 acc : 0.569981 reg : 0.353449 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.3536, Accuracy: 189.0/300 (63%)\n",
      "Epoch 5 Loss 1.025849 acc : 0.603220 reg : 0.336143 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.2650, Accuracy: 187.00000095367432/300 (62%)\n",
      "Epoch 6 Loss 0.968620 acc : 0.646780 reg : 0.301965 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.2863, Accuracy: 188.00000095367432/300 (63%)\n",
      "Epoch 7 Loss 0.916734 acc : 0.674527 reg : 0.303878 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.1647, Accuracy: 196.99999928474426/300 (66%)\n",
      "Epoch 8 Loss 0.882603 acc : 0.688447 reg : 0.291301 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.1511, Accuracy: 202.00000095367432/300 (67%)\n",
      "Epoch 9 Loss 0.843737 acc : 0.711080 reg : 0.290511 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 3.1452, Accuracy: 204.0/300 (68%)\n",
      "Epoch 10 Loss 0.829694 acc : 0.716288 reg : 0.256618 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.9815, Accuracy: 212.99999928474426/300 (71%)\n",
      "Epoch 11 Loss 0.813628 acc : 0.728409 reg : 0.246260 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.8875, Accuracy: 217.99999904632568/300 (73%)\n",
      "Epoch 12 Loss 0.792143 acc : 0.747538 reg : 0.272309 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.7603, Accuracy: 226.00000071525574/300 (75%)\n",
      "Epoch 13 Loss 0.789169 acc : 0.742045 reg : 0.250231 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.8755, Accuracy: 223.00000071525574/300 (74%)\n",
      "Epoch 14 Loss 0.770706 acc : 0.765530 reg : 0.248873 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.6424, Accuracy: 229.00000071525574/300 (76%)\n",
      "Epoch 15 Loss 0.758427 acc : 0.766856 reg : 0.236073 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.6680, Accuracy: 224.00000071525574/300 (75%)\n",
      "Epoch 16 Loss 0.747769 acc : 0.769886 reg : 0.237782 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.6416, Accuracy: 229.9999988079071/300 (77%)\n",
      "Epoch 17 Loss 0.748452 acc : 0.767235 reg : 0.256397 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.6305, Accuracy: 231.99999976158142/300 (77%)\n",
      "Epoch 18 Loss 0.731308 acc : 0.777178 reg : 0.247788 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.5004, Accuracy: 236.99999976158142/300 (79%)\n",
      "Epoch 19 Loss 0.714210 acc : 0.785606 reg : 0.241190 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4147, Accuracy: 235.99999976158142/300 (79%)\n",
      "Epoch 20 Loss 0.712945 acc : 0.785606 reg : 0.236068 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.5747, Accuracy: 228.99999904632568/300 (76%)\n",
      "Epoch 21 Loss 0.714153 acc : 0.777652 reg : 0.214219 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4099, Accuracy: 237.9999988079071/300 (79%)\n",
      "Epoch 22 Loss 0.706500 acc : 0.792803 reg : 0.225758 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4658, Accuracy: 240.9999988079071/300 (80%)\n",
      "Epoch 23 Loss 0.694780 acc : 0.798390 reg : 0.211536 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4233, Accuracy: 240.9999988079071/300 (80%)\n",
      "Epoch 24 Loss 0.684188 acc : 0.793561 reg : 0.203854 stop count : 2 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3510, Accuracy: 245.9999988079071/300 (82%)\n",
      "Epoch 25 Loss 0.676951 acc : 0.795360 reg : 0.191393 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.5114, Accuracy: 236.9999988079071/300 (79%)\n",
      "Epoch 26 Loss 0.677827 acc : 0.787027 reg : 0.207618 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3943, Accuracy: 238.9999988079071/300 (80%)\n",
      "Epoch 27 Loss 0.665396 acc : 0.795360 reg : 0.210744 stop count : 2 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4552, Accuracy: 237.99999976158142/300 (79%)\n",
      "Epoch 28 Loss 0.656654 acc : 0.798295 reg : 0.219607 stop count : 3 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3530, Accuracy: 243.99999976158142/300 (81%)\n",
      "Epoch 29 Loss 0.663791 acc : 0.803030 reg : 0.212018 stop count : 4 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3248, Accuracy: 242.99999976158142/300 (81%)\n",
      "Epoch 30 Loss 0.654880 acc : 0.796875 reg : 0.181414 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4892, Accuracy: 238.9999988079071/300 (80%)\n",
      "Epoch 31 Loss 0.637260 acc : 0.795170 reg : 0.176919 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2954, Accuracy: 245.9999988079071/300 (82%)\n",
      "Epoch 32 Loss 0.637651 acc : 0.801515 reg : 0.190249 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.5105, Accuracy: 237.99999976158142/300 (79%)\n",
      "Epoch 33 Loss 0.644714 acc : 0.790341 reg : 0.182439 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3239, Accuracy: 243.99999976158142/300 (81%)\n",
      "Epoch 34 Loss 0.633390 acc : 0.804167 reg : 0.177124 stop count : 2 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3248, Accuracy: 242.99999976158142/300 (81%)\n",
      "Epoch 35 Loss 0.617737 acc : 0.808333 reg : 0.169692 stop count : 3 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3714, Accuracy: 241.9999988079071/300 (81%)\n",
      "Epoch 36 Loss 0.619128 acc : 0.803314 reg : 0.182617 stop count : 4 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3375, Accuracy: 242.9999988079071/300 (81%)\n",
      "Epoch 37 Loss 0.610784 acc : 0.807102 reg : 0.161576 stop count : 5 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2510, Accuracy: 245.00000071525574/300 (82%)\n",
      "Epoch 38 Loss 0.603707 acc : 0.815152 reg : 0.168579 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2561, Accuracy: 244.00000071525574/300 (81%)\n",
      "Epoch 39 Loss 0.601830 acc : 0.812973 reg : 0.175007 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2688, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 40 Loss 0.601584 acc : 0.806629 reg : 0.183004 stop count : 2 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2464, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 41 Loss 0.593399 acc : 0.819981 reg : 0.169326 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2266, Accuracy: 250.9999988079071/300 (84%)\n",
      "Epoch 42 Loss 0.598290 acc : 0.818466 reg : 0.160040 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.1933, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 43 Loss 0.605399 acc : 0.804356 reg : 0.165338 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2311, Accuracy: 251.9999988079071/300 (84%)\n",
      "Epoch 44 Loss 0.587416 acc : 0.814110 reg : 0.166289 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.3475, Accuracy: 241.99999976158142/300 (81%)\n",
      "Epoch 45 Loss 0.609494 acc : 0.805398 reg : 0.163337 stop count : 2 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.4128, Accuracy: 240.99999976158142/300 (80%)\n",
      "Epoch 46 Loss 0.608020 acc : 0.794223 reg : 0.176231 stop count : 3 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2670, Accuracy: 246.9999988079071/300 (82%)\n",
      "Epoch 47 Loss 0.586949 acc : 0.812216 reg : 0.154755 stop count : 4 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2218, Accuracy: 249.00000047683716/300 (83%)\n",
      "Epoch 48 Loss 0.572330 acc : 0.825379 reg : 0.169748 stop count : 5 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.1440, Accuracy: 248.9999988079071/300 (83%)\n",
      "Epoch 49 Loss 0.567038 acc : 0.821686 reg : 0.168320 stop count : 0 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.2189, Accuracy: 247.9999988079071/300 (83%)\n",
      "Epoch 50 Loss 0.568371 acc : 0.826136 reg : 0.158521 stop count : 1 lamb : 0.051278451674345873 lamb_enp : 0.08184797072635908\n",
      "Test set: Average loss: 2.1457, Accuracy: 248.9999988079071/300 (83%)\n",
      "\n",
      " Pretrain acc : 248.9999988079071\n",
      "\n",
      "\n",
      "Test start\n",
      "\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.5980, Accuracy: 153.0/300 (51%)\n",
      "Epoch 1 Loss 0.677710 acc : 0.540436 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 3.1191, Accuracy: 216.00000071525574/300 (72%)\n",
      "Epoch 2 Loss 0.608098 acc : 0.792803 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.8571, Accuracy: 251.00000047683716/300 (84%)\n",
      "Epoch 3 Loss 0.552690 acc : 0.851705 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.5591, Accuracy: 263.0000002384186/300 (88%)\n",
      "Epoch 4 Loss 0.492061 acc : 0.896023 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.2634, Accuracy: 272.99999928474426/300 (91%)\n",
      "Epoch 5 Loss 0.430286 acc : 0.904545 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.9485, Accuracy: 275.0000002384186/300 (92%)\n",
      "Epoch 6 Loss 0.364347 acc : 0.908712 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.6766, Accuracy: 276.0000002384186/300 (92%)\n",
      "Epoch 7 Loss 0.314774 acc : 0.914489 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4992, Accuracy: 276.0000002384186/300 (92%)\n",
      "Epoch 8 Loss 0.285979 acc : 0.911364 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.4089, Accuracy: 275.0000011920929/300 (92%)\n",
      "Epoch 9 Loss 0.271415 acc : 0.908617 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3614, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 10 Loss 0.263647 acc : 0.908523 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3366, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 11 Loss 0.258256 acc : 0.908996 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3248, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 12 Loss 0.255982 acc : 0.908333 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3093, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 13 Loss 0.252359 acc : 0.907102 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2926, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 14 Loss 0.249659 acc : 0.908902 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2779, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 15 Loss 0.247337 acc : 0.910133 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2653, Accuracy: 274.0000011920929/300 (91%)\n",
      "Epoch 16 Loss 0.246046 acc : 0.908523 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2506, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 17 Loss 0.244778 acc : 0.911364 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2465, Accuracy: 274.99999952316284/300 (92%)\n",
      "Epoch 18 Loss 0.244252 acc : 0.909943 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2427, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 19 Loss 0.242971 acc : 0.911269 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2296, Accuracy: 276.99999952316284/300 (92%)\n",
      "Epoch 20 Loss 0.241778 acc : 0.911648 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2264, Accuracy: 275.99999952316284/300 (92%)\n",
      "Epoch 21 Loss 0.241578 acc : 0.910133 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2277, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 22 Loss 0.241423 acc : 0.909848 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2210, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 23 Loss 0.241078 acc : 0.911648 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2167, Accuracy: 276.99999952316284/300 (92%)\n",
      "Epoch 24 Loss 0.240750 acc : 0.911269 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2189, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 25 Loss 0.239771 acc : 0.910322 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2167, Accuracy: 276.99999952316284/300 (92%)\n",
      "Test Accuracy: 277.00\n",
      "Test set \n",
      " lamb : 0.08984004614562653 \n",
      " lamb_enp :  0.5942132301291498 \n",
      " grid : 2\n",
      "\n",
      "Test set: Average loss: 5.3972, Accuracy: 171.0/300 (57%)\n",
      "Epoch 1 Loss 3.236876 acc : 0.521307 reg : 1.920986 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 4.5400, Accuracy: 171.0000011920929/300 (57%)\n",
      "Epoch 2 Loss 2.437238 acc : 0.520076 reg : 1.434149 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.7835, Accuracy: 168.00000047683716/300 (56%)\n",
      "Epoch 3 Loss 2.095158 acc : 0.535227 reg : 1.208716 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.7145, Accuracy: 173.99999952316284/300 (58%)\n",
      "Epoch 4 Loss 1.883625 acc : 0.545549 reg : 1.015924 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.5842, Accuracy: 180.0/300 (60%)\n",
      "Epoch 5 Loss 1.745226 acc : 0.580398 reg : 0.954676 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.5367, Accuracy: 184.00000023841858/300 (61%)\n",
      "Epoch 6 Loss 1.635791 acc : 0.585890 reg : 0.868631 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.4451, Accuracy: 185.00000023841858/300 (62%)\n",
      "Epoch 7 Loss 1.546908 acc : 0.606155 reg : 0.838425 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.3629, Accuracy: 184.00000023841858/300 (61%)\n",
      "Epoch 8 Loss 1.486951 acc : 0.621496 reg : 0.817284 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.3412, Accuracy: 188.0000011920929/300 (63%)\n",
      "Epoch 9 Loss 1.432790 acc : 0.636174 reg : 0.785985 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.3040, Accuracy: 189.00000023841858/300 (63%)\n",
      "Epoch 10 Loss 1.389491 acc : 0.661648 reg : 0.718342 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.2473, Accuracy: 190.0000011920929/300 (63%)\n",
      "Epoch 11 Loss 1.354790 acc : 0.668087 reg : 0.707538 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.2121, Accuracy: 195.99999928474426/300 (65%)\n",
      "Epoch 12 Loss 1.325149 acc : 0.664489 reg : 0.727827 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.1526, Accuracy: 201.0/300 (67%)\n",
      "Epoch 13 Loss 1.298684 acc : 0.686742 reg : 0.672750 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.0727, Accuracy: 210.0/300 (70%)\n",
      "Epoch 14 Loss 1.258218 acc : 0.715720 reg : 0.668679 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.9560, Accuracy: 211.0/300 (70%)\n",
      "Epoch 15 Loss 1.234587 acc : 0.717045 reg : 0.650207 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.9568, Accuracy: 221.00000095367432/300 (74%)\n",
      "Epoch 16 Loss 1.216497 acc : 0.725568 reg : 0.616431 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.9909, Accuracy: 231.00000071525574/300 (77%)\n",
      "Epoch 17 Loss 1.186295 acc : 0.741477 reg : 0.625774 stop count : 2 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.6818, Accuracy: 224.00000095367432/300 (75%)\n",
      "Epoch 18 Loss 1.156238 acc : 0.745833 reg : 0.607193 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.6110, Accuracy: 231.0/300 (77%)\n",
      "Epoch 19 Loss 1.111498 acc : 0.753125 reg : 0.590902 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.4807, Accuracy: 231.0/300 (77%)\n",
      "Epoch 20 Loss 1.080410 acc : 0.751326 reg : 0.563651 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.4687, Accuracy: 234.99999904632568/300 (78%)\n",
      "Epoch 21 Loss 1.059988 acc : 0.770360 reg : 0.533046 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.4521, Accuracy: 237.00000071525574/300 (79%)\n",
      "Epoch 22 Loss 1.037850 acc : 0.771496 reg : 0.536267 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.4213, Accuracy: 235.99999976158142/300 (79%)\n",
      "Epoch 23 Loss 1.033003 acc : 0.774053 reg : 0.524141 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3745, Accuracy: 241.99999976158142/300 (81%)\n",
      "Epoch 24 Loss 1.003256 acc : 0.788068 reg : 0.494195 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3669, Accuracy: 243.99999976158142/300 (81%)\n",
      "Epoch 25 Loss 0.988525 acc : 0.792614 reg : 0.487096 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2963, Accuracy: 247.99999952316284/300 (83%)\n",
      "Epoch 26 Loss 0.985358 acc : 0.781061 reg : 0.481306 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3388, Accuracy: 241.00000047683716/300 (80%)\n",
      "Epoch 27 Loss 0.968962 acc : 0.808428 reg : 0.494473 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3115, Accuracy: 242.99999952316284/300 (81%)\n",
      "Epoch 28 Loss 0.959603 acc : 0.798390 reg : 0.494552 stop count : 2 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3296, Accuracy: 239.99999952316284/300 (80%)\n",
      "Epoch 29 Loss 0.951089 acc : 0.808712 reg : 0.478352 stop count : 3 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2084, Accuracy: 250.9999988079071/300 (84%)\n",
      "Epoch 30 Loss 0.947713 acc : 0.791667 reg : 0.449324 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.4149, Accuracy: 238.99999976158142/300 (80%)\n",
      "Epoch 31 Loss 0.924415 acc : 0.803977 reg : 0.444199 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2033, Accuracy: 253.0000011920929/300 (84%)\n",
      "Epoch 32 Loss 0.931063 acc : 0.792992 reg : 0.452528 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2652, Accuracy: 250.0000011920929/300 (83%)\n",
      "Epoch 33 Loss 0.942866 acc : 0.797348 reg : 0.439147 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2253, Accuracy: 246.99999952316284/300 (82%)\n",
      "Epoch 34 Loss 0.916723 acc : 0.805871 reg : 0.427728 stop count : 2 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1896, Accuracy: 249.00000047683716/300 (83%)\n",
      "Epoch 35 Loss 0.953768 acc : 0.777083 reg : 0.428284 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3334, Accuracy: 245.00000047683716/300 (82%)\n",
      "Epoch 36 Loss 0.928002 acc : 0.788920 reg : 0.443093 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1392, Accuracy: 250.99999952316284/300 (84%)\n",
      "Epoch 37 Loss 0.888114 acc : 0.812784 reg : 0.416199 stop count : 0 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2390, Accuracy: 247.99999952316284/300 (83%)\n",
      "Epoch 38 Loss 0.881821 acc : 0.806629 reg : 0.401520 stop count : 1 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2330, Accuracy: 250.9999988079071/300 (84%)\n",
      "Epoch 39 Loss 0.873666 acc : 0.817235 reg : 0.415428 stop count : 2 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1879, Accuracy: 254.99999952316284/300 (85%)\n",
      "Epoch 40 Loss 0.885688 acc : 0.815436 reg : 0.425684 stop count : 3 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1566, Accuracy: 254.99999952316284/300 (85%)\n",
      "Epoch 41 Loss 0.869777 acc : 0.821307 reg : 0.404163 stop count : 4 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1795, Accuracy: 255.0000011920929/300 (85%)\n",
      "Epoch 42 Loss 0.865557 acc : 0.815530 reg : 0.394137 stop count : 5 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.2696, Accuracy: 242.00000047683716/300 (81%)\n",
      "Epoch 43 Loss 0.936692 acc : 0.798011 reg : 0.419039 stop count : 6 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.5764, Accuracy: 213.99999904632568/300 (71%)\n",
      "Epoch 44 Loss 1.305739 acc : 0.704545 reg : 0.450004 stop count : 7 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 3.4012, Accuracy: 216.00000095367432/300 (72%)\n",
      "Epoch 45 Loss 1.161024 acc : 0.714489 reg : 0.449790 stop count : 8 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.9970, Accuracy: 222.00000047683716/300 (74%)\n",
      "Epoch 46 Loss 0.958444 acc : 0.760890 reg : 0.442281 stop count : 9 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.5595, Accuracy: 227.00000047683716/300 (76%)\n",
      "Epoch 47 Loss 0.901816 acc : 0.796496 reg : 0.403172 stop count : 10 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3625, Accuracy: 235.99999952316284/300 (79%)\n",
      "Epoch 48 Loss 0.883313 acc : 0.804356 reg : 0.425095 stop count : 11 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3220, Accuracy: 238.99999952316284/300 (80%)\n",
      "Epoch 49 Loss 0.862147 acc : 0.808617 reg : 0.412024 stop count : 12 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.3125, Accuracy: 233.99999952316284/300 (78%)\n",
      "Epoch 50 Loss 0.855899 acc : 0.805777 reg : 0.385442 stop count : 13 lamb : 0.08984004614562653 lamb_enp : 0.5942132301291498\n",
      "Test set: Average loss: 2.1406, Accuracy: 249.99999952316284/300 (83%)\n",
      "\n",
      " Pretrain acc : 249.99999952316284\n",
      "\n",
      "\n",
      "Test start\n",
      "\n",
      "\n",
      "\n",
      "Test set: Average loss: 3.4783, Accuracy: 153.0/300 (51%)\n",
      "Epoch 1 Loss 0.682940 acc : 0.595833 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 3.3227, Accuracy: 259.99999952316284/300 (87%)\n",
      "Epoch 2 Loss 0.643897 acc : 0.883049 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 3.0697, Accuracy: 272.99999952316284/300 (91%)\n",
      "Epoch 3 Loss 0.584027 acc : 0.911174 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.7034, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 4 Loss 0.503246 acc : 0.912973 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 2.2779, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 5 Loss 0.420710 acc : 0.911174 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.8977, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 6 Loss 0.352451 acc : 0.910227 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.5919, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 7 Loss 0.301969 acc : 0.914489 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.3876, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 8 Loss 0.270754 acc : 0.912784 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2958, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 9 Loss 0.256812 acc : 0.911553 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2388, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 10 Loss 0.251068 acc : 0.912784 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2256, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 11 Loss 0.249511 acc : 0.909848 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2166, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 12 Loss 0.247021 acc : 0.914583 reg : 0.000000 stop count : 0 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2247, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 13 Loss 0.247193 acc : 0.908902 reg : 0.000000 stop count : 1 lamb : 0 lamb_enp : 2.0\n",
      "Test set: Average loss: 1.2186, Accuracy: 273.99999952316284/300 (91%)\n",
      "Epoch 14 Loss 0.212472 acc : 0.927083 reg : 0.000000 stop count : 2 lamb : 0 lamb_enp : 2.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      5\u001b[0m space \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlamb_entropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m : hp\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m9\u001b[39m))\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 11\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[69], line 180\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest start\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    179\u001b[0m train_process \u001b[38;5;241m=\u001b[39m Early_stop_train(model, optimizer, criterion)\n\u001b[1;32m--> 180\u001b[0m \u001b[43mtrain_process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlamb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m _,acc \u001b[38;5;241m=\u001b[39m train_process\u001b[38;5;241m.\u001b[39mtest(test_loader)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mEarly_stop_train.train_model\u001b[1;34m(self, train_loader, test_loader, epochs, res, lamb, lamb_entropy)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     76\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pop75\\anaconda3\\envs\\Lee\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "space = {\n",
    "    'lamb': hp.uniform('lamb',0,0.1),\n",
    "    'lamb_entropy': hp.uniform('lamb_entropy',0,2),\n",
    "    'grid' : hp.choice('grid',range(1,9))\n",
    "}\n",
    "\n",
    "best = fmin(fn=search, space=space, algo=tpe.suggest, max_evals=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
